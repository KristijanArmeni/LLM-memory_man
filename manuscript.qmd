---
title: Memory mechanisms in transformer language models
author:
    - name: Kristijan Armeni
      affiliation: Johns Hopkins University, Department of Psychological and Brain Sciences
    - name: Tal Linzen
      affiliation: 
      - name: New York University, Department of Linguistics
      - name: New York University, Center for Data Science
    - name: Christopher Honey
      affiliation: Johns Hopkins University, Department of Psychological and Brain Sciences
toc: true
footnotes-hover: true
number-sections: true
abstract: |
  Retrieving specific items from recent input is a core computation in humans and artificial systems that predict and 
  generate language. This is a form of short-term memory, as when we encounter arbitrary items {'a','b','c'}, and we 
  must later retrieve them in order.  It was recently shown that transformer language models can retrieve verbatim 
  copies of arbitrary nouns that occurred earlier in text. What are transformer mechanisms that support short-term 
  memory? Analyzing GPT-2 attention in a short-term memory task, we found that, upon detecting a repeated token (e.g. a 
  repetition of 'b'), a distributed set of attention heads in early/middle layers attended to the matching token (i.e. 
  original 'b') whereas a distributed set of heads in late layers attended to the tokens that _followed_ the 
  matching token (i.e. original 'c'). Ablating a small number of matching heads drastically hurt GPT-2's ability to 
  retrieve post-match nouns, suggesting matching heads have causal primacy over post-match heads. Finally, we show that 
  the circuits generalize to other tasks: ablating the short-term memory circuit also hurts GPT-2 language modeling 
  performance and the ability to retrieve the grammatical number of past tokens.
bibliography: library.bib
format: html
---

# Introduction

Flexibly maintaining and accessing recent text from short-term memory is a core computation in human [@elman_finding_1990] Flexibly maintaining and accessing recent text from short-term memory is a core computation in human [@baddeley_working_2003-1; @cowan_what_2008; @oberauer_benchmarks_2018] and artificial systems [@elman_finding_1990; @nematzadeh_memory_2020] predicting and generating natural language. For example, when we are told 'Your ticket number is 172' and shortly after asked 'What is your ticket number?' we retrieve the number '172' from short-term memory.

Contemporary transformer-based language models [LM, @vaswani_attention_2017] are currently the most successful algorithms at predicting words based on context and, seemingly, many natural language tasks that can be reformatted as text generation. A possible reason is that by weighing all context words in a task-relevant manner [attention, @bahdanau_neural_2014], transformers learn to retrieve the past tokens that are relevant in order to correctly predict upcoming words, a kind of short-term memory retrieval. For example, the ability to solve tasks by completing text prompts containing instructions and a small number of examples, without any explicit parameter updates, (in-context learning, see @liu_pre-train_2021, for review) suggests that transformer LMs do so by retrieving instructions and relevant examples from short-term memory.

```{python}
#| echo: false
#| out-width: 100%
#| fig-cap: Testing the figure caption.
from wm_suite.viz.ablation import fig_attn
from wm_suite.paths import get_paths
from wm_suite.utils import set_logger_level

set_logger_level("critical")

cfg = get_paths()
cfg.data = 'c:\\users\\karmeni1\\project\\lm-mem\\data\\ablation'
fig = fig_attn.make_plot(datadir=cfg.data, query="colon-colon-p1")
```

# Attention in verbatim retrieval

Previous experiments showed that GPT-2 could retrieve verbatim sequences of nouns from past context [@armeni_characterizing_2022]. The evidence for this short-term memory ability was obtained by showing that GPT-2 surprisal on a sequence of arbitrary nouns reduced by nearly 100% (to a near-zero absolute surprisal) if the same sequence has occurred earlier in text. This relative change in surprisal indicated to what extent GPT-2 expected the same nouns to reoccur (and, by extension, to what extent it retrieved the past representations of these nouns). Here, we were interested in attention patterns at the moment when the model is about to retrieve the repeated second list: are there attention heads in GPT-2 that attend to the original list of nouns?

## Methods

**Attention pattern analysis** To characterize the attention patterns in GPT-2, we tested what attention heads attended from the second colon $c_{t+k}$ (occurring $k$ tokens after the original colon \$c_t\$) towards the following target time-windows: a) the original colon $c_t$ (match token), b) the three nouns $n_{t+1}= [w_{t+1}, w_{t+2}, w_{t+3}]$[^1] that originally followed the match token (postmatch tokens), and c) towards three recent tokens ($[w_{(t+k)-3}, w_{(t+k)-2}, w_{(t+k)-1}]$) immediately preceding $c_{t+k}$. In each time-window, we\[\^New fotnote\] selected heads that fell into the top-20 heads with strongest attention and had at least the (summed) attention score of 0.2 (i.e. heads had to place at least one fifth of their probability mass to the target tokens). For target time windows that contained multiple tokens, we summed the attention weights over the tokens.

[^1]: In the notation and analysis, we excluded the comma tokens (serving as enumeration symbols) as these receive near-zero attention (see Fig. \ref{fig:attn_weights_example}).

**Verbatim retrieval** In the original experiment by [@armeni_characterizing_2022] there were a total of $N = 23$ noun lists, each containing $10$ arbitrary nouns.[^2] Each list was additionally circularly shifted such that each noun was tested in all ordinal list positions. This resulted in the total of $23 \times 10 = 230$ noun lists. Here, we analyzed the version of the experiment with the shortest sequences where list length was kept at 3 nouns. We report attention scores averaged across the $N = 230$ total sequences. Each input sequence was the same except for the list of nouns used to fill the noun list slot in the template (see Appendix \ref{app:paradigm} for details).

[^2]: The nouns were sampled from the Toronto Noun Word Pool curated to test working memory in human experiments: [http://memory.psych.upenn.edu/files/wordpools/nouns.txt](#0).

## Results

**Attention heads attending to match tokens (matching heads) are sparse and found in early layers.**

# Ablation experiment

So far, we showed that transformer language models (in our case GPT-2), contain attention heads that upon encountering a repeated token $c_{t+k}$ (query) attended back either to its first occurrence $c_t$, $k$ tokens ago, or towards the nouns $n_{t+1}$ that followed $c_t$ (postmatch nouns). Such selective attention patterns make them potential candidate heads for short-term memory retrieval [@elhage_mathematical_2021; @olsson_-context_2022]. If so, are these heads with strongly selective attention patterns, necessary for GPT-2 retrieval?

## Methods

**Zero ablation** To remove the contribution of any single head in the model, we set the attention weights $\alpha$ of an attention head to $0$ after they were computed in the forward pass. Thus, the output of the ablated attention head $k$ for current layer $\textbf{h}^k_l = \mathbf{W}^v \alpha^0 =\mathbf{0}$, a product of value vectors $\mathbf{W}^v$ with the zeroed attention weights $\alpha^0$, resulted in a zero vector $\mathbf{0}$ which was added to the concatenated layer output $\mathbf{z}_l$ (e.g. $\mathbf{z}_l = [h^1, h^2_{*}=\mathbf{0}, ... h^{12}])$. After the ablation, $\mathbf{z}_l$ was projected via output weights $\mathbf{W^o}$ and added to token representations $\mathbf{x}_{l-1}$ computed by the previous layer (a.k.a the residual stream) in order to compute updated token representations for the current layer $\mathbf{x}_{l} = \mathbf{x}_{l-1} + W^o\mathbf{z}_l$.

**Quantifying memory retrieval** We re-evaluated the ablated models on the short-term memory task measuring LM verbatim retrieval of nouns from context [@armeni_characterizing_2022]. In this experiment, the nouns ($n_{t+1}$) that followed the original colon $c_t$ were now repeated $k$ tokens later, after the second occurrence of the colon token $c_{t+k}$. Verbatim retrieval was operationalized as *repeat surprisal*: the ratio (expressed as percentage) of GPT-2 surprisal on the two list non-initial nouns $n_{(t+2)} = [w_{(t+2)}, w_{(t+3)}]$ in the first list relative to the same nouns $n_{(t+k+2)} = [w_{(t+k+2)}, w_{(t+k+3)}]$ in the second list. Prior to computing the ratio, list-specific surprisal was averaged across the two tokens. We decided to quantify repeat surprisal at list non-initial nouns because the memory effect stabilizes after the first noun in the list [@armeni_characterizing_2022]. A reduction in repeat surprisal (i.e. \< 100%) indicates the degree to which GPT-2 expected the same nouns to reoccur. GPT-2 has a near-zero repeat surprisal (i.e. near perfect retrieval) on sequences of repeated arbitrary nouns [@armeni_characterizing_2022]. A maximally effective ablation that rendered the model incapable of retrieving the nouns would therefore lead to a repeat surprisal of 100%.

**Greedy search** In order to find a set of heads among $n$ selected heads $\mathbf{H} = [h_1, h_2, ..., h_n]$ that support the performance on the retrieval task, we ran a greedy search procedure. We started by ablating each head in $\mathbf{H}$ individually and logged the head $h_{max}$ with the strongest effect on memory retrieval. In the subsequent iteration, we then ablated all possible pairs consisting of $h_{max}$ and each of the remaining heads in $\mathbf{H}$. We logged the maximally effective pair of heads and in the next iteration proceeded to ablate all possible combinations of the maximal pair and each of the remaining heads in $\mathbf{H}$ etc. The search was stopped early if 6 consecutive iterations did not have lead to least 0.5% increase in repeat surprisal.

**Copying score**

## Results

------------------------------------------------------------------------