
@article{niso_meg-bids_2018,
	title = {{MEG}-{BIDS}, the brain imaging data structure extended to magnetoencephalography},
	volume = {5},
	issn = {2052-4463},
	url = {http://www.nature.com/articles/sdata2018110},
	doi = {https://doi.org/10.1038/sdata.2018.110},
	language = {en},
	number = {1},
	urldate = {2020-04-25},
	journal = {Scientific Data},
	author = {Niso, Guiomar and Gorgolewski, Krzysztof J. and Bock, Elizabeth and Brooks, Teon L. and Flandin, Guillaume and Gramfort, Alexandre and Henson, Richard N. and Jas, Mainak and Litvak, Vladimir and T. Moreau, Jeremy and Oostenveld, Robert and Schoffelen, Jan-Mathijs and Tadel, Francois and Wexler, Joseph and Baillet, Sylvain},
	month = dec,
	year = {2018},
	keywords = {BIDS, MEG, open science, reproducibility},
}

@inproceedings{mikolov_distributed_2013,
	address = {Red Hook, NY, USA},
	series = {{NIPS}'13},
	title = {Distributed representations of words and phrases and their compositionality},
	url = {https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf},
	abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling.An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
	booktitle = {Proceedings of the 26th {International} {Conference} on {Neural} {Information} {Processing} {Systems} - {Volume} 2},
	publisher = {Curran Associates Inc.},
	author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	year = {2013},
	note = {event-place: Lake Tahoe, Nevada},
	pages = {3111--3119},
}

@article{schlag_learning_2021,
	title = {Learning associative inference using fast weight memory},
	url = {http://arxiv.org/abs/2011.07831},
	abstract = {Humans can quickly associate stimuli to solve problems in novel contexts. Our novel neural network model learns state representations of facts that can be composed to perform such associative inference. To this end, we augment the LSTM model with an associative memory, dubbed Fast Weight Memory (FWM). Through differentiable operations at every step of a given input sequence, the LSTM updates and maintains compositional associations stored in the rapidly changing FWM weights. Our model is trained end-to-end by gradient descent and yields excellent performance on compositional language reasoning problems, meta-reinforcement-learning for POMDPs, and small-scale word-level language modelling.},
	urldate = {2021-09-16},
	journal = {arXiv:2011.07831 [cs]},
	author = {Schlag, Imanol and Munkhdalai, Tsendsuren and Schmidhuber, Jürgen},
	month = feb,
	year = {2021},
	note = {arXiv: 2011.07831},
}

@article{ravishankar_word_2022,
	title = {Word {Order} {Does} {Matter} ({And} {Shuffled} {Language} {Models} {Know} {It})},
	url = {http://arxiv.org/abs/2203.10995},
	abstract = {Recent studies have shown that language models pretrained and/or fine-tuned on randomly permuted sentences exhibit competitive performance on GLUE, putting into question the importance of word order information. Somewhat counter-intuitively, some of these studies also report that position embeddings appear to be crucial for models' good performance with shuffled text. We probe these language models for word order information and investigate what position embeddings learned from shuffled text encode, showing that these models retain information pertaining to the original, naturalistic word order. We show this is in part due to a subtlety in how shuffling is implemented in previous work -- before rather than after subword segmentation. Surprisingly, we find even Language models trained on text shuffled after subword segmentation retain some semblance of information about word order because of the statistical dependencies between sentence length and unigram probabilities. Finally, we show that beyond GLUE, a variety of language understanding tasks do require word order information, often to an extent that cannot be learned through fine-tuning.},
	urldate = {2022-03-23},
	journal = {arXiv:2203.10995 [cs]},
	author = {Ravishankar, Vinit and Abdou, Mostafa and Kulmizev, Artur and Søgaard, Anders},
	month = mar,
	year = {2022},
	note = {arXiv: 2203.10995},
}

@article{linzen_assessing_2016,
	title = {Assessing the ability of {LSTMs} to learn {Syntax}-sensitive dependencies},
	volume = {4},
	issn = {2307-387X},
	url = {https://doi.org/10.1162/tacl_a_00115},
	doi = {10.1162/tacl_a_00115},
	abstract = {The success of long short-term memory (LSTM) neural networks in language
processing is typically attributed to their ability to capture long-distance
statistical regularities. Linguistic regularities are often sensitive to
syntactic structure; can such dependencies be captured by LSTMs, which do not
have explicit structural representations? We begin addressing this question
using number agreement in English subject-verb dependencies. We probe the
architecture’s grammatical competence both using training objectives with an
explicit grammatical target (number prediction, grammaticality judgments) and
using language models. In the strongly supervised settings, the LSTM achieved
very high overall accuracy (less than 1\% errors), but errors increased when
sequential and structural information conflicted. The frequency of such errors
rose sharply in the language-modeling setting. We conclude that LSTMs can
capture a non-trivial amount of grammatical structure given targeted
supervision, but stronger architectures may be required to further reduce
errors; furthermore, the language modeling signal is insufficient for capturing
syntax-sensitive dependencies, and should be supplemented with more direct
supervision if such dependencies need to be captured.},
	urldate = {2022-02-01},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Linzen, Tal and Dupoux, Emmanuel and Goldberg, Yoav},
	month = dec,
	year = {2016},
	pages = {521--535},
}

@inproceedings{ba_using_2016,
	address = {Red Hook, NY, USA},
	series = {{NIPS}'16},
	title = {Using fast weights to attend to the recent past},
	isbn = {978-1-5108-3881-9},
	abstract = {Until recently, research on artificial neural networks was largely restricted to systems with only two types of variable: Neural activities that represent the current or recent input and weights that learn to capture regularities among inputs, outputs and payoffs. There is no good reason for this restriction. Synapses have dynamics at many different time-scales and this suggests that artificial neural networks might benefit from variables that change slower than activities but much faster than the standard weights. These "fast weights" can be used to store temporary memories of the recent past and they provide a neurally plausible way of implementing the type of attention to the past that has recently proved very helpful in sequence-to-sequence models. By using fast weights we can avoid the need to store copies of neural activity patterns.},
	urldate = {2022-02-01},
	booktitle = {Proceedings of the 30th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Ba, Jimmy and Hinton, Geoffrey and Mnih, Volodymyr and Leibo, Joel Z. and Ionescu, Catalin},
	month = dec,
	year = {2016},
	pages = {4338--4346},
}

@inproceedings{brown_language_2020,
	title = {Language models are few-shot learners},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	year = {2020},
	pages = {1877--1901},
}

@inproceedings{yogatama_memory_2018,
	title = {Memory {Architectures} in {Recurrent} {Neural} {Network} {Language} {Models}},
	url = {https://openreview.net/forum?id=SkFqf0lAZ},
	abstract = {We compare and analyze sequential, random access, and stack memory architectures for recurrent neural network language models. Our experiments on the Penn Treebank and Wikitext-2 datasets show that...},
	language = {en},
	urldate = {2022-02-01},
	author = {Yogatama, Dani and Miao, Yishu and Melis, Gabor and Ling, Wang and Kuncoro, Adhiguna and Dyer, Chris and Blunsom, Phil},
	month = feb,
	year = {2018},
}

@inproceedings{weston_memory_2015,
	address = {San Diego},
	title = {Memory networks},
	abstract = {We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chaining multiple supporting sentences to answer questions that require understanding the intension of verbs.},
	language = {English (US)},
	booktitle = {3rd {International} {Conference} on {Learning} {Representations}, {ICLR} 2015},
	publisher = {3rd International Conference on Learning Representations},
	author = {Weston, Jason and Chopra, Sumit and Bordes, Antoine},
	year = {2015},
}

@inproceedings{weston_memory_2015-1,
	title = {Memory networks: 3rd {International} {Conference} on {Learning} {Representations}, {ICLR} 2015},
	shorttitle = {Memory networks},
	url = {http://www.scopus.com/inward/record.url?scp=85083951616&partnerID=8YFLogxK},
	abstract = {We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chaining multiple supporting sentences to answer questions that require understanding the intension of verbs.},
	urldate = {2022-02-01},
	author = {Weston, Jason and Chopra, Sumit and Bordes, Antoine},
	year = {2015},
}

@article{weston_memory_2015-2,
	title = {Memory {Networks}},
	volume = {abs/1410.3916},
	journal = {CoRR},
	author = {Weston, Jason and Chopra, Sumit and Bordes, Antoine},
	year = {2015},
}

@inproceedings{vaswani_attention_2017,
	address = {Red Hook, NY, USA},
	series = {{NIPS}'17},
	title = {Attention is all you need},
	isbn = {978-1-5108-6096-4},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
	urldate = {2022-02-01},
	booktitle = {Proceedings of the 31st {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Łukasz and Polosukhin, Illia},
	month = dec,
	year = {2017},
	pages = {6000--6010},
}

@inproceedings{van_schijndel_quantity_2019,
	address = {Hong Kong, China},
	title = {Quantity doesn't buy quality syntax with neural language models},
	url = {https://aclanthology.org/D19-1592},
	doi = {10.18653/v1/D19-1592},
	abstract = {Recurrent neural networks can learn to predict upcoming words remarkably well on average; in syntactically complex contexts, however, they often assign unexpectedly high probabilities to ungrammatical words. We investigate to what extent these shortcomings can be mitigated by increasing the size of the network and the corpus on which it is trained. We find that gains from increasing network size are minimal beyond a certain point. Likewise, expanding the training corpus yields diminishing returns; we estimate that the training corpus would need to be unrealistically large for the models to match human performance. A comparison to GPT and BERT, Transformer-based models trained on billions of words, reveals that these models perform even more poorly than our LSTMs in some constructions. Our results make the case for more data efficient architectures.},
	urldate = {2022-02-01},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {van Schijndel, Marten and Mueller, Aaron and Linzen, Tal},
	month = nov,
	year = {2019},
	pages = {5831--5837},
}

@article{weston_memory_2015-3,
	title = {Memory networks},
	url = {http://arxiv.org/abs/1410.3916},
	abstract = {We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chaining multiple supporting sentences to answer questions that require understanding the intension of verbs.},
	urldate = {2022-01-19},
	journal = {arXiv:1410.3916 [cs, stat]},
	author = {Weston, Jason and Chopra, Sumit and Bordes, Antoine},
	month = nov,
	year = {2015},
	note = {arXiv: 1410.3916},
}

@article{tay_efficient_2020,
	title = {Efficient transformers: {A} survey},
	shorttitle = {Efficient {Transformers}},
	url = {http://arxiv.org/abs/2009.06732},
	abstract = {Transformer model architectures have garnered immense interest lately due to their effectiveness across a range of domains like language, vision and reinforcement learning. In the field of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack. Recently, a dizzying number of "X-former" models have been proposed - Reformer, Linformer, Performer, Longformer, to name a few - which improve upon the original Transformer architecture, many of which make improvements around computational and memory efficiency. With the aim of helping the avid researcher navigate this flurry, this paper characterizes a large and thoughtful selection of recent efficiency-flavored "X-former" models, providing an organized and comprehensive overview of existing work and models across multiple domains.},
	urldate = {2022-01-19},
	journal = {arXiv:2009.06732 [cs]},
	author = {Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
	month = sep,
	year = {2020},
	note = {arXiv: 2009.06732
version: 2},
	file = {arXiv Fulltext PDF:/Users/kriarm/Zotero/storage/AA49WYHI/Tay et al. - 2020 - Efficient Transformers A Survey.pdf:application/pdf;arXiv.org Snapshot:/Users/kriarm/Zotero/storage/S5JAYTZB/2009.html:text/html},
}

@inproceedings{ritter_cognitive_2017,
	title = {Cognitive psychology for deep neural networks: {A} shape bias case study},
	shorttitle = {Cognitive {Psychology} for {Deep} {Neural} {Networks}},
	url = {https://proceedings.mlr.press/v70/ritter17a.html},
	abstract = {Deep neural networks (DNNs) have advanced performance on a wide range of complex tasks, rapidly outpacing our understanding of the nature of their solutions. While past work sought to advance our understanding of these models, none has made use of the rich history of problem descriptions, theories, and experimental methods developed by cognitive psychologists to study the human mind. To explore the potential value of these tools, we chose a well-established analysis from developmental psychology that explains how children learn word labels for objects, and applied that analysis to DNNs. Using datasets of stimuli inspired by the original cognitive psychology experiments, we find that state-of-the-art one shot learning models trained on ImageNet exhibit a similar bias to that observed in humans: they prefer to categorize objects according to shape rather than color. The magnitude of this shape bias varies greatly among architecturally identical, but differently seeded models, and even fluctuates within seeds throughout training, despite nearly equivalent classification performance. These results demonstrate the capability of tools from cognitive psychology for exposing hidden computational properties of DNNs, while concurrently providing us with a computational model for human word learning.},
	language = {en},
	urldate = {2022-02-01},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Ritter, Samuel and Barrett, David G. T. and Santoro, Adam and Botvinick, Matt M.},
	month = jul,
	year = {2017},
	note = {ISSN: 2640-3498},
	pages = {2940--2949},
}

@inproceedings{oconnor_what_2021,
	address = {Online},
	title = {What context features can transformer language models use?},
	url = {https://aclanthology.org/2021.acl-long.70},
	doi = {10.18653/v1/2021.acl-long.70},
	abstract = {Transformer-based language models benefit from conditioning on contexts of hundreds to thousands of previous tokens. What aspects of these contexts contribute to accurate model prediction? We describe a series of experiments that measure usable information by selectively ablating lexical and structural information in transformer language models trained on English Wikipedia. In both mid- and long-range contexts, we find that several extremely destructive context manipulations—including shuffling word order within sentences and deleting all words other than nouns—remove less than 15\% of the usable information. Our results suggest that long contexts, but not their detailed syntactic and propositional content, are important for the low perplexity of current transformer language models.},
	urldate = {2022-02-01},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {O'Connor, Joe and Andreas, Jacob},
	month = aug,
	year = {2021},
	pages = {851--864},
}

@article{merity_pointer_2016,
	title = {Pointer sentinel mixture models},
	url = {https://openreview.net/forum?id=Byj72udxe},
	abstract = {Pointer sentinel mixture models provide a method to combine a traditional vocabulary softmax with a pointer network, providing state of the art results in language modeling on PTB and the newly...},
	language = {en},
	urldate = {2022-02-01},
	author = {Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
	month = nov,
	year = {2016},
}

@inproceedings{merity_regularizing_2018,
	title = {Regularizing and optimizing {LSTM} language models},
	url = {https://openreview.net/forum?id=SyyGPP0TZ},
	abstract = {Effective regularization and optimization strategies for LSTM-based language models achieves SOTA on PTB and WT2.},
	language = {en},
	urldate = {2022-02-01},
	author = {Merity, Stephen and Keskar, Nitish Shirish and Socher, Richard},
	month = feb,
	year = {2018},
}

@inproceedings{marvin_targeted_2018,
	address = {Brussels, Belgium},
	title = {Targeted syntactic evaluation of language models},
	url = {https://aclanthology.org/D18-1151},
	doi = {10.18653/v1/D18-1151},
	abstract = {We present a data set for evaluating the grammaticality of the predictions of a language model. We automatically construct a large number of minimally different pairs of English sentences, each consisting of a grammatical and an ungrammatical sentence. The sentence pairs represent different variations of structure-sensitive phenomena: subject-verb agreement, reflexive anaphora and negative polarity items. We expect a language model to assign a higher probability to the grammatical sentence than the ungrammatical one. In an experiment using this data set, an LSTM language model performed poorly on many of the constructions. Multi-task training with a syntactic objective (CCG supertagging) improved the LSTM's accuracy, but a large gap remained between its performance and the accuracy of human participants recruited online. This suggests that there is considerable room for improvement over LSTMs in capturing syntax in a language model.},
	urldate = {2022-02-01},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Marvin, Rebecca and Linzen, Tal},
	month = oct,
	year = {2018},
	pages = {1192--1202},
}

@article{linzen_syntactic_2021,
	title = {Syntactic structure from deep learning},
	volume = {7},
	url = {https://doi.org/10.1146/annurev-linguistics-032020-051035},
	doi = {10.1146/annurev-linguistics-032020-051035},
	abstract = {Modern deep neural networks achieve impressive performance in engineering applications that require extensive linguistic skills, such as machine translation. This success has sparked interest in probing whether these models are inducing human-like grammatical knowledge from the raw data they are exposed to and, consequently, whether they can shed new light on long-standing debates concerning the innate structure necessary for language acquisition. In this article, we survey representative studies of the syntactic abilities of deep networks and discuss the broader implications that this work has for theoretical linguistics.},
	number = {1},
	urldate = {2022-02-01},
	journal = {Annual Review of Linguistics},
	author = {Linzen, Tal and Baroni, Marco},
	year = {2021},
	note = {\_eprint: https://doi.org/10.1146/annurev-linguistics-032020-051035},
	keywords = {deep learning, nature versus nurture, probing linguistic knowledge, syntax},
	pages = {195--212},
}

@inproceedings{lakretz_emergence_2019,
	address = {Minneapolis, Minnesota},
	title = {The emergence of number and syntax units in {LSTM} language models},
	url = {https://aclanthology.org/N19-1002},
	doi = {10.18653/v1/N19-1002},
	abstract = {Recent work has shown that LSTMs trained on a generic language modeling objective capture syntax-sensitive generalizations such as long-distance number agreement. We have however no mechanistic understanding of how they accomplish this remarkable feat. Some have conjectured it depends on heuristics that do not truly take hierarchical structure into account. We present here a detailed study of the inner mechanics of number tracking in LSTMs at the single neuron level. We discover that long-distance number information is largely managed by two “number units”. Importantly, the behaviour of these units is partially controlled by other units independently shown to track syntactic structure. We conclude that LSTMs are, to some extent, implementing genuinely syntactic processing mechanisms, paving the way to a more general understanding of grammatical encoding in LSTMs.},
	urldate = {2022-02-01},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Lakretz, Yair and Kruszewski, German and Desbordes, Theo and Hupkes, Dieuwke and Dehaene, Stanislas and Baroni, Marco},
	month = jun,
	year = {2019},
	pages = {11--20},
}

@article{lakretz_mechanisms_2021,
	title = {Mechanisms for handling nested dependencies in neural-network language models and humans},
	volume = {213},
	issn = {00100277},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0010027721001189},
	doi = {10.1016/j.cognition.2021.104699},
	language = {en},
	urldate = {2022-02-01},
	journal = {Cognition},
	author = {Lakretz, Yair and Hupkes, Dieuwke and Vergallito, Alessandra and Marelli, Marco and Baroni, Marco and Dehaene, Stanislas},
	month = aug,
	year = {2021},
	pages = {104699},
}

@inproceedings{khandelwal_sharp_2018,
	address = {Melbourne, Australia},
	title = {Sharp nearby, fuzzy far away: {How} neural language models use context},
	shorttitle = {Sharp {Nearby}, {Fuzzy} {Far} {Away}},
	url = {https://aclanthology.org/P18-1027},
	doi = {10.18653/v1/P18-1027},
	abstract = {We know very little about how neural language models (LM) use prior linguistic context. In this paper, we investigate the role of context in an LSTM LM, through ablation studies. Specifically, we analyze the increase in perplexity when prior context words are shuffled, replaced, or dropped. On two standard datasets, Penn Treebank and WikiText-2, we find that the model is capable of using about 200 tokens of context on average, but sharply distinguishes nearby context (recent 50 tokens) from the distant history. The model is highly sensitive to the order of words within the most recent sentence, but ignores word order in the long-range context (beyond 50 tokens), suggesting the distant past is modeled only as a rough semantic field or topic. We further find that the neural caching model (Grave et al., 2017b) especially helps the LSTM to copy words from within this distant context. Overall, our analysis not only provides a better understanding of how neural LMs use their context, but also sheds light on recent success from cache-based models.},
	urldate = {2022-02-01},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Khandelwal, Urvashi and He, He and Qi, Peng and Jurafsky, Dan},
	month = jul,
	year = {2018},
	pages = {284--294},
}

@inproceedings{holtzman_curious_2020,
	title = {The curious case of neural text degeneration},
	url = {https://iclr.cc/virtual_2020/poster_rygGQyrFvH.html},
	abstract = {Despite considerable advances in neural language modeling, it remains an open question what the best decoding strategy is for text generation from a language model (e.g. to generate a story). The counter-intuitive empirical observation is that even though the use of likelihood as training objective leads to high quality models for a broad range of language understanding tasks, maximization-based decoding methods such as beam search lead to degeneration — output text that is bland, incoherent, or gets stuck in repetitive loops. To address this we propose Nucleus Sampling, a simple but effective method to draw considerably higher quality text out of neural language models than previous decoding strategies. Our approach avoids text degeneration by truncating the unreliable tail of the probability distribution, sampling from the dynamic nucleus of tokens containing the vast majority of the probability mass. To properly examine current maximization-based and stochastic decoding methods, we compare generations from each of these methods to the distribution of human text along several axes such as likelihood, diversity, and repetition. Our results show that (1) maximization is an inappropriate decoding objective for open-ended text generation, (2) the probability distributions of the best current language models have an unreliable tail which needs to be truncated during generation and (3) Nucleus Sampling is currently the best available decoding strategy for generating long-form text that is both high-quality — as measured by human evaluation — and as diverse as human-written text.},
	language = {en},
	urldate = {2022-02-01},
	author = {Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
	month = apr,
	year = {2020},
}

@inproceedings{gulordava_colorless_2018,
	address = {New Orleans, Louisiana},
	title = {Colorless green recurrent networks dream hierarchically},
	url = {https://aclanthology.org/N18-1108},
	doi = {10.18653/v1/N18-1108},
	abstract = {Recurrent neural networks (RNNs) achieved impressive results in a variety of linguistic processing tasks, suggesting that they can induce non-trivial properties of language. We investigate to what extent RNNs learn to track abstract hierarchical syntactic structure. We test whether RNNs trained with a generic language modeling objective in four languages (Italian, English, Hebrew, Russian) can predict long-distance number agreement in various constructions. We include in our evaluation nonsensical sentences where RNNs cannot rely on semantic or lexical cues (“The colorless green ideas I ate with the chair sleep furiously”), and, for Italian, we compare model performance to human intuitions. Our language-model-trained RNNs make reliable predictions about long-distance agreement, and do not lag much behind human performance. We thus bring support to the hypothesis that RNNs are not just shallow-pattern extractors, but they also acquire deeper grammatical competence.},
	urldate = {2022-02-01},
	booktitle = {Proceedings of the 2018 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Gulordava, Kristina and Bojanowski, Piotr and Grave, Edouard and Linzen, Tal and Baroni, Marco},
	month = jun,
	year = {2018},
	pages = {1195--1205},
}

@inproceedings{gu_incorporating_2016,
	address = {Berlin, Germany},
	title = {Incorporating copying mechanism in sequence-to-sequence learning},
	url = {https://aclanthology.org/P16-1154},
	doi = {10.18653/v1/P16-1154},
	urldate = {2022-02-01},
	booktitle = {Proceedings of the 54th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Gu, Jiatao and Lu, Zhengdong and Li, Hang and Li, Victor O.K.},
	month = aug,
	year = {2016},
	pages = {1631--1640},
}

@inproceedings{grefenstette_learning_2015,
	address = {Cambridge, MA, USA},
	series = {{NIPS}'15},
	title = {Learning to transduce with unbounded memory},
	abstract = {Recently, strong results have been demonstrated by Deep Recurrent Neural Networks on natural language transduction problems. In this paper we explore the representational power of these models using synthetic grammars designed to exhibit phenomena similar to those found in real transduction problems such as machine translation. These experiments lead us to propose new memory-based recurrent networks that implement continuously differentiable analogues of traditional data structures such as Stacks, Queues, and DeQues. We show that these architectures exhibit superior generalisation performance to Deep RNNs and are often able to learn the underlying generating algorithms in our transduction experiments.},
	urldate = {2022-02-01},
	booktitle = {Proceedings of the 28th {International} {Conference} on {Neural} {Information} {Processing} {Systems} - {Volume} 2},
	publisher = {MIT Press},
	author = {Grefenstette, Edward and Hermann, Karl Moritz and Suleyman, Mustafa and Blunsom, Phil},
	month = dec,
	year = {2015},
	pages = {1828--1836},
}

@inproceedings{devlin_bert_2019,
	address = {Minneapolis, Minnesota},
	title = {{BERT}: {Pre}-training of deep bidirectional transformers for language understanding},
	shorttitle = {{BERT}},
	url = {https://aclanthology.org/N19-1423},
	doi = {10.18653/v1/N19-1423},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2022-02-01},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = jun,
	year = {2019},
	pages = {4171--4186},
}

@article{major_effects_2002,
	title = {The effects of nonnative {Accents} on listening comprehension: {Implications} for {ESL} assessment},
	volume = {36},
	issn = {1545-7249},
	shorttitle = {The {Effects} of {Nonnative} {Accents} on {Listening} {Comprehension}},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.2307/3588329},
	doi = {10.2307/3588329},
	abstract = {Tests of ESL listening comprehension attempt to sample from language that reflects what examinees will be expected to comprehend in the relevant contexts, but most listening tests do not address the fact that accented English is a normal part of ESL listening. To begin to explore the issue, this study examined the extent to which native-English-speaking and ESL listeners performed better on a test when the speaker shared their native language. Four groups of 100 listeners, whose native languages were Chinese, Japanese, Spanish, and standard American English, heard brief lectures presented in English by speakers with different native languages and answered questions based on the lectures. The results indicated that both native and nonnative listeners scored significantly lower on listening comprehension tests when they listened to nonnative speakers of English, native speakers of Spanish scored significantly higher when listening to Spanish-accented speech, and native speakers of Chinese scored significantly lower when listening to speakers who shared their native language.},
	language = {en},
	number = {2},
	urldate = {2022-01-24},
	journal = {TESOL Quarterly},
	author = {Major, Roy C. and Fitzmaurice, Susan F. and Bunta, Ferenc and Balasubramanian, Chandrika},
	year = {2002},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.2307/3588329},
	pages = {173--190},
}

@article{strauber_replicability_2021,
	title = {Replicability of neural responses to speech accent is driven by study design and analytical parameters},
	volume = {11},
	copyright = {2021 The Author(s)},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/s41598-021-82782-4},
	doi = {10.1038/s41598-021-82782-4},
	abstract = {Recent studies have reported evidence that listeners' brains process meaning differently in speech with an in-group as compared to an out-group accent. However, among studies that have used electroencephalography (EEG) to examine neural correlates of semantic processing of speech in different accents, the details of findings are often in conflict, potentially reflecting critical variations in experimental design and/or data analysis parameters. To determine which of these factors might be driving inconsistencies in results across studies, we systematically investigate how analysis parameter sets from several of these studies impact results obtained from our own EEG data set. Data were collected from forty-nine monolingual North American English listeners in an event-related potential (ERP) paradigm as they listened to semantically congruent and incongruent sentences spoken in an American accent and an Indian accent. Several key effects of in-group as compared to out-group accent were robust across the range of parameters found in the literature, including more negative scalp-wide responses to incongruence in the N400 range, more positive posterior responses to congruence in the N400 range, and more positive posterior responses to incongruence in the P600 range. These findings, however, are not fully consistent with the reported observations of the studies whose parameters we used, indicating variation in experimental design may be at play. Other reported effects only emerged under a subset of the analytical parameters tested, suggesting that analytical parameters also drive differences. We hope this spurs discussion of analytical parameters and investigation of the contributions of individual study design variables in this growing field.},
	language = {en},
	number = {1},
	urldate = {2022-01-24},
	journal = {Scientific Reports},
	author = {Strauber, C. Benjamin and Ali, Lestat R. and Fujioka, Takako and Thille, Candace and McCandliss, Bruce D.},
	month = feb,
	year = {2021},
	note = {Bandiera\_abtest: a
Cc\_license\_type: cc\_by
Cg\_type: Nature Research Journals
Number: 1
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Language;Perception
Subject\_term\_id: language;perception},
	keywords = {Language, Perception},
	pages = {4777},
}

@article{schoffelen_204-subject_2019,
	title = {A 204-subject multimodal neuroimaging dataset to study language processing},
	volume = {6},
	copyright = {2019 The Author(s)},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/s41597-019-0020-y},
	doi = {10.1038/s41597-019-0020-y},
	abstract = {This dataset, colloquially known as the Mother Of Unification Studies (MOUS) dataset, contains multimodal neuroimaging data that has been acquired from 204 healthy human subjects. The neuroimaging protocol consisted of magnetic resonance imaging (MRI) to derive information at high spatial resolution about brain anatomy and structural connections, and functional data during task, and at rest. In addition, magnetoencephalography (MEG) was used to obtain high temporal resolution electrophysiological measurements during task, and at rest. All subjects performed a language task, during which they processed linguistic utterances that either consisted of normal or scrambled sentences. Half of the subjects were reading the stimuli, the other half listened to the stimuli. The resting state measurements consisted of 5 minutes eyes-open for the MEG and 7 minutes eyes-closed for fMRI. The neuroimaging data, as well as the information about the experimental events are shared according to the Brain Imaging Data Structure (BIDS) format. This unprecedented neuroimaging language data collection allows for the investigation of various aspects of the neurobiological correlates of language.},
	language = {en},
	number = {1},
	urldate = {2022-01-24},
	journal = {Scientific Data},
	author = {Schoffelen, Jan-Mathijs and Oostenveld, Robert and Lam, Nietzsche H. L. and Uddén, Julia and Hultén, Annika and Hagoort, Peter},
	month = apr,
	year = {2019},
	note = {Bandiera\_abtest: a
Cc\_license\_type: cc\_publicdomain
Cg\_type: Nature Research Journals
Number: 1
Primary\_atype: Research
Publisher: Nature Publishing Group
Subject\_term: Electrophysiology;Functional magnetic resonance imaging;Language;Psychology
Subject\_term\_id: electrophysiology;functional-magnetic-resonance-imaging;language;psychology},
	keywords = {Language, Electrophysiology, Functional magnetic resonance imaging, Psychology},
	pages = {17},
}

@article{nastase_narratives_2021,
	title = {The “{Narratives}” {fMRI} dataset for evaluating models of naturalistic language comprehension},
	volume = {8},
	issn = {2052-4463},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC8479122/},
	doi = {10.1038/s41597-021-01033-3},
	abstract = {The “Narratives” collection aggregates a variety of functional MRI datasets collected while human subjects listened to naturalistic spoken stories. The current release includes 345 subjects, 891 functional scans, and 27 diverse stories of varying duration totaling {\textasciitilde}4.6 hours of unique stimuli ({\textasciitilde}43,000 words). This data collection is well-suited for naturalistic neuroimaging analysis, and is intended to serve as a benchmark for models of language and narrative comprehension. We provide standardized MRI data accompanied by rich metadata, preprocessed versions of the data ready for immediate use, and the spoken story stimuli with time-stamped phoneme- and word-level transcripts. All code and data are publicly available with full provenance in keeping with current best practices in transparent and reproducible neuroimaging., 
Measurement(s)functional brain measurementTechnology Type(s)functional magnetic resonance imagingFactor Type(s)story • native languageSample Characteristic - OrganismHomo sapiens
, Machine-accessible metadata file describing the reported data: 10.6084/m9.figshare.14818587},
	urldate = {2022-01-24},
	journal = {Scientific Data},
	author = {Nastase, Samuel A. and Liu, Yun-Fei and Hillman, Hanna and Zadbood, Asieh and Hasenfratz, Liat and Keshavarzian, Neggin and Chen, Janice and Honey, Christopher J. and Yeshurun, Yaara and Regev, Mor and Nguyen, Mai and Chang, Claire H. C. and Baldassano, Christopher and Lositsky, Olga and Simony, Erez and Chow, Michael A. and Leong, Yuan Chang and Brooks, Paula P. and Micciche, Emily and Choe, Gina and Goldstein, Ariel and Vanderwal, Tamara and Halchenko, Yaroslav O. and Norman, Kenneth A. and Hasson, Uri},
	month = sep,
	year = {2021},
	pmid = {34584100},
	pmcid = {PMC8479122},
	pages = {250},
}

@article{waskom_seaborn_2021,
	title = {seaborn: statistical data visualization},
	volume = {6},
	url = {https://doi.org/10.21105/joss.03021},
	doi = {10.21105/joss.03021},
	number = {60},
	journal = {Journal of Open Source Software},
	author = {Waskom, Michael L.},
	year = {2021},
	note = {Publisher: The Open Journal},
	pages = {3021},
}

@article{hasson_future_2012,
	title = {Future trends in neuroimaging: neural processes as expressed within real-life contexts},
	volume = {62},
	doi = {10.1016/j.neuroimage.2012.02.004},
	abstract = {Human neuroscience research has changed dramatically with the proliferation and refinement of functional magnetic resonance imaging (fMRI) technologies. The early years of the technique were largely devoted to methods development and validation, and to the coarse-grained mapping of functional topographies. This paper will cover three emerging trends that we believe will be central to fMRI research in the coming decade. In the first section of this paper, we argue in favor of a shift from fine-grained functional labeling toward the characterization of underlying neural processes. In the second section, we examine three methodological developments that have improved our ability to characterize these neural processes using fMRI. In the last section, we highlight the trend towards more ecologically valid fMRI experiments, which engage neural circuits in real life conditions. We note that many of our cognitive faculties emerge from interpersonal interactions, and that a complete understanding of the cognitive processes within a single individual's brain cannot be achieved without understanding the interactions among individuals. Looking forward to the future of human fMRI, we conclude that the major constraint on new discoveries will not be related to the spatiotemporal resolution of the BOLD signal, which is constantly improving, but rather to the precision of our hypotheses and the creativity of our methods for testing them. ?? 2012 Elsevier Inc.},
	number = {2},
	journal = {NeuroImage},
	author = {Hasson, Uri and Honey, Christopher J.},
	year = {2012},
	note = {tex.mendeley-tags: cognitive neuroscience,fMRI},
	keywords = {cognitive neuroscience, fMRI},
	pages = {1272--1278},
}

@article{nastase_keep_2020,
	title = {Keep it real: rethinking the primacy of experimental control in cognitive neuroscience},
	volume = {222},
	issn = {10538119},
	shorttitle = {Keep it real},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1053811920307400},
	doi = {10.1016/j.neuroimage.2020.117254},
	language = {en},
	urldate = {2022-01-23},
	journal = {NeuroImage},
	author = {Nastase, Samuel A. and Goldstein, Ariel and Hasson, Uri},
	month = nov,
	year = {2020},
	pages = {117254},
}

@article{ritter_cognitive_2017-1,
	title = {Cognitive psychology for deep neural networks: {A} shape bias case study},
	shorttitle = {Cognitive {Psychology} for {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1706.08606},
	abstract = {Deep neural networks (DNNs) have achieved unprecedented performance on a wide range of complex tasks, rapidly outpacing our understanding of the nature of their solutions. This has caused a recent surge of interest in methods for rendering modern neural systems more interpretable. In this work, we propose to address the interpretability problem in modern DNNs using the rich history of problem descriptions, theories and experimental methods developed by cognitive psychologists to study the human mind. To explore the potential value of these tools, we chose a well-established analysis from developmental psychology that explains how children learn word labels for objects, and applied that analysis to DNNs. Using datasets of stimuli inspired by the original cognitive psychology experiments, we find that state-of-the-art one shot learning models trained on ImageNet exhibit a similar bias to that observed in humans: they prefer to categorize objects according to shape rather than color. The magnitude of this shape bias varies greatly among architecturally identical, but differently seeded models, and even fluctuates within seeds throughout training, despite nearly equivalent classification performance. These results demonstrate the capability of tools from cognitive psychology for exposing hidden computational properties of DNNs, while concurrently providing us with a computational model for human word learning.},
	urldate = {2022-01-19},
	journal = {arXiv:1706.08606 [cs, stat]},
	author = {Ritter, Samuel and Barrett, David G. T. and Santoro, Adam and Botvinick, Matt M.},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.08606},
}

@inproceedings{futrell_neural_2019,
	address = {Minneapolis, Minnesota},
	title = {Neural language models as psycholinguistic subjects: {Representations} of syntactic state},
	shorttitle = {Neural language models as psycholinguistic subjects},
	url = {https://aclanthology.org/N19-1004},
	doi = {10.18653/v1/N19-1004},
	abstract = {We investigate the extent to which the behavior of neural network language models reflects incremental representations of syntactic state. To do so, we employ experimental methodologies which were originally developed in the field of psycholinguistics to study syntactic representation in the human mind. We examine neural network model behavior on sets of artificial sentences containing a variety of syntactically complex structures. These sentences not only test whether the networks have a representation of syntactic state, they also reveal the specific lexical cues that networks use to update these states. We test four models: two publicly available LSTM sequence models of English (Jozefowicz et al., 2016; Gulordava et al., 2018) trained on large datasets; an RNN Grammar (Dyer et al., 2016) trained on a small, parsed dataset; and an LSTM trained on the same small corpus as the RNNG. We find evidence for basic syntactic state representations in all models, but only the models trained on large datasets are sensitive to subtle lexical cues signaling changes in syntactic state.},
	urldate = {2022-01-19},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Futrell, Richard and Wilcox, Ethan and Morita, Takashi and Qian, Peng and Ballesteros, Miguel and Levy, Roger},
	month = jun,
	year = {2019},
	pages = {32--42},
}

@article{waskom_seaborn_2021-1,
	title = {seaborn: statistical data visualization},
	volume = {6},
	issn = {2475-9066},
	shorttitle = {seaborn},
	url = {https://joss.theoj.org/papers/10.21105/joss.03021},
	doi = {10.21105/joss.03021},
	abstract = {Waskom, M. L., (2021). seaborn: statistical data visualization. Journal of Open Source Software, 6(60), 3021, https://doi.org/10.21105/joss.03021},
	language = {en},
	number = {60},
	urldate = {2022-01-18},
	journal = {Journal of Open Source Software},
	author = {Waskom, Michael L.},
	month = apr,
	year = {2021},
	pages = {3021},
}

@inproceedings{skrlj_exploring_2021,
	address = {Online},
	title = {Exploring neural language models via analysis of local and global self-attention spaces},
	url = {https://aclanthology.org/2021.hackashop-1.11},
	abstract = {Large pretrained language models using the transformer neural network architecture are becoming a dominant methodology for many natural language processing tasks, such as question answering, text classification, word sense disambiguation, text completion and machine translation. Commonly comprising hundreds of millions of parameters, these models offer state-of-the-art performance, but at the expense of interpretability. The attention mechanism is the main component of transformer networks. We present AttViz, a method for exploration of self-attention in transformer networks, which can help in explanation and debugging of the trained models by showing associations between text tokens in an input sequence. We show that existing deep learning pipelines can be explored with AttViz, which offers novel visualizations of the attention heads and their aggregations. We implemented the proposed methods in an online toolkit and an offline library. Using examples from news analysis, we demonstrate how AttViz can be used to inspect and potentially better understand what a model has learned.},
	urldate = {2021-12-14},
	booktitle = {Proceedings of the {EACL} {Hackashop} on {News} {Media} {Content} {Analysis} and {Automated} {Report} {Generation}},
	publisher = {Association for Computational Linguistics},
	author = {Škrlj, Blaž and Sheehan, Shane and Eržen, Nika and Robnik-Šikonja, Marko and Luz, Saturnino and Pollak, Senja},
	month = apr,
	year = {2021},
	pages = {76--83},
	file = {Full Text PDF:/Users/kriarm/Zotero/storage/9K4STY35/Škrlj et al. - 2021 - Exploring Neural Language Models via Analysis of L.pdf:application/pdf},
}

@article{miok_ban_2021,
	title = {To {BAN} or not to {BAN}: {Bayesian} attention networks for reliable hate speech detection},
	issn = {1866-9956, 1866-9964},
	shorttitle = {To {BAN} or {Not} to {BAN}},
	url = {http://link.springer.com/10.1007/s12559-021-09826-9},
	doi = {10.1007/s12559-021-09826-9},
	abstract = {Abstract
            Hate speech is an important problem in the management of user-generated content. To remove offensive content or ban misbehaving users, content moderators need reliable hate speech detectors. Recently, deep neural networks based on the transformer architecture, such as the (multilingual) BERT model, have achieved superior performance in many natural language classification tasks, including hate speech detection. So far, these methods have not been able to quantify their output in terms of reliability. We propose a Bayesian method using Monte Carlo dropout within the attention layers of the transformer models to provide well-calibrated reliability estimates. We evaluate and visualize the results of the proposed approach on hate speech detection problems in several languages. Additionally, we test whether affective dimensions can enhance the information extracted by the BERT model in hate speech classification. Our experiments show that Monte Carlo dropout provides a viable mechanism for reliability estimation in transformer networks. Used within the BERT model, it offers state-of-the-art classification performance and can detect less trusted predictions.},
	language = {en},
	urldate = {2021-12-14},
	journal = {Cognitive Computation},
	author = {Miok, Kristian and Škrlj, Blaž and Zaharie, Daniela and Robnik-Šikonja, Marko},
	month = jan,
	year = {2021},
	file = {Full Text:/Users/kriarm/Zotero/storage/QZ6H85JX/Miok et al. - 2021 - To BAN or Not to BAN Bayesian Attention Networks .pdf:application/pdf},
}

@article{pelicon_investigating_2021,
	title = {Investigating cross-lingual training for offensive language detection},
	volume = {7},
	issn = {2376-5992},
	url = {https://peerj.com/articles/cs-559},
	doi = {10.7717/peerj-cs.559},
	abstract = {Platforms that feature user-generated content (social media, online forums, newspaper comment sections etc.) have to detect and filter offensive speech within large, fast-changing datasets. While many automatic methods have been proposed and achieve good accuracies, most of these focus on the English language, and are hard to apply directly to languages in which few labeled datasets exist. Recent work has therefore investigated the use of
              cross-lingual transfer learning
              to solve this problem, training a model in a well-resourced language and transferring to a less-resourced target language; but performance has so far been significantly less impressive. In this paper, we investigate the reasons for this performance drop, via a systematic comparison of pre-trained models and intermediate training regimes on five different languages. We show that using a better pre-trained language model results in a large gain in overall performance and in zero-shot transfer, and that intermediate training on other languages is effective when little target-language data is available. We then use multiple analyses of classifier confidence and language model vocabulary to shed light on exactly where these gains come from and gain insight into the sources of the most typical mistakes.},
	language = {en},
	urldate = {2021-12-14},
	journal = {PeerJ Computer Science},
	author = {Pelicon, Andraž and Shekhar, Ravi and Škrlj, Blaž and Purver, Matthew and Pollak, Senja},
	month = jun,
	year = {2021},
	pages = {e559},
}

@article{martinc_tnt-kid_2021,
	title = {{TNT}-{KID}: {Transformer}-based neural tagger for keyword identification},
	issn = {1351-3249, 1469-8110},
	shorttitle = {{TNT}-{KID}},
	url = {https://www.cambridge.org/core/product/identifier/S1351324921000127/type/journal_article},
	doi = {10.1017/S1351324921000127},
	abstract = {Abstract
            With growing amounts of available textual data, development of algorithms capable of automatic analysis, categorization, and summarization of these data has become a necessity. In this research, we present a novel algorithm for keyword identification, that is, an extraction of one or multiword phrases representing key aspects of a given document, called Transformer-Based Neural Tagger for Keyword IDentification (TNT-KID). By adapting the transformer architecture for a specific task at hand and leveraging language model pretraining on a domain-specific corpus, the model is capable of overcoming deficiencies of both supervised and unsupervised state-of-the-art approaches to keyword extraction by offering competitive and robust performance on a variety of different datasets while requiring only a fraction of manually labeled data required by the best-performing systems. This study also offers thorough error analysis with valuable insights into the inner workings of the model and an ablation study measuring the influence of specific components of the keyword identification workflow on the overall performance.},
	language = {en},
	urldate = {2021-12-14},
	journal = {Natural Language Engineering},
	author = {Martinc, Matej and Škrlj, Blaž and Pollak, Senja},
	month = jun,
	year = {2021},
	pages = {1--40},
	file = {Full Text:/Users/kriarm/Zotero/storage/8SEFXV2P/Martinc et al. - 2021 - TNT-KID Transformer-based neural tagger for keywo.pdf:application/pdf},
}

@inproceedings{martinc_leveraging_2020,
	address = {Marseille, France},
	title = {Leveraging contextual embeddings for detecting diachronic semantic shift},
	isbn = {979-10-95546-34-4},
	url = {https://aclanthology.org/2020.lrec-1.592},
	abstract = {We propose a new method that leverages contextual embeddings for the task of diachronic semantic shift detection by generating time specific word representations from BERT embeddings. The results of our experiments in the domain specific LiverpoolFC corpus suggest that the proposed method has performance comparable to the current state-of-the-art without requiring any time consuming domain adaptation on large corpora. The results on the newly created Brexit news corpus suggest that the method can be successfully used for the detection of a short-term yearly semantic shift. And lastly, the model also shows promising results in a multilingual settings, where the task was to detect differences and similarities between diachronic semantic shifts in different languages.},
	language = {English},
	urldate = {2021-12-14},
	booktitle = {Proceedings of the 12th {Language} {Resources} and {Evaluation} {Conference}},
	publisher = {European Language Resources Association},
	author = {Martinc, Matej and Kralj Novak, Petra and Pollak, Senja},
	month = may,
	year = {2020},
	pages = {4811--4819},
}

@article{martinc_supervised_2021,
	title = {Supervised and unsupervised neural approaches to text readability},
	volume = {47},
	issn = {0891-2017, 1530-9312},
	url = {https://direct.mit.edu/coli/article/47/1/141/97334/Supervised-and-Unsupervised-Neural-Approaches-to},
	doi = {10.1162/coli_a_00398},
	abstract = {Abstract
            We present a set of novel neural supervised and unsupervised approaches for determining the readability of documents. In the unsupervised setting, we leverage neural language models, whereas in the supervised setting, three different neural classification architectures are tested. We show that the proposed neural unsupervised approach is robust, transferable across languages, and allows adaptation to a specific readability task and data set. By systematic comparison of several neural architectures on a number of benchmark and new labeled readability data sets in two languages, this study also offers a comprehensive analysis of different neural approaches to readability classification. We expose their strengths and weaknesses, compare their performance to current state-of-the-art classification approaches to readability, which in most cases still rely on extensive feature engineering, and propose possibilities for improvements.},
	language = {en},
	number = {1},
	urldate = {2021-12-13},
	journal = {Computational Linguistics},
	author = {Martinc, Matej and Pollak, Senja and Robnik-Šikonja, Marko},
	month = apr,
	year = {2021},
	pages = {141--179},
	file = {Full Text:/Users/kriarm/Zotero/storage/IU2NJ26G/Martinc et al. - 2021 - Supervised and Unsupervised Neural Approaches to T.pdf:application/pdf},
}

@article{raji_ai_2021,
	title = {{AI} and the everything in the whole wide world benchmark},
	url = {http://arxiv.org/abs/2111.15366},
	abstract = {There is a tendency across different subfields in AI to valorize a small collection of influential benchmarks. These benchmarks operate as stand-ins for a range of anointed common problems that are frequently framed as foundational milestones on the path towards flexible and generalizable AI systems. State-of-the-art performance on these benchmarks is widely understood as indicative of progress towards these long-term goals. In this position paper, we explore the limits of such benchmarks in order to reveal the construct validity issues in their framing as the functionally "general" broad measures of progress they are set up to be.},
	urldate = {2021-12-08},
	journal = {arXiv:2111.15366 [cs]},
	author = {Raji, Inioluwa Deborah and Bender, Emily M. and Paullada, Amandalynne and Denton, Emily and Hanna, Alex},
	month = nov,
	year = {2021},
	note = {arXiv: 2111.15366},
	keywords = {AI ethics, benchmark, GLUE, image recognition, imageNet, language model},
	file = {arXiv Fulltext PDF:/Users/kriarm/Zotero/storage/BXHRDADZ/Raji et al. - 2021 - AI and the Everything in the Whole Wide World Benc.pdf:application/pdf;arXiv.org Snapshot:/Users/kriarm/Zotero/storage/XXYQTJTA/2111.html:text/html},
}

@article{whittaker_steep_2021,
	title = {The steep cost of capture},
	volume = {28},
	issn = {1072-5520},
	url = {https://doi.org/10.1145/3488666},
	doi = {10.1145/3488666},
	number = {6},
	urldate = {2021-11-24},
	journal = {Interactions},
	author = {Whittaker, Meredith},
	month = nov,
	year = {2021},
	keywords = {AI ethics, big tech, capitalism, GPT-3, NLP},
	pages = {50--55},
}

@article{merity_regularizing_2017,
	title = {Regularizing and optimizing {LSTM} language models},
	url = {http://arxiv.org/abs/1708.02182},
	abstract = {Recurrent neural networks (RNNs), such as long short-term memory networks (LSTMs), serve as a fundamental building block for many sequence learning tasks, including machine translation, language modeling, and question answering. In this paper, we consider the specific problem of word-level language modeling and investigate strategies for regularizing and optimizing LSTM-based models. We propose the weight-dropped LSTM which uses DropConnect on hidden-to-hidden weights as a form of recurrent regularization. Further, we introduce NT-ASGD, a variant of the averaged stochastic gradient method, wherein the averaging trigger is determined using a non-monotonic condition as opposed to being tuned by the user. Using these and other regularization strategies, we achieve state-of-the-art word level perplexities on two data sets: 57.3 on Penn Treebank and 65.8 on WikiText-2. In exploring the effectiveness of a neural cache in conjunction with our proposed model, we achieve an even lower state-of-the-art perplexity of 52.8 on Penn Treebank and 52.0 on WikiText-2.},
	urldate = {2021-11-22},
	journal = {arXiv:1708.02182 [cs]},
	author = {Merity, Stephen and Keskar, Nitish Shirish and Socher, Richard},
	month = aug,
	year = {2017},
	note = {arXiv: 1708.02182},
	keywords = {NLP, gradient descent, LM, LSTM},
}

@article{ferreira_good_2007,
	title = {The ‘{Good} {Enough}’ approach to language comprehension},
	volume = {1},
	issn = {1749818X},
	shorttitle = {The ‘{Good} {Enough}’ {Approach} to {Language} {Comprehension}},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1749-818X.2007.00007.x},
	doi = {10.1111/j.1749-818X.2007.00007.x},
	language = {en},
	number = {1-2},
	urldate = {2021-11-18},
	journal = {Language and Linguistics Compass},
	author = {Ferreira, Fernanda and Patson, Nikole D.},
	month = mar,
	year = {2007},
	pages = {71--83},
}

@article{gu_incorporating_2016-1,
	title = {Incorporating copying mechanism in sequence-to-sequence learning},
	url = {http://arxiv.org/abs/1603.06393},
	abstract = {We address an important problem in sequence-to-sequence (Seq2Seq) learning referred to as copying, in which certain segments in the input sequence are selectively replicated in the output sequence. A similar phenomenon is observable in human language communication. For example, humans tend to repeat entity names or even long phrases in conversation. The challenge with regard to copying in Seq2Seq is that new machinery is needed to decide when to perform the operation. In this paper, we incorporate copying into neural network-based Seq2Seq learning and propose a new model called CopyNet with encoder-decoder structure. CopyNet can nicely integrate the regular way of word generation in the decoder with the new copying mechanism which can choose sub-sequences in the input sequence and put them at proper places in the output sequence. Our empirical study on both synthetic data sets and real world data sets demonstrates the efficacy of CopyNet. For example, CopyNet can outperform regular RNN-based model with remarkable margins on text summarization tasks.},
	urldate = {2021-11-18},
	journal = {arXiv:1603.06393 [cs]},
	author = {Gu, Jiatao and Lu, Zhengdong and Li, Hang and Li, Victor O. K.},
	month = jun,
	year = {2016},
	note = {arXiv: 1603.06393},
}

@inproceedings{saxe_random_2011,
	address = {Madison, WI, USA},
	series = {{ICML}'11},
	title = {On random weights and unsupervised feature learning},
	isbn = {978-1-4503-0619-5},
	abstract = {Recently two anomalous results in the literature have shown that certain feature learning architectures can yield useful features for object recognition tasks even with untrained, random weights. In this paper we pose the question: why do random weights sometimes do so well? Our answer is that certain convolutional pooling architectures can be inherently frequency selective and translation invariant, even with random weights. Based on this we demonstrate the viability of extremely fast architecture search by using random weights to evaluate candidate architectures, thereby sidestepping the time-consuming learning process. We then show that a surprising fraction of the performance of certain state-of-the-art methods can be attributed to the architecture alone.},
	booktitle = {Proceedings of the 28th {International} {Conference} on {International} {Conference} on {Machine} {Learning}},
	publisher = {Omnipress},
	author = {Saxe, Andrew M. and Koh, Pang Wei and Chen, Zhenghao and Bhand, Maneesh and Suresh, Bipin and Ng, Andrew Y.},
	year = {2011},
	note = {event-place: Bellevue, Washington, USA},
	pages = {1089--1096},
}

@article{calvo_natural_2017,
	title = {Natural language processing in mental health applications using non-clinical texts},
	volume = {23},
	issn = {1351-3249, 1469-8110},
	url = {https://www.cambridge.org/core/journals/natural-language-engineering/article/natural-language-processing-in-mental-health-applications-using-nonclinical-texts/32645FFCFD37C67DA62CA06DB66EB2F4},
	doi = {10.1017/S1351324916000383},
	abstract = {Natural language processing (NLP) techniques can be used to make inferences about peoples’ mental states from what they write on Facebook, Twitter and other social media. These inferences can then be used to create online pathways to direct people to health information and assistance and also to generate personalized interventions. Regrettably, the computational methods used to collect, process and utilize online writing data, as well as the evaluations of these techniques, are still dispersed in the literature. This paper provides a taxonomy of data sources and techniques that have been used for mental health support and intervention. Specifically, we review how social media and other data sources have been used to detect emotions and identify people who may be in need of psychological assistance; the computational techniques used in labeling and diagnosis; and finally, we discuss ways to generate and personalize mental health interventions. The overarching aim of this scoping review is to highlight areas of research where NLP has been applied in the mental health literature and to help develop a common language that draws together the fields of mental health, human-computer interaction and NLP.},
	language = {en},
	number = {5},
	urldate = {2021-11-16},
	journal = {Natural Language Engineering},
	author = {Calvo, Rafael A. and Milne, David N. and Hussain, M. Sazzad and Christensen, Helen},
	month = sep,
	year = {2017},
	note = {Publisher: Cambridge University Press},
	keywords = {NLP, mental health, twitter},
	pages = {649--685},
}

@techreport{stephen_latent_2021,
	type = {preprint},
	title = {Latent neural dynamics encode temporal context in speech},
	url = {http://biorxiv.org/lookup/doi/10.1101/2021.07.15.452519},
	abstract = {Abstract
          Direct neural recordings from human auditory cortex have demonstrated encoding for acoustic-phonetic features of consonants and vowels. Neural responses also encode distinct acoustic amplitude cues related to timing, such as those that occur at the onset of a sentence after a silent period or the onset of the vowel in each syllable. Here, we used a group reduced rank regression model to show that distributed cortical responses support a low-dimensional latent state representation of temporal context in speech. The timing cues each capture more unique variance than all other phonetic features and exhibit rotational or cyclical dynamics in latent space from activity that is widespread over the superior temporal gyrus. We propose that these spatially distributed timing signals could serve to provide temporal context for, and possibly bind across time, the concurrent processing of individual phonetic features, to compose higher-order phonological (e.g. word-level) representations.},
	language = {en},
	urldate = {2021-11-10},
	institution = {Neuroscience},
	author = {Stephen, Emily P and Li, Yuanning and Metzger, Sean and Oganian, Yulia and Chang, Edward F},
	month = jul,
	year = {2021},
	doi = {10.1101/2021.07.15.452519},
	file = {Submitted Version:/Users/kriarm/Zotero/storage/YKSRTKYW/Stephen et al. - 2021 - Latent neural dynamics encode temporal context in .pdf:application/pdf},
}

@misc{talat_word_2021,
	title = {A {Word} on {Machine} {Ethics}: {A} {Response} to {Jiang} et al. (2021)},
	author = {Talat\${\textasciicircum}*\$, Zeerak and Blix\${\textasciicircum}*\$, Hagen and Valvoda, Josef and Indira Ganesh, Maya and Cotterell, Ryan and Williams, Adina},
	year = {2021},
	keywords = {language model, NLP, LM, Delphi, deontics, machine ethics, moral judgments, morality},
}

@misc{honea_when_nodate,
	title = {When dams cause more problems than they solve, removing them can pay off for people and nature},
	url = {http://theconversation.com/when-dams-cause-more-problems-than-they-solve-removing-them-can-pay-off-for-people-and-nature-137346},
	abstract = {Thousands of dams across the US are aging and overdue for maintenance. Taking them down can revive rivers, restore fish runs and create new opportunities for tourism and outdoor activities.},
	language = {en},
	urldate = {2021-10-23},
	journal = {The Conversation},
	author = {Honea, Jon},
	file = {Snapshot:/Users/kriarm/Zotero/storage/BTEZPLXD/when-dams-cause-more-problems-than-they-solve-removing-them-can-pay-off-for-people-and-nature-1.html:text/html},
}

@techreport{uria-martinez_us_2021,
	title = {U.{S}. {Hydropower} {Market} {Report}},
	url = {https://www.energy.gov/eere/water/downloads/us-hydropower-market-report},
	abstract = {Report combines data from public and commercial sources to provide a comprehensive picture of developments in hydropower, PSH, and industry trends.},
	language = {en},
	urldate = {2021-10-23},
	institution = {Department of Energy},
	author = {Uría-Martínez, Rocío and Johnson, Megan M. and Rui, Shan},
	collaborator = {Samu, Nicole M. and Oladosu, Gbadebo and Werble, Joseph M. and Battey, Hoyt},
	month = jan,
	year = {2021},
	file = {Snapshot:/Users/kriarm/Zotero/storage/JRUDFBJM/us-hydropower-market-report.html:text/html},
}

@techreport{yang_next-generation_2021,
	title = {Next-generation of recurrent neural network models for cognition},
	url = {https://psyarxiv.com/w34n2/},
	abstract = {Recurrent Neural Networks (RNNs) trained with machine learning techniques on cognitive tasks have become a widely accepted tool for neuroscientists. In this short opinion piece, we discuss fundamental challenges faced by early work of this approach, and recent steps to overcome such challenges and build next-generation RNN models for cognition. We propose several essential questions that practitioners of this approach should address to continue building future generations of RNN models.},
	language = {en-us},
	urldate = {2021-10-19},
	institution = {PsyArXiv},
	author = {Yang, Guangyu Robert and Mazon, Manuel Molano},
	month = apr,
	year = {2021},
	doi = {10.31234/osf.io/w34n2},
	note = {type: article},
	keywords = {biological plausibility, BPTT, evolution, hierarchy, learning, RNN, RNN interpretability},
}

@inproceedings{misra_exploring_2020,
	address = {Online},
	title = {Exploring {BERT}'s sensitivity to lexical cues using tests from semantic priming},
	url = {https://aclanthology.org/2020.findings-emnlp.415},
	doi = {10.18653/v1/2020.findings-emnlp.415},
	abstract = {Models trained to estimate word probabilities in context have become ubiquitous in natural language processing. How do these models use lexical cues in context to inform their word probabilities? To answer this question, we present a case study analyzing the pre-trained BERT model with tests informed by semantic priming. Using English lexical stimuli that show priming in humans, we find that BERT too shows “priming”, predicting a word with greater probability when the context includes a related word versus an unrelated one. This effect decreases as the amount of information provided by the context increases. Follow-up analysis shows BERT to be increasingly distracted by related prime words as context becomes more informative, assigning lower probabilities to related words. Our findings highlight the importance of considering contextual constraint effects when studying word prediction in these models, and highlight possible parallels with human processing.},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2020},
	publisher = {Association for Computational Linguistics},
	author = {Misra, Kanishka and Ettinger, Allyson and Rayz, Julia},
	month = nov,
	year = {2020},
	keywords = {NLP, LM, BERT, N400, priming, semantic priming},
	pages = {4625--4635},
}

@article{bommasani_opportunities_2021,
	title = {On the opportunities and risks of foundation models},
	url = {http://arxiv.org/abs/2108.07258},
	abstract = {AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.},
	urldate = {2021-10-18},
	journal = {arXiv:2108.07258 [cs]},
	author = {Bommasani, Rishi and Hudson, Drew A. and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S. and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and Brynjolfsson, Erik and Buch, Shyamal and Card, Dallas and Castellon, Rodrigo and Chatterji, Niladri and Chen, Annie and Creel, Kathleen and Davis, Jared Quincy and Demszky, Dora and Donahue, Chris and Doumbouya, Moussa and Durmus, Esin and Ermon, Stefano and Etchemendy, John and Ethayarajh, Kawin and Fei-Fei, Li and Finn, Chelsea and Gale, Trevor and Gillespie, Lauren and Goel, Karan and Goodman, Noah and Grossman, Shelby and Guha, Neel and Hashimoto, Tatsunori and Henderson, Peter and Hewitt, John and Ho, Daniel E. and Hong, Jenny and Hsu, Kyle and Huang, Jing and Icard, Thomas and Jain, Saahil and Jurafsky, Dan and Kalluri, Pratyusha and Karamcheti, Siddharth and Keeling, Geoff and Khani, Fereshte and Khattab, Omar and Koh, Pang Wei and Krass, Mark and Krishna, Ranjay and Kuditipudi, Rohith and Kumar, Ananya and Ladhak, Faisal and Lee, Mina and Lee, Tony and Leskovec, Jure and Levent, Isabelle and Li, Xiang Lisa and Li, Xuechen and Ma, Tengyu and Malik, Ali and Manning, Christopher D. and Mirchandani, Suvir and Mitchell, Eric and Munyikwa, Zanele and Nair, Suraj and Narayan, Avanika and Narayanan, Deepak and Newman, Ben and Nie, Allen and Niebles, Juan Carlos and Nilforoshan, Hamed and Nyarko, Julian and Ogut, Giray and Orr, Laurel and Papadimitriou, Isabel and Park, Joon Sung and Piech, Chris and Portelance, Eva and Potts, Christopher and Raghunathan, Aditi and Reich, Rob and Ren, Hongyu and Rong, Frieda and Roohani, Yusuf and Ruiz, Camilo and Ryan, Jack and Ré, Christopher and Sadigh, Dorsa and Sagawa, Shiori and Santhanam, Keshav and Shih, Andy and Srinivasan, Krishnan and Tamkin, Alex and Taori, Rohan and Thomas, Armin W. and Tramèr, Florian and Wang, Rose E. and Wang, William and Wu, Bohan and Wu, Jiajun and Wu, Yuhuai and Xie, Sang Michael and Yasunaga, Michihiro and You, Jiaxuan and Zaharia, Matei and Zhang, Michael and Zhang, Tianyi and Zhang, Xikun and Zhang, Yuhui and Zheng, Lucia and Zhou, Kaitlyn and Liang, Percy},
	month = aug,
	year = {2021},
	note = {arXiv: 2108.07258},
	keywords = {AI ethics, NLP, LM, transfer learning},
	file = {arXiv Fulltext PDF:/Users/kriarm/Zotero/storage/4I9WNBBQ/Bommasani et al. - 2021 - On the Opportunities and Risks of Foundation Model.pdf:application/pdf;arXiv.org Snapshot:/Users/kriarm/Zotero/storage/JZJYLDIP/2108.html:text/html},
}

@article{ba_using_2016-1,
	title = {Using fast weights to attend to the recent past},
	url = {http://arxiv.org/abs/1610.06258},
	abstract = {Until recently, research on artificial neural networks was largely restricted to systems with only two types of variable: Neural activities that represent the current or recent input and weights that learn to capture regularities among inputs, outputs and payoffs. There is no good reason for this restriction. Synapses have dynamics at many different time-scales and this suggests that artificial neural networks might benefit from variables that change slower than activities but much faster than the standard weights. These "fast weights" can be used to store temporary memories of the recent past and they provide a neurally plausible way of implementing the type of attention to the past that has recently proved very helpful in sequence-to-sequence models. By using fast weights we can avoid the need to store copies of neural activity patterns.},
	urldate = {2021-09-16},
	journal = {arXiv:1610.06258 [cs, stat]},
	author = {Ba, Jimmy and Hinton, Geoffrey and Mnih, Volodymyr and Leibo, Joel Z. and Ionescu, Catalin},
	month = dec,
	year = {2016},
	note = {arXiv: 1610.06258},
	keywords = {NLP, LM, LSTM, RNN, attention, fast weights, working memory},
	file = {arXiv.org Snapshot:/Users/kriarm/Zotero/storage/PG65INQX/1610.html:text/html},
}

@article{michaelov_different_2021,
	title = {Different kinds of cognitive plausibility: why are transformers better than {RNNs} at predicting {N400} amplitude?},
	shorttitle = {Different kinds of cognitive plausibility},
	url = {http://arxiv.org/abs/2107.09648},
	abstract = {Despite being designed for performance rather than cognitive plausibility, transformer language models have been found to be better at predicting metrics used to assess human language comprehension than language models with other architectures, such as recurrent neural networks. Based on how well they predict the N400, a neural signal associated with processing difficulty, we propose and provide evidence for one possible explanation - their predictions are affected by the preceding context in a way analogous to the effect of semantic facilitation in humans.},
	urldate = {2021-10-18},
	journal = {arXiv:2107.09648 [cs]},
	author = {Michaelov, James A. and Bardolph, Megan D. and Coulson, Seana and Bergen, Benjamin K.},
	month = jul,
	year = {2021},
	note = {arXiv: 2107.09648},
	keywords = {LSTM, N400, working memory, GPT-2, JRNN, WM},
}

@article{moradi_gpt-3_2021,
	title = {{GPT}-3 {Models} are poor few-shot learners in the biomedical domain},
	url = {http://arxiv.org/abs/2109.02555},
	abstract = {Deep neural language models have set new breakthroughs in many tasks of Natural Language Processing (NLP). Recent work has shown that deep transformer language models (pretrained on large amounts of texts) can achieve high levels of task-specific few-shot performance comparable to state-of-the-art models. However, the ability of these large language models in few-shot transfer learning has not yet been explored in the biomedical domain. We investigated the performance of two powerful transformer language models, i.e. GPT-3 and BioBERT, in few-shot settings on various biomedical NLP tasks. The experimental results showed that, to a great extent, both the models underperform a language model fine-tuned on the full training data. Although GPT-3 had already achieved near state-of-the-art results in few-shot knowledge transfer on open-domain NLP tasks, it could not perform as effectively as BioBERT, which is orders of magnitude smaller than GPT-3. Regarding that BioBERT was already pretrained on large biomedical text corpora, our study suggests that language models may largely benefit from in-domain pretraining in task-specific few-shot learning. However, in-domain pretraining seems not to be sufficient; novel pretraining and few-shot learning strategies are required in the biomedical NLP domain.},
	urldate = {2021-10-18},
	journal = {arXiv:2109.02555 [cs]},
	author = {Moradi, Milad and Blagec, Kathrin and Haberl, Florian and Samwald, Matthias},
	month = sep,
	year = {2021},
	note = {arXiv: 2109.02555},
	keywords = {GPT-3, NLP, bioBERT, bioNLP, few-shot learning},
	file = {arXiv Fulltext PDF:/Users/kriarm/Zotero/storage/IH2MXKGS/Moradi et al. - 2021 - GPT-3 Models are Poor Few-Shot Learners in the Bio.pdf:application/pdf;arXiv.org Snapshot:/Users/kriarm/Zotero/storage/E4WZFLAF/2109.html:text/html},
}

@misc{nematzadeh_memory_2020,
	title = {On memory in human and artificial language processing systems},
	url = {/paper/ON-MEMORY-IN-HUMAN-AND-ARTIFICIAL-LANGUAGE-SYSTEMS-Nematzadeh-Ruder/bcaf0a460e5a2f1ac48902071652d570023219f4},
	abstract = {Memory in humans and artificial intelligence (AI) systems has similar functions— both are responsible for encoding, retrieving, and storing of information. While memory in humans has specialized systems for different functions (e.g., working memory, semantic memory, episodic memory), memory in AI systems is often implicitly represented in the weights of parametric neural networks. Focusing on language processing systems, we argue that this property makes it hard for AI systems to generalize across complex linguistic tasks. We consider the separation of computation and storage as necessary, suggest desired properties of the storage system, and discuss the benefit of integrating different types of human memory (as separate modules) into next-generation language processing systems.},
	language = {en},
	urldate = {2021-04-02},
	author = {Nematzadeh, A. and Ruder, Sebastian and Yogatama, Dani},
	year = {2020},
	file = {Snapshot:/Users/kriarm/Zotero/storage/FI45Y5US/bcaf0a460e5a2f1ac48902071652d570023219f4.html:text/html},
}

@article{macpherson_natural_2021,
	title = {Natural and {Artificial} {Intelligence}: {A} brief introduction to the interplay between {AI} and neuroscience research},
	volume = {144},
	issn = {08936080},
	shorttitle = {Natural and {Artificial} {Intelligence}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0893608021003683},
	doi = {10.1016/j.neunet.2021.09.018},
	language = {en},
	urldate = {2021-10-14},
	journal = {Neural Networks},
	author = {Macpherson, Tom and Churchland, Anne and Sejnowski, Terry and DiCarlo, James and Kamitani, Yukiyasu and Takahashi, Hidehiko and Hikida, Takatoshi},
	month = dec,
	year = {2021},
	keywords = {biological plausibility, RNN, working memory, AI, backprop, spiking neural networks},
	pages = {603--613},
}

@misc{noauthor_using_2021,
	title = {Using {DeepSpeed} and {Megatron} to {Train} {Megatron}-{Turing} {NLG} {530B}, the {World}’s {Largest} and {Most} {Powerful} {Generative} {Language} {Model}},
	url = {https://developer.nvidia.com/blog/using-deepspeed-and-megatron-to-train-megatron-turing-nlg-530b-the-worlds-largest-and-most-powerful-generative-language-model/},
	abstract = {MT-NLG has 3x the number of parameters compared to the existing largest model of this type and demonstrates unmatched accuracy in a broad set of natural language tasks…},
	language = {en-US},
	urldate = {2021-10-11},
	journal = {NVIDIA Developer Blog},
	month = oct,
	year = {2021},
	keywords = {GPT-3, NLP, LM, HANS, natural language generation, NLG, scaling laws},
}

@inproceedings{alkhatib_live_2021,
	address = {Yokohama Japan},
	title = {To live in their utopia: {Why} algorithmic systems create absurd outcomes},
	isbn = {978-1-4503-8096-6},
	shorttitle = {To {Live} in {Their} {Utopia}},
	url = {https://dl.acm.org/doi/10.1145/3411764.3445740},
	doi = {10.1145/3411764.3445740},
	language = {en},
	urldate = {2021-10-08},
	booktitle = {Proceedings of the 2021 {CHI} {Conference} on {Human} {Factors} in {Computing} {Systems}},
	publisher = {ACM},
	author = {Alkhatib, Ali},
	month = may,
	year = {2021},
	keywords = {ethics, Graeber, ML ethics, power},
	pages = {1--9},
}

@book{luck_introduction_2014,
	address = {Cambridge, Massachusetts},
	edition = {Second},
	title = {An introduction to the event-related potential technique},
	isbn = {978-0-262-52585-5},
	publisher = {The MIT Press},
	author = {Luck, Steven J.},
	year = {2014},
	keywords = {EEG, ERP, methods},
}

@article{crosse_multivariate_2016,
	title = {The multivariate temporal response function ({mTRF}) {Toolbox}: {A} {MATLAB} {Toolbox} for relating neural signals to continuous stimuli},
	volume = {10},
	doi = {10.3389/fnhum.2016.00604},
	abstract = {Understanding how brains process sensory signals in natural environments is one of the key goals of 21st century neuroscience. While brain imaging and invasive electrophysiology will play key roles in this endeavor, there is also an important role to be played by noninvasive, macroscopic techniques with high temporal resolution such as electro- and magnetoencephalography. But challenges exist in determining how best to analyze such complex, time-varying neural responses to complex, time-varying and multivariate natural sensory stimuli. There has been a long history of applying system identification techniques to relate the firing activity of neurons to complex sensory stimuli and such techniques are now seeing increased application to EEG and MEG data. One particular example involves fitting a filter – often referred to as a temporal response function – that describes a mapping between some feature(s) of a sensory stimulus and the neural response. Here, we first briefly review the history of these system identification approaches and describe a specific technique for deriving temporal response functions known as regularized linear regression. We then introduce a new open-source toolbox for performing this analysis. We describe how it can be used to derive (multivariate) temporal response functions describing a mapping between stimulus and response in both directions. We also explain the importance of regularizing the analysis and how this regularization can be optimized for a particular dataset. We then outline specifically how the toolbox implements these analyses and provide several examples of the types of results that the toolbox can produce. Finally, we consider some of the limitations of the toolbox and opportunities for future development and application.},
	journal = {Frontiers in Human Neuroscience},
	author = {Crosse, Michael J. and Di Liberto, Giovanni M. and Bednar, Adam and Lalor, Edmund C.},
	month = nov,
	year = {2016},
	keywords = {matlab, project.lstmMEG, project.streams, readme, trf},
	pages = {604},
}

@article{caucheteux_language_2020,
	title = {Language processing in brains and deep neural networks: computational convergence and its limits},
	copyright = {© 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	shorttitle = {Language processing in brains and deep neural networks},
	url = {https://www.biorxiv.org/content/10.1101/2020.07.03.186288v1},
	doi = {10.1101/2020.07.03.186288},
	abstract = {{\textless}p{\textgreater}Deep learning has recently allowed substantial progress in language tasks such as translation and completion. Do such models process language similarly to humans, and is this similarity driven by systematic structural, functional and learning principles? To address these issues, we tested whether the activations of 7,400 artificial neural networks trained on image, word and sentence processing linearly map onto the hierarchy of human brain responses elicited during a reading task, using source-localized magneto-encephalography (MEG) recordings of one hundred and four subjects. Our results confirm that visual, word and language models sequentially correlate with distinct areas of the left-lateralized cortical hierarchy of reading. However, only specific subsets of these models converge towards brain-like representations during their training. Specifically, when the algorithms are trained on language modeling, their middle layers become increasingly similar to the late responses of the language network in the brain. By contrast, input and output word embedding layers often diverge away from brain activity during training. These differences are primarily rooted in the sustained and bilateral responses of the temporal and frontal cortices. Together, these results suggest that the compositional - but not the lexical - representations of modern language models converge to a brain-like solution.{\textless}/p{\textgreater}},
	language = {en},
	urldate = {2020-07-10},
	journal = {bioRxiv},
	author = {Caucheteux, Charlotte and King, Jean-Rémi},
	month = jul,
	year = {2020},
	note = {Publisher: Cold Spring Harbor Laboratory
Section: New Results},
	keywords = {MEG, CNN, ecog-nlp, transformer, word2vec},
	pages = {2020.07.03.186288},
}

@article{fyshe_interpretable_2014,
	title = {Interpretable semantic vectors from a joint model of brain- and text-based meaning},
	volume = {2014},
	issn = {0736-587X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4497373/},
	abstract = {Vector space models (VSMs) represent word meanings as points in a high dimensional space. VSMs are typically created using a large text corpora, and so represent word semantics as observed in text. We present a new algorithm (JNNSE) that can incorporate a measure of semantics not previously used to create VSMs: brain activation data recorded while people read words. The resulting model takes advantage of the complementary strengths and weaknesses of corpus and brain activation data to give a more complete representation of semantics. Evaluations show that the model 1) matches a behavioral measure of semantics more closely, 2) can be used to predict corpus data for unseen words and 3) has predictive power that generalizes across brain imaging technologies and across subjects. We believe that the model is thus a more faithful representation of mental vocabularies.},
	urldate = {2018-02-20},
	journal = {Proceedings of the Conference of Association for Computational Linguistics. Meeting},
	author = {Fyshe, Alona and Talukdar, Partha P and Murphy, Brian and Mitchell, Tom M},
	month = jun,
	year = {2014},
	pmid = {26166940},
	pmcid = {PMC4497373},
	keywords = {fMRI, project.lstmMEG, readme, distributional semantics, project.vsmMEG, VSM},
	pages = {489--499},
}

@incollection{paszke_pytorch_2019,
	title = {{PyTorch}: {An} imperative style, high-performance deep learning library},
	url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
	publisher = {Curran Associates, Inc.},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d' and Fox, E. and Garnett, R.},
	year = {2019},
	pages = {8024--8035},
}

@article{schmidhuber_learning_1992,
	title = {Learning to control fast-weight memories: {An} alternative to dynamic recurrent networks},
	volume = {4},
	issn = {0899-7667, 1530-888X},
	shorttitle = {Learning to {Control} {Fast}-{Weight} {Memories}},
	url = {https://direct.mit.edu/neco/article/4/1/131-139/5620},
	doi = {10.1162/neco.1992.4.1.131},
	abstract = {Previous algorithms for supervised sequence learning are based on dynamic recurrent networks. This paper describes an alternative class of gradient-based systems consisting of two feedforward nets that learn to deal with temporal sequences using fast weights: The first net learns to produce context-dependent weight changes for the second net whose weights may vary very quickly. The method offers the potential for STM storage efficiency: A single weight (instead of a full-fledged unit) may be sufficient for storing temporal information. Various learning methods are derived. Two experiments with unknown time delays illustrate the approach. One experiment shows how the system can be used for adaptive temporary variable binding.},
	language = {en},
	number = {1},
	urldate = {2021-09-16},
	journal = {Neural Computation},
	author = {Schmidhuber, Jürgen},
	month = jan,
	year = {1992},
	keywords = {NLP, LSTM, fast weights},
	pages = {131--139},
}

@inproceedings{schlag_linear_2021,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Linear transformers are secretly fast weight programmers},
	volume = {139},
	url = {https://proceedings.mlr.press/v139/schlag21a.html},
	abstract = {We show the formal equivalence of linearised self-attention mechanisms and fast weight controllers from the early ’90s, where a slow neural net learns by gradient descent to program the fast weights of another net through sequences of elementary programming instructions which are additive outer products of self-invented activation patterns (today called keys and values). Such Fast Weight Programmers (FWPs) learn to manipulate the contents of a finite memory and dynamically interact with it. We infer a memory capacity limitation of recent linearised softmax attention variants, and replace the purely additive outer products by a delta rule-like programming instruction, such that the FWP can more easily learn to correct the current mapping from keys to values. The FWP also learns to compute dynamically changing learning rates. We also propose a new kernel function to linearise attention which balances simplicity and effectiveness. We conduct experiments on synthetic retrieval problems as well as standard machine translation and language modelling tasks which demonstrate the benefits of our methods.},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Schlag, Imanol and Irie, Kazuki and Schmidhuber, Jürgen},
	editor = {Meila, Marina and Zhang, Tong},
	month = jul,
	year = {2021},
	keywords = {NLP, LSTM, RNN, fast weights, meta-learning},
	pages = {9355--9366},
}

@article{white_value_1980,
	title = {The value of narrativity in the representation of reality},
	volume = {7},
	issn = {0093-1896, 1539-7858},
	url = {https://www.journals.uchicago.edu/doi/10.1086/448086},
	doi = {10.1086/448086},
	language = {en},
	number = {1},
	urldate = {2021-08-30},
	journal = {Critical Inquiry},
	author = {White, Hayden},
	month = oct,
	year = {1980},
	pages = {5--27},
}

@unpublished{radford_language_2019,
	title = {Language models are unsupervised multitask learners},
	url = {https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf},
	author = {Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
	year = {2019},
}

@article{humphries_strong_2021,
	title = {Strong and weak principles of neural dimension reduction},
	url = {http://arxiv.org/abs/2011.08088},
	abstract = {If spikes are the medium, what is the message? Answering that question is driving the development of large-scale, single neuron resolution recordings from behaving animals, on the scale of thousands of neurons. But these data are inherently high-dimensional, with as many dimensions as neurons - so how do we make sense of them? For many the answer is to reduce the number of dimensions. Here I argue we can distinguish weak and strong principles of neural dimension reduction. The weak principle is that dimension reduction is a convenient tool for making sense of complex neural data. The strong principle is that dimension reduction shows us how neural circuits actually operate and compute. Elucidating these principles is crucial, for which we subscribe to provides radically different interpretations of the same neural activity data. I show how we could make either the weak or strong principles appear to be true based on innocuous looking decisions about how we use dimension reduction on our data. To counteract these confounds, I outline the experimental evidence for the strong principle that do not come from dimension reduction; but also show there are a number of neural phenomena that the strong principle fails to address. To reconcile these conflicting data, I suggest that the brain has both principles at play.},
	urldate = {2021-08-19},
	journal = {arXiv:2011.08088 [q-bio]},
	author = {Humphries, Mark D.},
	month = may,
	year = {2021},
	note = {arXiv: 2011.08088},
	keywords = {RNN, dimensionality reduction, dynamical system, manifold, PCA},
	file = {arXiv.org Snapshot:/Users/kriarm/Zotero/storage/ZY5WEV8Z/2011.html:text/html},
}

@misc{sutton_bitter_2019,
	title = {The {Bitter} {Lesson}},
	url = {http://www.incompleteideas.net/IncIdeas/BitterLesson.html},
	urldate = {2021-07-15},
	journal = {Incomplete Ideas},
	author = {Sutton, Rich},
	month = mar,
	year = {2019},
	keywords = {NLP, scaling laws, DeepMind, Moore's law},
	file = {The Bitter Lesson:/Users/kriarm/Zotero/storage/V4NZP4H3/BitterLesson.html:text/html},
}

@article{futrell_lossycontext_2020,
	title = {Lossy‐context {Surprisal}: {An} information‐theoretic model of memory effects in sentence processing},
	volume = {44},
	issn = {0364-0213, 1551-6709},
	shorttitle = {Lossy‐{Context} {Surprisal}},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/cogs.12814},
	doi = {10.1111/cogs.12814},
	language = {en},
	number = {3},
	urldate = {2021-07-14},
	journal = {Cognitive Science},
	author = {Futrell, Richard and Gibson, Edward and Levy, Roger P.},
	month = mar,
	year = {2020},
	file = {Full Text:/Users/kriarm/Zotero/storage/GWJN9WN8/Futrell et al. - 2020 - Lossy‐Context Surprisal An Information‐Theoretic .pdf:application/pdf},
}

@article{chien_mapping_2021,
	title = {Mapping the timescale organization of neural language models},
	url = {http://arxiv.org/abs/2012.06717},
	abstract = {In the human brain, sequences of language input are processed within a distributed and hierarchical architecture, in which higher stages of processing encode contextual information over longer timescales. In contrast, in recurrent neural networks which perform natural language processing, we know little about how the multiple timescales of contextual information are functionally organized. Therefore, we applied tools developed in neuroscience to map the "processing timescales" of individual units within a word-level LSTM language model. This timescale-mapping method assigned long timescales to units previously found to track long-range syntactic dependencies. Additionally, the mapping revealed a small subset of the network (less than 15\% of units) with long timescales and whose function had not previously been explored. We next probed the functional organization of the network by examining the relationship between the processing timescale of units and their network connectivity. We identified two classes of long-timescale units: "controller" units composed a densely interconnected subnetwork and strongly projected to the rest of the network, while "integrator" units showed the longest timescales in the network, and expressed projection profiles closer to the mean projection profile. Ablating integrator and controller units affected model performance at different positions within a sentence, suggesting distinctive functions of these two sets of units. Finally, we tested the generalization of these results to a character-level LSTM model and models with different architectures. In summary, we demonstrated a model-free technique for mapping the timescale organization in recurrent neural networks, and we applied this method to reveal the timescale and functional organization of neural language models.},
	urldate = {2021-07-13},
	journal = {arXiv:2012.06717 [cs]},
	author = {Chien, Hsiang-Yun Sherry and Zhang, Jinhan and Honey, Christopher J.},
	month = mar,
	year = {2021},
	note = {arXiv: 2012.06717},
}

@article{buonomano_state-dependent_2009,
	title = {State-dependent computations: spatiotemporal processing in cortical networks},
	volume = {10},
	issn = {1471-003X, 1471-0048},
	shorttitle = {State-dependent computations},
	url = {http://www.nature.com/articles/nrn2558},
	doi = {10.1038/nrn2558},
	language = {en},
	number = {2},
	urldate = {2021-07-12},
	journal = {Nature Reviews Neuroscience},
	author = {Buonomano, Dean V. and Maass, Wolfgang},
	month = feb,
	year = {2009},
	pages = {113--125},
}

@article{chen_evaluating_2021,
	title = {Evaluating large language models trained on code},
	url = {http://arxiv.org/abs/2107.03374},
	abstract = {We introduce Codex, a GPT language model fine-tuned on publicly available code from GitHub, and study its Python code-writing capabilities. A distinct production version of Codex powers GitHub Copilot. On HumanEval, a new evaluation set we release to measure functional correctness for synthesizing programs from docstrings, our model solves 28.8\% of the problems, while GPT-3 solves 0\% and GPT-J solves 11.4\%. Furthermore, we find that repeated sampling from the model is a surprisingly effective strategy for producing working solutions to difficult prompts. Using this method, we solve 70.2\% of our problems with 100 samples per problem. Careful investigation of our model reveals its limitations, including difficulty with docstrings describing long chains of operations and with binding operations to variables. Finally, we discuss the potential broader impacts of deploying powerful code generation technologies, covering safety, security, and economics.},
	urldate = {2021-07-09},
	journal = {arXiv:2107.03374 [cs]},
	author = {Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Ponde, Henrique and Kaplan, Jared and Edwards, Harri and Burda, Yura and Joseph, Nicholas and Brockman, Greg and Ray, Alex and Puri, Raul and Krueger, Gretchen and Petrov, Michael and Khlaaf, Heidy and Sastry, Girish and Mishkin, Pamela and Chan, Brooke and Gray, Scott and Ryder, Nick and Pavlov, Mikhail and Power, Alethea and Kaiser, Lukasz and Bavarian, Mohammad and Winter, Clemens and Tillet, Philippe and Such, Felipe and Cummings, Dave and Plappert, Matthias and Chantzis, Fotios and Barnes, Elizabeth and Herbert-Voss, Ariel and Guss, Will and Nichol, Alex and Babuschkin, Igor and Balaji, Suchir and Jain, Shantanu and Carr, Andrew and Leike, Jan and Achiam, Josh and Misra, Vedant and Morikawa, Evan and Radford, Alec and Knight, Matthew and Brundage, Miles and Murati, Mira and Mayer, Katie and Welinder, Peter and McGrew, Bob and Amodei, Dario and McCandlish, Sam and Sutskever, Ilya and Zaremba, Wojciech},
	month = jul,
	year = {2021},
	note = {arXiv: 2107.03374},
	keywords = {GPT-3, NLG, BLEU, code generation, copyright, github},
	file = {arXiv.org Snapshot:/Users/kriarm/Zotero/storage/JA8AFM9P/2107.html:text/html},
}

@article{keator_towards_2013,
	title = {Towards structured sharing of raw and derived neuroimaging data across existing resources},
	volume = {82},
	issn = {1053-8119},
	url = {http://www.sciencedirect.com/science/article/pii/S105381191300596X},
	doi = {10.1016/j.neuroimage.2013.05.094},
	abstract = {Data sharing efforts increasingly contribute to the acceleration of scientific discovery. Neuroimaging data is accumulating in distributed domain-specific databases and there is currently no integrated access mechanism nor an accepted format for the critically important meta-data that is necessary for making use of the combined, available neuroimaging data. In this manuscript, we present work from the Derived Data Working Group, an open-access group sponsored by the Biomedical Informatics Research Network (BIRN) and the International Neuroimaging Coordinating Facility (INCF) focused on practical tools for distributed access to neuroimaging data. The working group develops models and tools facilitating the structured interchange of neuroimaging meta-data and is making progress towards a unified set of tools for such data and meta-data exchange. We report on the key components required for integrated access to raw and derived neuroimaging data as well as associated meta-data and provenance across neuroimaging resources. The components include (1) a structured terminology that provides semantic context to data, (2) a formal data model for neuroimaging with robust tracking of data provenance, (3) a web service-based application programming interface (API) that provides a consistent mechanism to access and query the data model, and (4) a provenance library that can be used for the extraction of provenance data by image analysts and imaging software developers. We believe that the framework and set of tools outlined in this manuscript have great potential for solving many of the issues the neuroimaging community faces when sharing raw and derived neuroimaging data across the various existing database systems for the purpose of accelerating scientific discovery.},
	language = {en},
	urldate = {2020-08-29},
	journal = {NeuroImage},
	author = {Keator, D. B. and Helmer, K. and Steffener, J. and Turner, J. A. and Van Erp, T. G. M. and Gadde, S. and Ashish, N. and Burns, G. A. and Nichols, B. N.},
	month = nov,
	year = {2013},
	pages = {647--661},
}

@article{kaplan_scaling_2020,
	title = {Scaling laws for neural language models},
	url = {http://arxiv.org/abs/2001.08361},
	abstract = {We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.},
	urldate = {2021-05-23},
	journal = {arXiv:2001.08361 [cs, stat]},
	author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
	month = jan,
	year = {2020},
	note = {arXiv: 2001.08361},
}

@article{grefenstette_learning_2015-1,
	title = {Learning to transduce with unbounded memory},
	url = {http://arxiv.org/abs/1506.02516},
	abstract = {Recently, strong results have been demonstrated by Deep Recurrent Neural Networks on natural language transduction problems. In this paper we explore the representational power of these models using synthetic grammars designed to exhibit phenomena similar to those found in real transduction problems such as machine translation. These experiments lead us to propose new memory-based recurrent networks that implement continuously differentiable analogues of traditional data structures such as Stacks, Queues, and DeQues. We show that these architectures exhibit superior generalisation performance to Deep RNNs and are often able to learn the underlying generating algorithms in our transduction experiments.},
	urldate = {2021-05-23},
	journal = {arXiv:1506.02516 [cs]},
	author = {Grefenstette, Edward and Hermann, Karl Moritz and Suleyman, Mustafa and Blunsom, Phil},
	month = nov,
	year = {2015},
	note = {arXiv: 1506.02516},
}

@article{sukhbaatar_end--end_2015,
	title = {End-to-end memory networks},
	url = {http://arxiv.org/abs/1503.08895},
	abstract = {We introduce a neural network with a recurrent attention model over a possibly large external memory. The architecture is a form of Memory Network (Weston et al., 2015) but unlike the model in that work, it is trained end-to-end, and hence requires significantly less supervision during training, making it more generally applicable in realistic settings. It can also be seen as an extension of RNNsearch to the case where multiple computational steps (hops) are performed per output symbol. The flexibility of the model allows us to apply it to tasks as diverse as (synthetic) question answering and to language modeling. For the former our approach is competitive with Memory Networks, but with less supervision. For the latter, on the Penn TreeBank and Text8 datasets our approach demonstrates comparable performance to RNNs and LSTMs. In both cases we show that the key concept of multiple computational hops yields improved results.},
	urldate = {2021-04-05},
	journal = {arXiv:1503.08895 [cs]},
	author = {Sukhbaatar, Sainbayar and Szlam, Arthur and Weston, Jason and Fergus, Rob},
	month = nov,
	year = {2015},
	note = {arXiv: 1503.08895},
}

@article{mitchell_why_2021,
	title = {Why {AI} is harder than we think},
	url = {http://arxiv.org/abs/2104.12871},
	abstract = {Since its beginning in the 1950s, the field of artificial intelligence has cycled several times between periods of optimistic predictions and massive investment ("AI spring") and periods of disappointment, loss of confidence, and reduced funding ("AI winter"). Even with today's seemingly fast pace of AI breakthroughs, the development of long-promised technologies such as self-driving cars, housekeeping robots, and conversational companions has turned out to be much harder than many people expected. One reason for these repeating cycles is our limited understanding of the nature and complexity of intelligence itself. In this paper I describe four fallacies in common assumptions made by AI researchers, which can lead to overconfident predictions about the field. I conclude by discussing the open questions spurred by these fallacies, including the age-old challenge of imbuing machines with humanlike common sense.},
	urldate = {2021-04-28},
	journal = {arXiv:2104.12871 [cs]},
	author = {Mitchell, Melanie},
	month = apr,
	year = {2021},
	note = {arXiv: 2104.12871},
	file = {arXiv Fulltext PDF:/Users/kriarm/Zotero/storage/IAJZRGCA/Mitchell - 2021 - Why AI is Harder Than We Think.pdf:application/pdf;arXiv.org Snapshot:/Users/kriarm/Zotero/storage/X9KDKHY7/2104.html:text/html},
}

@article{merity_pointer_2016-1,
	title = {Pointer sentinel mixture models},
	url = {http://arxiv.org/abs/1609.07843},
	abstract = {Recent neural network sequence models with softmax classifiers have achieved their best language modeling performance only with very large hidden states and large vocabularies. Even then they struggle to predict rare or unseen words even if the context makes the prediction unambiguous. We introduce the pointer sentinel mixture architecture for neural sequence models which has the ability to either reproduce a word from the recent context or produce a word from a standard softmax classifier. Our pointer sentinel-LSTM model achieves state of the art language modeling performance on the Penn Treebank (70.9 perplexity) while using far fewer parameters than a standard softmax LSTM. In order to evaluate how well language models can exploit longer contexts and deal with more realistic vocabularies and larger corpora we also introduce the freely available WikiText corpus.},
	urldate = {2021-05-26},
	journal = {arXiv:1609.07843 [cs]},
	author = {Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
	month = sep,
	year = {2016},
	note = {arXiv: 1609.07843},
}

@article{jaeger_categorical_2008,
	title = {Categorical data analysis: {Away} from {ANOVAs} (transformation or not) and towards logit mixed models},
	volume = {59},
	issn = {0749596X},
	shorttitle = {Categorical data analysis},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0749596X07001337},
	doi = {10.1016/j.jml.2007.11.007},
	language = {en},
	number = {4},
	urldate = {2021-07-08},
	journal = {Journal of Memory and Language},
	author = {Jaeger, T. Florian},
	month = nov,
	year = {2008},
	keywords = {methods, ANOVA, linear mixed models, statistics},
	pages = {434--446},
	file = {Accepted Version:/Users/kriarm/Zotero/storage/FVRAUKWF/Jaeger - 2008 - Categorical data analysis Away from ANOVAs (trans.pdf:application/pdf},
}

@techreport{norman-haignere_multiscale_2020,
	type = {preprint},
	title = {Multiscale integration organizes hierarchical computation in human auditory cortex},
	url = {http://biorxiv.org/lookup/doi/10.1101/2020.09.30.321687},
	abstract = {Abstract
          To derive meaning from sound, the brain must integrate information across tens (e.g. phonemes) to hundreds (e.g. words) of milliseconds, but the neural computations that enable multiscale integration remain unclear. Prior evidence suggests that human auditory cortex analyzes sound using both generic acoustic features (e.g. spectrotemporal modulation) and category-specific computations, but how these putatively distinct computations integrate temporal information is unknown. To answer this question, we developed a novel method to estimate neural integration periods and applied the method to intracranial recordings from human epilepsy patients. We show that integration periods increase three-fold as one ascends the auditory cortical hierarchy. Moreover, we find that electrodes with short integration periods ({\textasciitilde}50-150 ms) respond selectively to spectrotemporal modulations, while electrodes with long integration periods ({\textasciitilde}200-300 ms) show prominent selectivity for sound categories such as speech and music. These findings reveal how multiscale temporal analysis organizes hierarchical computation in human auditory cortex.},
	language = {en},
	urldate = {2021-07-08},
	institution = {Neuroscience},
	author = {Norman-Haignere, Sam V and Long, Laura K. and Devinsky, Orrin and Doyle, Werner and Irobunda, Ifeoma and Merricks, Edward M. and Feldstein, Neil A. and McKhann, Guy M. and Schevon, Catherine A. and Flinker, Adeen and Mesgarani, Nima},
	month = oct,
	year = {2020},
	doi = {10.1101/2020.09.30.321687},
}

@techreport{goldstein_thinking_2020,
	type = {preprint},
	title = {Thinking ahead: spontaneous prediction in context as a keystone of language in humans and machines},
	shorttitle = {Thinking ahead},
	url = {http://biorxiv.org/lookup/doi/10.1101/2020.12.02.403477},
	abstract = {Abstract
          Departing from traditional linguistic models, advances in deep learning have resulted in a new type of predictive (autoregressive) deep language models (DLMs). These models are trained to generate appropriate linguistic responses in a given context using a self-supervised prediction task. We provide empirical evidence that the human brain and autoregressive DLMs share two computational principles: 1) both are engaged in continuous prediction; 2) both represent words as a function of the previous context. Behaviorally, we demonstrate a match between humans and DLM’s next-word predictions given sufficient contextual windows during the processing of a real-life narrative. Neurally, we demonstrate that the brain, like autoregressive DLMs, constantly predicts upcoming words in natural speech, hundreds of milliseconds before they are perceived. Finally, we show that DLM’s contextual embeddings capture the neural representation of context-specific word meaning better than arbitrary or static semantic embeddings. Our findings suggest that autoregressive DLMs provide a novel and biologically feasible computational framework for studying the neural basis of language.},
	language = {en},
	urldate = {2021-07-01},
	institution = {Neuroscience},
	author = {Goldstein, Ariel and Zada, Zaid and Buchnik, Eliav and Schain, Mariano and Price, Amy and Aubrey, Bobbi and Nastase, Samuel A. and Feder, Amir and Emanuel, Dotan and Cohen, Alon and Jansen, Aren and Gazula, Harshvardhan and Choe, Gina and Rao, Aditi and Kim, Catherine and Casto, Colton and Fanda, Lora and Doyle, Werner and Friedman, Daniel and Dugan, Patricia and Reichart, Roi and Devore, Sasha and Flinker, Adeen and Hasenfratz, Liat and Hassidim, Avinatan and Brenner, Michael and Matias, Yossi and Norman, Kenneth A. and Devinsky, Orrin and Hasson, Uri},
	month = dec,
	year = {2020},
	doi = {10.1101/2020.12.02.403477},
}

@inproceedings{sculley_machine_2014,
	title = {Machine learning: {The} high interest credit card of technical debt},
	booktitle = {{SE4ML}: {Software} {Engineering} for {Machine} {Learning} ({NIPS} 2014 {Workshop})},
	author = {Sculley, D. and Holt, Gary and Golovin, Daniel and Davydov, Eugene and Phillips, Todd and Ebner, Dietmar and Chaudhary, Vinay and Young, Michael},
	year = {2014},
	keywords = {ML, technical debt},
}

@techreport{caucheteux_gpt-2s_2021,
	type = {preprint},
	title = {{GPT}-2’s activations predict the degree of semantic comprehension in the human brain},
	url = {http://biorxiv.org/lookup/doi/10.1101/2021.04.20.440622},
	abstract = {Language transformers, like GPT-2, have demonstrated remarkable abilities to process text, and now constitute the backbone of deep translation, summarization and dialogue algorithms. However, whether these models encode information that relates to human comprehension remains controversial. Here, we show that the representations of GPT-2 not only map onto the brain responses to spoken stories, but also predict the extent to which subjects understand the narratives. To this end, we analyze 101 subjects recorded with functional Magnetic Resonance Imaging while listening to 70 min of short stories. We then fit a linear model to predict brain activity from GPT-2 activations, and correlate this mapping with subjects’ comprehension scores as assessed for each story. The results show that GPT-2’s brain predictions significantly correlate with semantic comprehension. These effects are bilaterally distributed in the language network and peak with a correlation above 30\% in the infero-frontal and medio-temporal gyri as well as in the superior frontal cortex, the planum temporale and the precuneus. Overall, this study shows how comparing deep language models and the brain paves the way to a computational model of semantic comprehension.},
	language = {en},
	urldate = {2021-06-25},
	institution = {Neuroscience},
	author = {Caucheteux, Charlotte and Gramfort, Alexandre and King, Jean-Rémi},
	month = apr,
	year = {2021},
	doi = {10.1101/2021.04.20.440622},
}

@article{musch_transformation_2020,
	title = {Transformation of speech sequences in human sensorimotor circuits},
	volume = {117},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1910939117},
	doi = {10.1073/pnas.1910939117},
	abstract = {After we listen to a series of words, we can silently replay them in our mind. Does this mental replay involve a reactivation of our original perceptual dynamics? We recorded electrocorticographic (ECoG) activity across the lateral cerebral cortex as people heard and then mentally rehearsed spoken sentences. For each region, we tested whether silent rehearsal of sentences involved reactivation of sentence-specific representations established during perception or transformation to a distinct representation. In sensorimotor and premotor cortex, we observed reliable and temporally precise responses to speech; these patterns transformed to distinct sentence-specific representations during mental rehearsal. In contrast, we observed less reliable and less temporally precise responses in prefrontal and temporoparietal cortex; these higher-order representations, which were sensitive to sentence semantics, were shared across perception and rehearsal of the same sentence. The mental rehearsal of natural speech involves the transformation of stimulus-locked speech representations in sensorimotor and premotor cortex, combined with diffuse reactivation of higher-order semantic representations.},
	language = {en},
	number = {6},
	urldate = {2021-06-21},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Müsch, Kathrin and Himberger, Kevin and Tan, Kean Ming and Valiante, Taufik A. and Honey, Christopher J.},
	month = feb,
	year = {2020},
	pages = {3203--3213},
}

@article{gao_neuronal_2020,
	title = {Neuronal timescales are functionally dynamic and shaped by cortical microarchitecture},
	volume = {9},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.61277},
	doi = {10.7554/eLife.61277},
	abstract = {Complex cognitive functions such as working memory and decision-making require information maintenance over seconds to years, from transient sensory stimuli to long-term contextual cues. While theoretical accounts predict the emergence of a corresponding hierarchy of neuronal timescales, direct electrophysiological evidence across the human cortex is lacking. Here, we infer neuronal timescales from invasive intracranial recordings. Timescales increase along the principal sensorimotor-to-association axis across the entire human cortex, and scale with single-unit timescales within macaques. Cortex-wide transcriptomic analysis shows direct alignment between timescales and expression of excitation- and inhibition-related genes, as well as genes specific to voltage-gated transmembrane ion transporters. Finally, neuronal timescales are functionally dynamic: prefrontal cortex timescales expand during working memory maintenance and predict individual performance, while cortex-wide timescales compress with aging. Thus, neuronal timescales follow cytoarchitectonic gradients across the human cortex and are relevant for cognition in both short and long terms, bridging microcircuit physiology with macroscale dynamics and behavior.},
	urldate = {2021-06-17},
	journal = {eLife},
	author = {Gao, Richard and van den Brink, Ruud L and Pfeffer, Thomas and Voytek, Bradley},
	editor = {Vinck, Martin and Colgin, Laura L and Womelsdorf, Thilo},
	month = nov,
	year = {2020},
	note = {Publisher: eLife Sciences Publications, Ltd},
	keywords = {cortical gradients, functional specialization, neuronal timescales, spectral analysis, transcriptomics},
	pages = {e61277},
}

@article{fedorenko_neural_2016,
	title = {Neural correlate of the construction of sentence meaning},
	volume = {113},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1612132113},
	doi = {10.1073/pnas.1612132113},
	abstract = {The neural processes that underlie your ability to read and understand this sentence are unknown. Sentence comprehension occurs very rapidly, and can only be understood at a mechanistic level by discovering the precise sequence of underlying computational and neural events. However, we have no continuous and online neural measure of sentence processing with high spatial and temporal resolution. Here we report just such a measure: intracranial recordings from the surface of the human brain show that neural activity, indexed by γ-power, increases monotonically over the course of a sentence as people read it. This steady increase in activity is absent when people read and remember nonword-lists, despite the higher cognitive demand entailed, ruling out accounts in terms of generic attention, working memory, and cognitive load. Response increases are lower for sentence structure without meaning (“Jabberwocky” sentences) and word meaning without sentence structure (word-lists), showing that this effect is not explained by responses to syntax or word meaning alone. Instead, the full effect is found only for sentences, implicating compositional processes of sentence understanding, a striking and unique feature of human language not shared with animal communication systems. This work opens up new avenues for investigating the sequence of neural events that underlie the construction of linguistic meaning.},
	language = {en},
	number = {41},
	urldate = {2021-06-17},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Fedorenko, Evelina and Scott, Terri L. and Brunner, Peter and Coon, William G. and Pritchett, Brianna and Schalk, Gerwin and Kanwisher, Nancy},
	month = oct,
	year = {2016},
	pages = {E6256--E6262},
}

@article{klein_brain_2017,
	title = {Brain regions as difference-makers},
	volume = {30},
	issn = {0951-5089},
	url = {https://doi.org/10.1080/09515089.2016.1253053},
	doi = {10.1080/09515089.2016.1253053},
	abstract = {Contrastive neuroimaging is often taken to provide evidence about the localization of cognitive functions. After canvassing some problems with this approach, I offer an alternative: neuroimaging gives evidence about regions of the brain that bear difference-making relationships to psychological processes of interest. I distinguish between the specificity and what I call the systematicity of a difference-making relationship, and I show how at least some neuroimaging experiments can give evidence for systematic difference-making.},
	number = {1-2},
	urldate = {2017-12-04},
	journal = {Philosophical Psychology},
	author = {Klein, Colin},
	month = feb,
	year = {2017},
	keywords = {fMRI, causation, evidence, explanation, fixme, neuroimaging, philosophy of neuroscience, philosophy of science, read},
	pages = {1--20},
}

@article{walker_emerging_2015,
	title = {Emerging trends in peer review—a survey},
	volume = {9},
	issn = {1662-453X},
	url = {https://www.frontiersin.org/articles/10.3389/fnins.2015.00169/full},
	doi = {10.3389/fnins.2015.00169},
	abstract = {“Classical peer review” has been subject to intense criticism for slowing down the publication process, bias against specific categories of paper and author, unreliability, inability to detect errors and fraud, unethical practices, and the lack of recognition for unpaid reviewers. This paper surveys innovative forms of peer review that attempt to address these issues. Based on an initial literature review, we construct a sample of 81 channels of scientific communication covering all forms of review identified by the survey and analyze the review mechanisms used by each channel. We identify two major trends: the rapidly expanding role of pre-print servers (e.g. ArXiv) that dispense with traditional peer review altogether and the growth of “non-selective review”, focusing on papers’ scientific quality rather than their perceived importance and novelty. Other potentially important developments include forms of “open review”, which remove reviewer anonymity and interactive review, as well as new mechanisms for post-publication review and out-of-channel reader commentary, especially critical commentary targeting high profile papers. One of the strongest findings of the survey is the persistence of major differences between the peer review processes used by different disciplines. None of these differences is likely to disappear in the foreseeable future. The most likely scenario for the coming years is thus continued diversification, in which different review mechanisms serve different author and reader needs. Relatively little is known about the impact of these innovations on the problems they address. These are important questions for future quantitative research.},
	language = {English},
	urldate = {2017-11-29},
	journal = {Frontiers in Neuroscience},
	author = {Walker, Richard and Rocha da Silva, Pascal},
	year = {2015},
	keywords = {readme, metascience, peer-review},
}

@article{ross-hellauer_what_2017,
	title = {What is open peer review? {A} systematic review},
	volume = {6},
	issn = {2046-1402},
	shorttitle = {What is open peer review?},
	url = {https://f1000research.com/articles/6-588/v2},
	doi = {10.12688/f1000research.11369.2},
	language = {en},
	urldate = {2017-11-28},
	journal = {F1000Research},
	author = {Ross-Hellauer, Tony},
	month = aug,
	year = {2017},
	keywords = {open science, read, metascience, peer-review},
	pages = {588},
	file = {Ross-Hellauer - 2017 - What is open peer review A systematic review.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Ross-Hellauer - 2017 - What is open peer review A systematic review.pdf:application/pdf},
}

@article{tkacik_information_2014,
	title = {Information processing in living systems},
	url = {http://arxiv.org/abs/1412.8752},
	abstract = {Life depends as much on the flow of information as on the flow of energy. Here we review the many efforts to make this intuition precise. Starting with the building blocks of information theory, we explore examples where it has been possible to measure, directly, the flow of information in biological networks, or more generally where information theoretic ideas have been used to guide the analysis of experiments. Systems of interest range from single molecules (the sequence diversity in families of proteins) to groups of organisms (the distribution of velocities in flocks of birds), and all scales in between. Many of these analyses are motivated by the idea that biological systems may have evolved to optimize the gathering and representation of information, and we review the experimental evidence for this optimization, again across a wide range of scales.},
	urldate = {2017-11-27},
	journal = {arXiv:1412.8752 [cond-mat, physics:physics, q-bio]},
	author = {Tkačik, Gašper and Bialek, William},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.8752},
	keywords = {readme, entropy, information, information theory, physics},
}

@article{yeshurun_amplification_2017,
	title = {Amplification of local changes along the timescale processing hierarchy.},
	volume = {114},
	doi = {10.1073/pnas.1701652114},
	abstract = {Small changes in word choice can lead to dramatically different interpretations of narratives. How does the brain accumulate and integrate such local changes to construct unique neural representations for different stories? In this study, we created two distinct narratives by changing only a few words in each sentence (e.g., "he" to "she" or "sobbing" to "laughing") while preserving the grammatical structure across stories. We then measured changes in neural responses between the two stories. We found that differences in neural responses between the two stories gradually increased along the hierarchy of processing timescales. For areas with short integration windows, such as early auditory cortex, the differences in neural responses between the two stories were relatively small. In contrast, in areas with the longest integration windows at the top of the hierarchy, such as the precuneus, temporal parietal junction, and medial frontal cortices, there were large differences in neural responses between stories. Furthermore, this gradual increase in neural differences between the stories was highly correlated with an area's ability to integrate information over time. Amplification of neural differences did not occur when changes in words did not alter the interpretation of the story (e.g., sobbing to "crying"). Our results demonstrate how subtle differences in words are gradually accumulated and amplified along the cortical hierarchy as the brain constructs a narrative over time.},
	number = {35},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Yeshurun, Yaara and Nguyen, Mai and Hasson, Uri},
	month = aug,
	year = {2017},
	pmid = {28811367},
	keywords = {cognitive neuroscience, fMRI, project.lstmMEG, readme, narrative, TRW},
	pages = {9475--9480},
}

@article{yamins_using_2016,
	title = {Using goal-driven deep learning models to understand sensory cortex},
	volume = {19},
	doi = {10.1038/nn.4244},
	abstract = {{\textbackslash}textlessp{\textbackslash}textgreaterRecent computational neuroscience developments have used deep neural networks to model neural responses in higher visual areas. This Perspective describes key algorithmic underpinnings in computer vision and artificial intelligence that have contributed to this progress a\&hellip;{\textbackslash}textless/p{\textbackslash}textgreater},
	number = {3},
	journal = {Nature Neuroscience},
	author = {Yamins, Daniel L.K. and DiCarlo, James J.},
	month = feb,
	year = {2016},
	keywords = {project.lstmMEG, read},
	pages = {356--365},
	file = {Yamins, DiCarlo - 2016 - Using goal-driven deep learning models to understand sensory cortex.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Yamins, DiCarlo - 2016 - Using goal-driven deep learning models to understand sensory cortex.pdf:application/pdf},
}

@article{raichle_brief_2009,
	title = {A brief history of human brain mapping},
	volume = {32},
	issn = {01662236},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0166223608002658},
	doi = {10.1016/j.tins.2008.11.001},
	language = {en},
	number = {2},
	urldate = {2018-12-17},
	journal = {Trends in Neurosciences},
	author = {Raichle, Marcus E.},
	month = feb,
	year = {2009},
	pages = {118--126},
}

@article{chater_probabilistic_2006,
	title = {Probabilistic models of cognition: {Conceptual} foundations},
	volume = {10},
	issn = {13646613},
	shorttitle = {Probabilistic models of cognition},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S136466130600132X},
	doi = {10.1016/j.tics.2006.05.007},
	language = {en},
	number = {7},
	urldate = {2018-12-14},
	journal = {Trends in Cognitive Sciences},
	author = {Chater, Nick and Tenenbaum, Joshua B. and Yuille, Alan},
	month = jul,
	year = {2006},
	keywords = {project.streams, read, probabilistic models of cognition, probability theory},
	pages = {287--291},
}

@article{mamashli_oscillatory_2018,
	title = {Oscillatory dynamics of cortical functional connections in semantic prediction},
	issn = {1065-9471, 1097-0193},
	url = {https://onlinelibrary.wiley.com/doi/abs/10.1002/hbm.24495},
	doi = {10.1002/hbm.24495},
	language = {en},
	urldate = {2018-12-14},
	journal = {Human Brain Mapping},
	author = {Mamashli, Fahimeh and Khan, Sheraz and Obleser, Jonas and Friederici, Angela D. and Maess, Burkhard},
	month = dec,
	year = {2018},
	keywords = {MEG, N400, functional connectivity, project.streams2},
}

@article{boston_parsing_2008,
	title = {Parsing costs as predictors of reading difficulty: {An} evaluation using the {Potsdam} {Sentence} {Corpus}},
	shorttitle = {Parsing costs as predictors of reading difficulty},
	url = {https://bop.unibe.ch/index.php/JEMR/article/view/2255},
	doi = {10.16910/jemr.2.1.1},
	abstract = {The surprisal of a word on a probabilistic grammar constitutes a promising complexity metric for human sentence comprehension difficulty. Using two different grammar types, surprisal is shown to have an effect on fixation durations and regression probabilities in a sample of German readers’ eye movements, the Potsdam Sentence Corpus. A linear mixed-effects model was used to quantify the effect of surprisal while taking into account unigram frequency and bigram frequency (transitional probability), word length, and empirically-derived word predictability; the socalled “early” and “late” measures of processing difficulty both showed an effect of surprisal. Surprisal is also shown to have a small but statistically non-significant effect on empirically-derived predictability itself. This work thus demonstrates the importance of including parsing costs as a predictor of comprehension difficulty in models of reading, and suggests that a simple identification of syntactic parsing costs with early measures and late measures with durations of post-syntactic events may be difficult to uphold.},
	language = {eng},
	urldate = {2018-12-12},
	author = {Boston, Marisa Ferrara and Hale, John and Kliegl, Reinhold and Patil, Umesh and Vasishth, Shravan},
	year = {2008},
	keywords = {project.streams, readme, eye-tracking, surprisal},
}

@article{staub_effect_2015,
	title = {The effect of lexical predictability on eye movements in reading: {Critical} review and theoretical interpretation: predictability and eye movements},
	volume = {9},
	issn = {1749818X},
	shorttitle = {The effect of lexical predictability on eye movements in {Reading}},
	url = {http://doi.wiley.com/10.1111/lnc3.12151},
	doi = {10.1111/lnc3.12151},
	language = {en},
	number = {8},
	urldate = {2018-12-12},
	journal = {Language and Linguistics Compass},
	author = {Staub, Adrian},
	month = aug,
	year = {2015},
	keywords = {project.streams, eye-tracking, prediction, sentence comprehension},
	pages = {311--327},
}

@article{smith_effect_2013,
	title = {The effect of word predictability on reading time is logarithmic},
	volume = {128},
	issn = {00100277},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0010027713000413},
	doi = {10.1016/j.cognition.2013.02.013},
	language = {en},
	number = {3},
	urldate = {2018-12-12},
	journal = {Cognition},
	author = {Smith, Nathaniel J. and Levy, Roger},
	month = sep,
	year = {2013},
	keywords = {project.streams, eye-tracking, sentence comprehension, word probability},
	pages = {302--319},
}

@article{mcdonald_eye_2003,
	title = {Eye movements reveal the on-line computation of lexical probabilities during reading},
	volume = {14},
	issn = {0956-7976, 1467-9280},
	url = {http://journals.sagepub.com/doi/10.1046/j.0956-7976.2003.psci_1480.x},
	doi = {10.1046/j.0956-7976.2003.psci_1480.x},
	language = {en},
	number = {6},
	urldate = {2018-12-12},
	journal = {Psychological Science},
	author = {McDonald, Scott A. and Shillcock, Richard C.},
	month = nov,
	year = {2003},
	keywords = {project.streams, eye-tracking, sentence comprehension},
	pages = {648--652},
}

@article{frank_insensitivity_2011,
	title = {Insensitivity of the human sentence-processing system to hierarchical structure},
	volume = {22},
	issn = {0956-7976, 1467-9280},
	url = {http://journals.sagepub.com/doi/10.1177/0956797611409589},
	doi = {10.1177/0956797611409589},
	language = {en},
	number = {6},
	urldate = {2018-12-12},
	journal = {Psychological Science},
	author = {Frank, Stefan L. and Bod, Rens},
	month = jun,
	year = {2011},
	keywords = {syntax, project.streams, eye-tracking, surprisal, sentence comprehension},
	pages = {829--834},
}

@book{gibson_ecological_2015,
	address = {New York, London},
	title = {The ecological approach to visual perception},
	isbn = {978-1-84872-578-2},
	language = {eng},
	publisher = {Psychology Press},
	author = {Gibson, James J.},
	year = {2015},
	note = {OCLC: 962481298},
}

@incollection{shagrir_marrs_2017,
	title = {Marr's computational level and delineating phenomena},
	isbn = {978-0-19-968550-9},
	url = {http://philsci-archive.pitt.edu/11224/},
	abstract = {A key component of scientific inquiry, especially inquiry devoted to developing mechanistic explanations, is delineating the phenomenon to be explained. The task of delineating phenomena, however, has not been sufficiently analyzed, even by the new mechanistic philosophers of science. We contend that Marr?s characterization of what he called the computational level (CL) provides a valuable resource for understanding what is involved in delineating phenomena. Unfortunately, the distinctive feature of Marr?s computational level, his dual emphasis on both what is computed and why it is computed, has not been appreciated in philosophical discussions of Marr. Accordingly we offer a distinctive account of CL. This then allows us to develop two important points about delineating phenomena. First, the accounts of phenomena that figure in explanatory practice are typically not qualitative but precise, formal or mathematical, representations. Second, delineating phenomena requires consideration of the demands the environment places on the mechanism–identifying, as Marr put it, the basis of the computed function in the world. As valuable as Marr?s account of CL is in characterizing phenomena, we contend that ultimately he did not go far enough. Determining the relevant demands of the environment on the mechanism often requires detailed empirical investigation. Moreover, often phenomena are reconstituted in the course of inquiry on the mechanism itself.},
	booktitle = {Explanation and integration in mind and brain science},
	publisher = {Oxford University Press},
	author = {Shagrir, Oron and Bechtel, William},
	editor = {Kaplan, David M.},
	year = {2017},
	keywords = {readme, computational level, David Marr, delineating phenomena, mechanistic explanation},
	pages = {190--214},
	file = {Shagrir and Bechtel - 2017 - Marr's computational level and delineating phenome.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Shagrir and Bechtel - 2017 - Marr's computational level and delineating phenome.pdf:application/pdf},
}

@book{kaplan_explanation_2017,
	address = {Oxford, New York},
	title = {Explanation and {Integration} in {Mind} and {Brain} {Science}},
	isbn = {2},
	abstract = {This collection brings together a set of new papers that advance the debate concerning the nature of explanation in mind and brain science, and help to clarify the prospects for bonafide integration across these fields. Long a topic of debate among philosophers and scientists alike, there is growing appreciation that understanding the complex relationship between the psychological sciences and the neurosciences, especially how their respective explanatory frameworks interrelate, is of fundamental importance for achieving progress across these scientific domains. Traditional philosophical discussions tend to construe the relationship between them in stark terms - either they are related in terms of complete independence (i.e., autonomy) or complete dependence (i.e., reduction), leaving little room for more interesting relations such as that of mutually beneficial interaction or integration. A unifying thread across the diverse set of contributions to this volume is the rejection of the assumption that no stable middle ground exists between these two extremes, and common embrace of the idea that these sciences are partially dependent on or constrained by one another. By addressing whether the explanatory patterns employed across these domains are similar or different in kind, and to what extent they inform and constrain each another, this volume helps to deepen our understanding of the prospects for successfully integrating mind and brain science.},
	publisher = {Oxford University Press},
	editor = {Kaplan, David M.},
	month = dec,
	year = {2017},
	keywords = {readme, explanation, philosophy of neuroscience, philosophy of science, computation},
	file = {Kaplan - 2017 - Explanation and Integration in Mind and Brain Scie.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Kaplan - 2017 - Explanation and Integration in Mind and Brain Scie.pdf:application/pdf},
}

@phdthesis{bachrach_imaging_2008,
	type = {Thesis},
	title = {Imaging neural correlates of syntactic complexity in a naturalistic context},
	url = {http://hdl.handle.net/1721.1/45900},
	abstract = {The aim of this thesis, and the research project within which it is embedded, is to delineate a neural model of grammatical competence. For this purpose, we develop here a novel integrated, multi-disciplinary experimental paradigm that endorses the fundamental premise of generative grammar, that the study of language is in essence, the study of the mind. We use functional Magnetic Resonance Imaging (fMRI) to monitor brain activation while subjects listen to short narratives. The texts have been written so as to introduce various syntactic complexities (relative clauses, embedded questions, etc.) not usually found (in such density) in actual corpora. We have calculated a number of complexity measures (both at the level of the single word and at that of the phrase) based on current linguistic and psycholinguistic theory and with the use of a computationally implemented probabilistic parser. By correlating these measures with observed brain activity, we are able to identify the different brain networks that support linguistic processing and characterize their particular function. Conversely, we use the rich brain data to inform our cognitive, and linguistic, theory. We report here the neural correlates of surprisal (based on contextual predictions), syntactic complexity, structural ambiguity and disambiguation, Theory of Mind and non-local dependencies. This work made use of novel solutions to compute numerical predictions for these linguistic dimensions, which are often tested only qualitatively, and of a novel parametric fMRI design that allowed for the use of single subject unaveraged data as the dependent variable. The thesis ends with a synthesis of the results in the form of a blue print for a neural model of grammatical competence.},
	language = {eng},
	school = {Massachusetts Institute of Technology},
	author = {Bachrach, Asaf},
	year = {2008},
	keywords = {project.streams},
}

@article{roopun_temporal_2008,
	title = {Temporal interactions between cortical rhythms},
	volume = {2},
	issn = {16624548},
	url = {http://journal.frontiersin.org/article/10.3389/neuro.01.034.2008/abstract},
	doi = {10.3389/neuro.01.034.2008},
	number = {2},
	urldate = {2018-12-06},
	journal = {frontiers in Neuroscience},
	author = {Roopun, Anita K},
	month = dec,
	year = {2008},
	keywords = {project.streams, readme, beta, neural oscillations},
	pages = {145--154},
	file = {Roopun - 2008 - Temporal interactions between cortical rhythms.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Roopun - 2008 - Temporal interactions between cortical rhythms.pdf:application/pdf},
}

@article{hale_finding_2018,
	title = {Finding syntax in human encephalography with beam search},
	url = {http://arxiv.org/abs/1806.04127},
	abstract = {Recurrent neural network grammars (RNNGs) are generative models of (tree,string) pairs that rely on neural networks to evaluate derivational choices. Parsing with them using beam search yields a variety of incremental complexity metrics such as word surprisal and parser action count. When used as regressors against human electrophysiological responses to naturalistic text, they derive two amplitude effects: an early peak and a P600-like later peak. By contrast, a non-syntactic neural language model yields no reliable effects. Model comparisons attribute the early peak to syntactic composition within the RNNG. This pattern of results recommends the RNNG+beam search combination as a mechanistic model of the syntactic processing that occurs during normal human language comprehension.},
	urldate = {2018-06-19},
	journal = {arXiv:1806.04127 [cs]},
	author = {Hale, John and Dyer, Chris and Kuncoro, Adhiguna and Brennan, Jonathan R.},
	month = jun,
	year = {2018},
	note = {arXiv: 1806.04127},
	keywords = {syntax, EEG, project.streams, project.vsmMEG, read, computational linguistics, neural networks, RNNG},
}

@article{rabovsky_simulating_2014,
	title = {Simulating the {N400} {ERP} component as semantic network error: {Insights} from a feature-based connectionist attractor model of word meaning},
	volume = {132},
	issn = {00100277},
	shorttitle = {Simulating the {N400} {ERP} component as semantic network error},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0010027714000493},
	doi = {10.1016/j.cognition.2014.03.010},
	language = {en},
	number = {1},
	urldate = {2018-12-06},
	journal = {Cognition},
	author = {Rabovsky, Milena and McRae, Ken},
	month = jul,
	year = {2014},
	keywords = {N400, project.streams, readme, connectionism},
	pages = {68--89},
}

@article{rabovsky_modelling_2018,
	title = {Modelling the {N400} brain potential as change in a probabilistic representation of meaning},
	volume = {2},
	issn = {2397-3374},
	url = {http://www.nature.com/articles/s41562-018-0406-4},
	doi = {10.1038/s41562-018-0406-4},
	language = {en},
	number = {9},
	urldate = {2018-12-06},
	journal = {Nature Human Behaviour},
	author = {Rabovsky, Milena and Hansen, Steven S. and McClelland, James L.},
	month = sep,
	year = {2018},
	keywords = {N400, project.streams, readme, neural networks, connectionism},
	pages = {693--705},
}

@article{meyer_neural_2018,
	title = {The neural oscillations of speech processing and language comprehension: state of the art and emerging mechanisms},
	volume = {48},
	issn = {0953816X},
	shorttitle = {The neural oscillations of speech processing and language comprehension},
	url = {http://doi.wiley.com/10.1111/ejn.13748},
	doi = {10.1111/ejn.13748},
	language = {en},
	number = {7},
	urldate = {2018-12-03},
	journal = {European Journal of Neuroscience},
	author = {Meyer, Lars},
	month = oct,
	year = {2018},
	keywords = {project.streams, readme, beta, neural oscillations, gamma, language comprehension, theta},
	pages = {2609--2621},
}

@article{chirimuuta_explanation_nodate,
	title = {Explanation in computational neuroscience: causal and non-causal},
	shorttitle = {Explanation in {Computational} {Neuroscience}},
	url = {https://academic.oup.com/bjps/advance-article/doi/10.1093/bjps/axw034/3069966},
	doi = {10.1093/bjps/axw034},
	abstract = {This article examines three candidate cases of non-causal explanation in computational neuroscience. I argue that there are instances of efficient coding explanation that are strongly analogous to examples of non-causal explanation in physics and biology, as presented by Batterman ([2002]), Woodward ([2003]), and Lange ([2013]). By integrating Lange’s and Woodward’s accounts, I offer a new way to elucidate the distinction between causal and non-causal explanation, and to address concerns about the explanatory sufficiency of non-mechanistic models in neuroscience. I also use this framework to shed light on the dispute over the interpretation of dynamical models of the brain. 1 Introduction1.1 Efficient coding explanation in computational neuroscience1.2 Defining non-causal explanation2 Case I: Hybrid Computation3 Case II: The Gabor Model Revisited4 Case III: A Dynamical Model of Prefrontal Cortex4.1 A new explanation of context-dependent computation4.2 Causal or non-causal?5 Causal and Non-causal: Does the Difference Matter?},
	urldate = {2017-11-24},
	journal = {The British Journal for the Philosophy of Science},
	author = {Chirimuuta, M.},
	keywords = {readme, explanation, philosophy of neuroscience, philosophy of science, computation, epistemology},
	file = {Snapshot:/Users/kriarm/Zotero/storage/PPRS32Y4/3069966.html:text/html},
}

@article{lewis_discourse-level_2017,
	title = {Discourse-level semantic coherence influences beta oscillatory dynamics and the {N400} during sentence comprehension},
	volume = {32},
	issn = {2327-3798, 2327-3801},
	url = {https://www.tandfonline.com/doi/full/10.1080/23273798.2016.1211300},
	doi = {10.1080/23273798.2016.1211300},
	language = {en},
	number = {5},
	urldate = {2018-12-03},
	journal = {Language, Cognition and Neuroscience},
	author = {Lewis, Ashley Glen and Schoffelen, Jan-Mathijs and Hoffmann, Christian and Bastiaansen, Marcel and Schriefers, Herbert},
	month = may,
	year = {2017},
	keywords = {project.streams, readme, beta, neural oscillations, discourse},
	pages = {601--617},
}

@article{brette_is_2018,
	title = {Is coding a relevant metaphor for the brain?},
	copyright = {© 2018, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	url = {https://www.biorxiv.org/content/early/2018/07/13/168237},
	doi = {10.1101/168237},
	abstract = {"Neural coding" is a popular metaphor in neuroscience, where objective properties of the world are communicated to the brain in the form of spikes. Here I argue that this metaphor is often inappropriate and misleading. First, when neurons are said to encode experimental parameters, the neural code depends on experimental details that are not carried by the coding variable. Thus, the representational power of neural codes is much more limited than generally implied. Second, neural codes carry information only by reference to things with known meaning. In contrast, perceptual systems must build information from relations between sensory signals and actions, forming a structured internal model. Neural codes are inadequate for this purpose because they are unstructured. Third, coding variables are observables tied to the temporality of experiments, while spikes are timed actions that mediate coupling in a distributed dynamical system. The coding metaphor tries to fit the dynamic, circular and distributed causal structure of the brain into a linear chain of transformations between observables, but the two causal structures are incongruent. I conclude that the neural coding metaphor cannot provide a basis for theories of brain function, because it is incompatible with both the causal structure of the brain and the informational requirements of cognition.},
	language = {en},
	urldate = {2018-12-01},
	journal = {bioRxiv},
	author = {Brette, Romain},
	month = jul,
	year = {2018},
	keywords = {readme, explanation, philosophy of science, epistemology, coding, neural code, neuroscience, spikes},
	pages = {168237},
	file = {Brette - 2018 - Is coding a relevant metaphor for the brain.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Brette - 2018 - Is coding a relevant metaphor for the brain.pdf:application/pdf},
}

@inproceedings{keller_cognitively_2010,
	address = {Uppsala, Sweden},
	title = {Cognitively plausible models of human language processing},
	url = {http://www.aclweb.org/anthology/P10-2012},
	booktitle = {{ACL} 2010, {Proceedings} of the 48th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Keller, Frank},
	year = {2010},
	pages = {60--67},
}

@article{sample_computer_2017,
	chapter = {Science},
	title = {Computer says no: why making {AIs} fair, accountable and transparent is crucial},
	issn = {0261-3077},
	shorttitle = {Computer says no},
	url = {https://www.theguardian.com/science/2017/nov/05/computer-says-no-why-making-ais-fair-accountable-and-transparent-is-crucial},
	abstract = {As powerful AIs proliferate in society, the ability to trace their decisions, challenge them and remove ingrained biases is a key area of research},
	language = {en-GB},
	urldate = {2018-11-27},
	journal = {The Guardian},
	author = {Sample, Ian},
	month = nov,
	year = {2017},
	keywords = {AI, ethics, interpretability},
	file = {Snapshot:/Users/kriarm/Zotero/storage/GHDGSWEW/computer-says-no-why-making-ais-fair-accountable-and-transparent-is-crucial.html:text/html},
}

@article{ince_tracing_2015,
	title = {Tracing the flow of perceptual features in an algorithmic brain network},
	volume = {5},
	copyright = {2015 Nature Publishing Group},
	issn = {2045-2322},
	url = {https://www.nature.com/articles/srep17681},
	doi = {10.1038/srep17681},
	abstract = {The model of the brain as an information processing machine is a profound hypothesis in which neuroscience, psychology and theory of computation are now deeply rooted. Modern neuroscience aims to model the brain as a network of densely interconnected functional nodes. However, to model the dynamic information processing mechanisms of perception and cognition, it is imperative to understand brain networks at an algorithmic level–i.e. as the information flow that network nodes code and communicate. Here, using innovative methods (Directed Feature Information), we reconstructed examples of possible algorithmic brain networks that code and communicate the specific features underlying two distinct perceptions of the same ambiguous picture. In each observer, we identified a network architecture comprising one occipito-temporal hub where the features underlying both perceptual decisions dynamically converge. Our focus on detailed information flow represents an important step towards a new brain algorithmics to model the mechanisms of perception and cognition.},
	language = {en},
	urldate = {2018-11-26},
	journal = {Scientific Reports},
	author = {Ince, Robin A. A. and van Rijsbergen, Nicola J. and Thut, Gregor and Rousselet, Guillaume A. and Gross, Joachim and Panzeri, Stefano and Schyns, Philippe G.},
	month = dec,
	year = {2015},
	keywords = {MEG, readme, information theory, project.streams2, mutual information, vision},
	pages = {17681},
	file = {Snapshot:/Users/kriarm/Zotero/storage/K6LBYSAN/srep17681.html:text/html},
}

@article{park_predictive_2018,
	title = {Predictive entrainment of natural speech through two fronto-motor top-down channels},
	volume = {0},
	issn = {2327-3798},
	url = {https://doi.org/10.1080/23273798.2018.1506589},
	doi = {10.1080/23273798.2018.1506589},
	abstract = {Natural communication between interlocutors is enabled by the ability to predict upcoming speech in a given context. Previously we showed that these predictions rely on a fronto-motor top-down control of low-frequency oscillations in auditory-temporal brain areas that track intelligible speech. However, a comprehensive spatio-temporal characterisation of this effect is still missing. Here, we applied transfer entropy to source-localised MEG data during continuous speech perception. First, at low frequencies (1–4 Hz, brain delta phase to speech delta phase), predictive effects start in left fronto-motor regions and progress to right temporal regions. Second, at higher frequencies (14–18 Hz, brain beta power to speech delta phase), predictive patterns show a transition from left inferior frontal gyrus via left precentral gyrus to left primary auditory areas. Our results suggest a progression of prediction processes from higher-order to early sensory areas in at least two different frequency channels.},
	number = {0},
	urldate = {2018-11-26},
	journal = {Language, Cognition and Neuroscience},
	author = {Park, Hyojin and Thut, Gregor and Gross, Joachim},
	month = sep,
	year = {2018},
	keywords = {project.streams2, beta, neural oscillations, speech comprehension},
	pages = {1--13},
	file = {Full Text PDF:/Users/kriarm/Zotero/storage/58TCY79P/Park et al. - 2018 - Predictive entrainment of natural speech through t.pdf:application/pdf;Snapshot:/Users/kriarm/Zotero/storage/9CU7TQSR/23273798.2018.html:text/html},
}

@article{huang_connecting_2018,
	title = {Connecting deep neural networks to physical, perceptual, and electrophysiological auditory signals},
	volume = {12},
	issn = {1662-453X},
	url = {https://www.frontiersin.org/articles/10.3389/fnins.2018.00532/full},
	doi = {10.3389/fnins.2018.00532},
	abstract = {Deep neural networks have been recently shown to capture intricate information transformation of signals from the sensory profiles to semantic representations that facilitate recognition or discrimination of complex stimuli. In this vein, convolution neural networks (CNNs) have been used very successfully in image and audio classification. Designed to imitate the hierarchical structure of the nervous system, CNNs reflect activation with increasing degrees of complexity that transform the incoming signal onto object-level representations. In this work, we employ a CNN trained for large-scale object classification to gain insights about the contribution of various audio representations that guide sound perception. The analysis contrasts activation of different layers of a convolutional neural network with acoustic features extracted directly from the scenes, perceptual salience obtained from behavioral responses of human listeners, as well as neural oscillations recorded by Electroencephalography (EEG) in response to the same natural scenes. All three measures are tightly linked quantities believed to guide percepts of salience and object formation when listening to complex scenes. The results paint a picture of the intricate interplay between low-level and object-level representations in guiding auditory salience that is very much dependent on context and sound category.},
	language = {English},
	urldate = {2018-11-22},
	journal = {Frontiers in Neuroscience},
	author = {Huang, Nicholas and Slaney, Malcolm and Elhilali, Mounya},
	year = {2018},
	keywords = {deep learning, project.lstmMEG, readme, auditory neuroscience, machine learning},
}

@article{lecun_deep_2015,
	title = {Deep learning},
	volume = {521},
	issn = {1476-4687},
	url = {http://www.nature.com/articles/nature14539},
	doi = {10.1038/nature14539},
	abstract = {Deep learning allows computational models that are composed of multiple processing layers to learn representations of data with multiple levels of abstraction. These methods have dramatically improved the state-of-the-art in speech recognition, visual object recognition, object detection and many other domains such as drug discovery and genomics. Deep learning discovers intricate structure in large data sets by using the backpropagation algorithm to indicate how a machine should change its internal parameters that are used to compute the representation in each layer from the representation in the previous layer. Deep convolutional nets have brought about breakthroughs in processing images, video, speech and audio, whereas recurrent nets have shone light on sequential data such as text and speech.},
	language = {en},
	number = {7553},
	urldate = {2018-11-22},
	journal = {Nature},
	author = {LeCun, Yann and Bengio, Yoshua and Hinton, Geoffrey},
	month = may,
	year = {2015},
	keywords = {deep learning, RNN, AI, read, machine learning},
	pages = {436--444},
}

@article{bahdanau_neural_2014,
	title = {Neural machine translation by jointly learning to align and translate},
	volume = {abs/1409.0473},
	url = {http://arxiv.org/abs/1409.0473},
	journal = {CoRR},
	author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	year = {2014},
	keywords = {LSTM, RNN, readme, computational linguistics},
}

@article{melis_state_2017,
	title = {On the state of the art of evaluation in neural language models},
	volume = {abs/1707.05589},
	url = {http://arxiv.org/abs/1707.05589},
	journal = {CoRR},
	author = {Melis, Gábor and Dyer, Chris and Blunsom, Phil},
	year = {2017},
	keywords = {LSTM, RNN, project.lstmMEG, readme, computational linguistics},
}

@article{bhattasali_localising_2018,
	title = {Localising memory retrieval and syntactic composition: an {fMRI} study of naturalistic language comprehension},
	issn = {2327-3798, 2327-3801},
	shorttitle = {Localising memory retrieval and syntactic composition},
	url = {https://www.tandfonline.com/doi/full/10.1080/23273798.2018.1518533},
	doi = {10.1080/23273798.2018.1518533},
	language = {en},
	urldate = {2018-11-20},
	journal = {Language, Cognition and Neuroscience},
	author = {Bhattasali, Shohini and Fabre, Murielle and Luh, Wen-Ming and Al Saied, Hazem and Constant, Mathieu and Pallier, Christophe and Brennan, Jonathan R. and Spreng, R. Nathan and Hale, John},
	month = sep,
	year = {2018},
	keywords = {syntax, fMRI, readme, computational linguistics, auditory comprehension},
	pages = {1--20},
}

@article{kuncoro_what_2016,
	title = {What do recurrent neural network grammars learn about syntax?},
	volume = {abs/1611.05774},
	url = {http://arxiv.org/abs/1611.05774},
	journal = {CoRR},
	author = {Kuncoro, Adhiguna and Ballesteros, Miguel and Kong, Lingpeng and Dyer, Chris and Neubig, Graham and Smith, Noah A.},
	year = {2016},
	keywords = {RNN, project.lstmMEG, readme, computational linguistics, grammar},
}

@article{dyer_recurrent_2016,
	title = {Recurrent neural network grammars},
	volume = {abs/1602.07776},
	url = {http://arxiv.org/abs/1602.07776},
	journal = {CoRR},
	author = {Dyer, Chris and Kuncoro, Adhiguna and Ballesteros, Miguel and Smith, Noah A.},
	year = {2016},
	keywords = {language model, RNN, project.lstmMEG, readme, computational linguistics, grammar},
}

@article{gleeson_commitment_2017,
	title = {A commitment to open source in neuroscience},
	volume = {96},
	issn = {0896-6273},
	url = {https://www.sciencedirect.com/science/article/pii/S0896627317309819},
	doi = {10.1016/j.neuron.2017.10.013},
	abstract = {Modern neuroscience increasingly relies on custom-developed software, but much of this is not being made available to the wider community. A group of researchers are pledging to make code they produce for data analysis and modeling open source, and are actively encouraging their colleagues to follow suit.},
	number = {5},
	urldate = {2017-12-07},
	journal = {Neuron},
	author = {Gleeson, Padraig and Davison, Andrew P. and Silver, R. Angus and Ascoli, Giorgio A.},
	month = dec,
	year = {2017},
	keywords = {open science, methods, read, metascience},
	pages = {964--965},
}

@article{kadar_representation_2017,
	title = {Representation of linguistic form and function in recurrent neural networks},
	volume = {43},
	issn = {0891-2017, 1530-9312},
	url = {http://www.mitpressjournals.org/doi/abs/10.1162/COLI_a_00300},
	doi = {10.1162/COLI_a_00300},
	language = {en},
	number = {4},
	urldate = {2018-11-10},
	journal = {Computational Linguistics},
	author = {Kádár, Ákos and Chrupała, Grzegorz and Alishahi, Afra},
	month = dec,
	year = {2017},
	keywords = {LSTM, project.lstmMEG, readme, neural networks, interpretability, artificial intelligence},
	pages = {761--780},
	file = {Kádár et al. - 2017 - Representation of linguistic form and function in .pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Kádár et al. - 2017 - Representation of linguistic form and function in .pdf:application/pdf},
}

@inproceedings{lawrence_can_1996,
	title = {Can recurrent neural networks learn natural language grammars?},
	volume = {4},
	isbn = {978-0-7803-3210-2},
	url = {http://ieeexplore.ieee.org/document/549183/},
	doi = {10.1109/ICNN.1996.549183},
	urldate = {2018-07-12},
	publisher = {IEEE},
	author = {Lawrence, S. and Giles, C.L. and {Sandiway Fong}},
	year = {1996},
	keywords = {syntax, language model, LSTM, project.lstmMEG, readme, language modeling, LSTM.syntax, project.lsnn},
	pages = {1853--1858},
}

@book{holland_induction:_1986,
	address = {Cambridge, Mass},
	series = {Computational models of cognition and perception},
	title = {Induction: processes of inference, learning, and discovery},
	isbn = {978-0-262-08160-3},
	shorttitle = {Induction},
	publisher = {MIT Press},
	editor = {Holland, John H.},
	year = {1986},
	keywords = {readme, neural networks, machine learning, artificial intelligence, deduction, induction, inference},
}

@article{agrawal_pixels_2014,
	title = {Pixels to voxels: modeling visual representation in the human brain},
	shorttitle = {Pixels to {Voxels}},
	url = {http://arxiv.org/abs/1407.5104},
	abstract = {The human brain is adept at solving difficult high-level visual processing problems such as image interpretation and object recognition in natural scenes. Over the past few years neuroscientists have made remarkable progress in understanding how the human brain represents categories of objects and actions in natural scenes. However, all current models of high-level human vision operate on hand annotated images in which the objects and actions have been assigned semantic tags by a human operator. No current models can account for high-level visual function directly in terms of low-level visual input (i.e., pixels). To overcome this fundamental limitation we sought to develop a new class of models that can predict human brain activity directly from low-level visual input (i.e., pixels). We explored two classes of models drawn from computer vision and machine learning. The first class of models was based on Fisher Vectors (FV) and the second was based on Convolutional Neural Networks (ConvNets). We find that both classes of models accurately predict brain activity in high-level visual areas, directly from pixels and without the need for any semantic tags or hand annotation of images. This is the first time that such a mapping has been obtained. The fit models provide a new platform for exploring the functional principles of human vision, and they show that modern methods of computer vision and machine learning provide important tools for characterizing brain function.},
	urldate = {2018-10-11},
	journal = {arXiv:1407.5104 [cs, q-bio]},
	author = {Agrawal, Pulkit and Stansbury, Dustin and Malik, Jitendra and Gallant, Jack L.},
	month = jul,
	year = {2014},
	note = {arXiv: 1407.5104},
	keywords = {fMRI, image recognition, project.lstmMEG, readme, CNN, neural networks, vision},
}

@book{dumas_les_1995,
	address = {Paris},
	title = {Les trois mousquetaires},
	isbn = {978-2-253-00888-0},
	publisher = {Librairie générale Française},
	author = {Dumas, Alexandre pêre},
	year = {1995},
	keywords = {fiction, fr, history, routine, writing},
}

@article{marder_memory_1996,
	title = {Memory from the dynamics of intrinsic membrane currents},
	volume = {93},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/cgi/doi/10.1073/pnas.93.24.13481},
	doi = {10.1073/pnas.93.24.13481},
	language = {en},
	number = {24},
	urldate = {2018-10-09},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Marder, E. and Abbott, L. F. and Turrigiano, G. G. and Liu, Z. and Golowasch, J.},
	month = nov,
	year = {1996},
	keywords = {learning, readme, project.lsnn, intrinsic plasticity},
	pages = {13481--13486},
}

@article{petersson_neurobiology_2012,
	title = {The neurobiology of syntax: beyond string sets},
	volume = {367},
	issn = {0962-8436, 1471-2970},
	shorttitle = {The neurobiology of syntax},
	url = {http://rstb.royalsocietypublishing.org/cgi/doi/10.1098/rstb.2012.0101},
	doi = {10.1098/rstb.2012.0101},
	language = {en},
	number = {1598},
	urldate = {2018-10-09},
	journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	author = {Petersson, K. M. and Hagoort, P.},
	month = jul,
	year = {2012},
	keywords = {syntax, read, project.lsnn, processing memory},
	pages = {1971--1983},
}

@article{van_gerven_editorial:_2017,
	title = {Editorial: artificial neural networks as models of neural information processing},
	volume = {11},
	issn = {1662-5188},
	shorttitle = {Editorial},
	url = {http://journal.frontiersin.org/article/10.3389/fncom.2017.00114/full},
	doi = {10.3389/fncom.2017.00114},
	urldate = {2018-10-04},
	journal = {Frontiers in Computational Neuroscience},
	author = {van Gerven, Marcel and Bohte, Sander},
	month = dec,
	year = {2017},
	keywords = {RNN, project.lstmMEG, explanation},
}

@article{van_gerven_computational_2017,
	title = {Computational foundations of natural intelligence},
	volume = {11},
	issn = {1662-5188},
	url = {http://journal.frontiersin.org/article/10.3389/fncom.2017.00112/full},
	doi = {10.3389/fncom.2017.00112},
	urldate = {2018-10-03},
	journal = {Frontiers in Computational Neuroscience},
	author = {van Gerven, Marcel},
	month = dec,
	year = {2017},
	keywords = {RNN, project.lstmMEG, readme, neural networks, ANN},
}

@article{cacioppo_letter_2008,
	title = {A letter to young scientists},
	volume = {21},
	url = {https://www.psychologicalscience.org/observer/a-letter-to-young-scientists},
	language = {en-US},
	number = {5},
	urldate = {2018-03-14},
	journal = {APS Observer},
	author = {Cacioppo, John T.},
	month = may,
	year = {2008},
	file = {Snapshot:/Users/kriarm/Zotero/storage/CSG3V3FQ/comment-page-1.html:text/html},
}

@article{rao_predictive_1999,
	title = {Predictive coding in the visual cortex: a functional interpretation of some extra-classical receptive-field effects},
	volume = {2},
	issn = {1097-6256, 1546-1726},
	shorttitle = {Predictive coding in the visual cortex},
	url = {http://www.nature.com/articles/nn0199_79},
	doi = {10.1038/4580},
	language = {en},
	number = {1},
	urldate = {2018-10-03},
	journal = {Nature Neuroscience},
	author = {Rao, Rajesh P. N. and Ballard, Dana H.},
	month = jan,
	year = {1999},
	pages = {79--87},
}

@article{kadar_representation_2016,
	title = {Representation of linguistic form and function in recurrent neural networks},
	url = {http://arxiv.org/abs/1602.08952},
	abstract = {We present novel methods for analyzing the activation patterns of RNNs from a linguistic point of view and explore the types of linguistic structure they learn. As a case study, we use a multi-task gated recurrent network architecture consisting of two parallel pathways with shared word embeddings trained on predicting the representations of the visual scene corresponding to an input sentence, and predicting the next word in the same sentence. Based on our proposed method to estimate the amount of contribution of individual tokens in the input to the final prediction of the networks we show that the image prediction pathway: a) is sensitive to the information structure of the sentence b) pays selective attention to lexical categories and grammatical functions that carry semantic information c) learns to treat the same input token differently depending on its grammatical functions in the sentence. In contrast the language model is comparatively more sensitive to words with a syntactic function. Furthermore, we propose methods to ex- plore the function of individual hidden units in RNNs and show that the two pathways of the architecture in our case study contain specialized units tuned to patterns informative for the task, some of which can carry activations to later time steps to encode long-term dependencies.},
	urldate = {2018-10-03},
	journal = {arXiv:1602.08952 [cs]},
	author = {Kádár, Ákos and Chrupała, Grzegorz and Alishahi, Afra},
	month = feb,
	year = {2016},
	note = {arXiv: 1602.08952},
	keywords = {language model, LSTM, RNN interpretability, project.lstmMEG, readme},
}

@article{dennett_real_1991,
	title = {Real patterns},
	volume = {88},
	issn = {0022362X},
	url = {http://www.pdcnet.org/oom/service?url_ver=Z39.88-2004&rft_val_fmt=&rft.imuse_id=jphil_1991_0088_0001_0027_0051&svc_id=info:www.pdcnet.org/collection},
	doi = {10.2307/2027085},
	number = {1},
	urldate = {2018-10-02},
	journal = {The Journal of Philosophy},
	author = {Dennett, Daniel C.},
	month = jan,
	year = {1991},
	keywords = {philosophy of science, philosophy of mind},
	pages = {27},
	file = {Dennett - 1991 - Real patterns.pdf:/Users/kriarm/Zotero/storage/JLJRXY2S/Dennett - 1991 - Real patterns.pdf:application/pdf},
}

@article{crick_recent_1989,
	title = {The recent excitement about neural networks},
	volume = {337},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/doifinder/10.1038/337129a0},
	doi = {10.1038/337129a0},
	language = {en},
	number = {6203},
	urldate = {2018-10-01},
	journal = {Nature},
	author = {Crick, Francis},
	month = jan,
	year = {1989},
	keywords = {project.lstmMEG, readme, neural networks, ANN},
	pages = {129--132},
}

@article{baker_new_2018,
	title = {New advances in encoding and decoding of brain signals},
	volume = {180},
	issn = {10538119},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1053811918305718},
	doi = {10.1016/j.neuroimage.2018.06.064},
	language = {en},
	urldate = {2018-09-30},
	journal = {NeuroImage},
	author = {Baker, Chris I. and van Gerven, Marcel},
	month = oct,
	year = {2018},
	keywords = {explanation, decoding models, encoding models, thesis.introduction},
	pages = {1--3},
}

@article{strobelt_lstmvis:_2016,
	title = {{LSTMVis}: {A} tool for visual analysis of hidden state dynamics in recurrent neural networks},
	shorttitle = {{LSTMVis}},
	url = {http://arxiv.org/abs/1606.07461},
	abstract = {Recurrent neural networks, and in particular long short-term memory (LSTM) networks, are a remarkably effective tool for sequence modeling that learn a dense black-box hidden representation of their sequential input. Researchers interested in better understanding these models have studied the changes in hidden state representations over time and noticed some interpretable patterns but also significant noise. In this work, we present LSTMVIS, a visual analysis tool for recurrent neural networks with a focus on understanding these hidden state dynamics. The tool allows users to select a hypothesis input range to focus on local state changes, to match these states changes to similar patterns in a large data set, and to align these results with structural annotations from their domain. We show several use cases of the tool for analyzing specific hidden state properties on dataset containing nesting, phrase structure, and chord progressions, and demonstrate how the tool can be used to isolate patterns for further statistical analysis. We characterize the domain, the different stakeholders, and their goals and tasks.},
	urldate = {2018-09-27},
	journal = {arXiv:1606.07461 [cs]},
	author = {Strobelt, Hendrik and Gehrmann, Sebastian and Pfister, Hanspeter and Rush, Alexander M.},
	month = jun,
	year = {2016},
	note = {arXiv: 1606.07461},
	keywords = {LSTM, RNN interpretability, project.lstmMEG, readme},
	file = {Strobelt et al. - 2016 - LSTMVis A tool for visual analysis of hidden stat.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Strobelt et al. - 2016 - LSTMVis A tool for visual analysis of hidden stat.pdf:application/pdf},
}

@article{hassabis_neuroscience-inspired_2017,
	title = {Neuroscience-inspired artificial intelligence},
	volume = {95},
	issn = {08966273},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627317305093},
	doi = {10.1016/j.neuron.2017.06.011},
	language = {en},
	number = {2},
	urldate = {2018-09-24},
	journal = {Neuron},
	author = {Hassabis, Demis and Kumaran, Dharshan and Summerfield, Christopher and Botvinick, Matthew},
	month = jul,
	year = {2017},
	keywords = {deep learning, AI, project.lstmMEG, readme, neural networks, project.lsnn},
	pages = {245--258},
}

@article{gallistel_neuroscience_2013,
	title = {The neuroscience of learning: {Beyond} the {Hebbian} synapse},
	volume = {64},
	issn = {0066-4308, 1545-2085},
	shorttitle = {The {Neuroscience} of {Learning}},
	url = {http://www.annualreviews.org/doi/10.1146/annurev-psych-113011-143807},
	doi = {10.1146/annurev-psych-113011-143807},
	language = {en},
	number = {1},
	urldate = {2018-09-11},
	journal = {Annual Review of Psychology},
	author = {Gallistel, C.R. and Matzel, Louis D.},
	month = jan,
	year = {2013},
	keywords = {learning, readme, explanation, computation, connectionism, project.lsnn, LTP, STDP, synaptic plasticity},
	pages = {169--200},
}

@incollection{maass_computational_1995,
	title = {On the computational complexity of networks of spiking neurons},
	url = {http://papers.nips.cc/paper/926-on-the-computational-complexity-of-networks-of-spiking-neurons.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 7},
	publisher = {MIT Press},
	author = {Maass, Wolfgang},
	editor = {Tesauro, G. and Touretzky, D. S. and Leen, T. K.},
	year = {1995},
	keywords = {readme, neural networks, project.lsnn, LIF, lsnn},
	pages = {183--190},
}

@article{maass_networks_1997,
	title = {Networks of spiking neurons: {The} third generation of neural network models},
	volume = {10},
	issn = {08936080},
	shorttitle = {Networks of spiking neurons},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0893608097000117},
	doi = {10.1016/S0893-6080(97)00011-7},
	abstract = {The computational power of formal models for networks of spiking neurons is compared with that of other neural network models based on McCulloch Pitts neurons (i.e., threshold gates), respectively, sigmoidal gates. In particular it is shown that networks of spiking neurons are, with regard to the number of neurons that are needed, computationally more powerful than these other neural network models. A concrete biologically relevant function is exhibited which can be computed by a single spiking neuron (for biologically reasonable values of its parameters), but which requires hundreds of hidden units on a sigmoidal neural net. On the other hand, it is known that any function that can be computed by a small sigmoidal neural net can also be computed by a small network of spiking neurons. This article does not assume prior knowledge about spiking neurons, and it contains an extensive list of references to the currently available literature on computations in networks of spiking neurons and relevant results from neurobiology. © 1997 Elsevier Science Ltd. All rights reserved.},
	language = {en},
	number = {9},
	urldate = {2018-09-17},
	journal = {Neural Networks},
	author = {Maass, Wolfgang},
	month = dec,
	year = {1997},
	keywords = {readme, project.lsnn, LIF, lsnn},
	pages = {1659--1671},
	file = {Maass - 1997 - Networks of spiking neurons The third generation .pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Maass - 1997 - Networks of spiking neurons The third generation .pdf:application/pdf},
}

@article{lerner_topographic_2011,
	title = {Topographic mapping of a hierarchy of temporal receptive windows using a narrated story},
	volume = {31},
	copyright = {Copyright © 2011 the authors 0270-6474/11/312906-10\$15.00/0},
	issn = {0270-6474, 1529-2401},
	doi = {10.1523/JNEUROSCI.3684-10.2011},
	abstract = {Real-life activities, such as watching a movie or engaging in conversation, unfold over many minutes. In the course of such activities, the brain has to integrate information over multiple time scales. We recently proposed that the brain uses similar strategies for integrating information across space and over time. Drawing a parallel with spatial receptive fields, we defined the temporal receptive window (TRW) of a cortical microcircuit as the length of time before a response during which sensory information may affect that response. Our previous findings in the visual system are consistent with the hypothesis that TRWs become larger when moving from low-level sensory to high-level perceptual and cognitive areas. In this study, we mapped TRWs in auditory and language areas by measuring fMRI activity in subjects listening to a real-life story scrambled at the time scales of words, sentences, and paragraphs. Our results revealed a hierarchical topography of TRWs. In early auditory cortices (A1+), brain responses were driven mainly by the momentary incoming input and were similarly reliable across all scrambling conditions. In areas with an intermediate TRW, coherent information at the sentence time scale or longer was necessary to evoke reliable responses. At the apex of the TRW hierarchy, we found parietal and frontal areas that responded reliably only when intact paragraphs were heard in a meaningful sequence. These results suggest that the time scale of processing is a functional property that may provide a general organizing principle for the human cerebral cortex.},
	language = {en},
	number = {8},
	urldate = {2018-01-09},
	journal = {Journal of Neuroscience},
	author = {Lerner, Yulia and Honey, Christopher J. and Silbert, Lauren J. and Hasson, Uri},
	month = feb,
	year = {2011},
	pmid = {21414912},
	keywords = {fMRI, project.lstmMEG, readme, narrative, TRW, inter-SC},
	pages = {2906--2915},
	file = {Lerner et al. - 2011 - Topographic mapping of a hierarchy of temporal rec.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Lerner et al. - 2011 - Topographic mapping of a hierarchy of temporal rec.pdf:application/pdf},
}

@book{varela_embodied_2016,
	address = {Cambridge, Massachusetts ; London England},
	edition = {revised edition},
	title = {The embodied mind: cognitive science and human experience},
	isbn = {978-0-262-52936-5},
	shorttitle = {The embodied mind},
	publisher = {MIT Press},
	author = {Varela, Francisco J. and Thompson, Evan and Rosch, Eleanor},
	year = {2016},
	keywords = {experiential learning},
}

@article{buzsaki_mechanisms_2012,
	title = {Mechanisms of gamma sscillations},
	volume = {35},
	issn = {0147-006X, 1545-4126},
	url = {http://www.annualreviews.org/doi/10.1146/annurev-neuro-062111-150444},
	doi = {10.1146/annurev-neuro-062111-150444},
	language = {en},
	number = {1},
	urldate = {2018-09-13},
	journal = {Annual Review of Neuroscience},
	author = {Buzsáki, György and Wang, Xiao-Jing},
	month = jul,
	year = {2012},
	keywords = {readme, neural oscillations, gamma},
	pages = {203--225},
}

@article{buzsaki_scaling_2013,
	title = {Scaling brain size, keeping timing: evolutionary preservation of brain rhythms},
	volume = {80},
	issn = {08966273},
	shorttitle = {Scaling {Brain} {Size}, {Keeping} {Timing}},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0896627313009045},
	doi = {10.1016/j.neuron.2013.10.002},
	language = {en},
	number = {3},
	urldate = {2018-09-12},
	journal = {Neuron},
	author = {Buzsáki, György and Logothetis, Nikos and Singer, Wolf},
	month = oct,
	year = {2013},
	keywords = {evolution, project.lstmMEG, read, neural oscillations, thesis.introduction},
	pages = {751--764},
}

@book{buzsaki_rhythms_2006,
	address = {Oxford ; New York},
	title = {Rhythms of the brain},
	isbn = {978-0-19-530106-9 978-0-19-982823-4},
	publisher = {Oxford University Press},
	author = {Buzsáki, G.},
	year = {2006},
	note = {OCLC: ocm63279497},
	keywords = {readme, neural oscillations},
}

@article{rosvold_continuous_1956,
	title = {A continuous performance test of brain damage.},
	volume = {20},
	issn = {0095-8891},
	url = {http://content.apa.org/journals/ccp/20/5/343},
	doi = {10.1037/h0043220},
	language = {en},
	number = {5},
	urldate = {2018-09-11},
	journal = {Journal of Consulting Psychology},
	author = {Rosvold, H. Enger and Mirsky, Allan F. and Sarason, Irwin and Bransome, Jr., Edwin D. and Beck, Lloyd H.},
	year = {1956},
	keywords = {working memory, readme, project.lsnn, CPT},
	pages = {343--350},
}

@article{jones_quantitative_2009,
	title = {Quantitative analysis and biophysically realistic neural modeling of the {MEG} mu rhythm: {Rhythmogenesis} and modulation of sensory-evoked responses},
	volume = {102},
	issn = {0022-3077, 1522-1598},
	shorttitle = {Quantitative {Analysis} and {Biophysically} {Realistic} {Neural} {Modeling} of the {MEG} {Mu} {Rhythm}},
	url = {http://www.physiology.org/doi/10.1152/jn.00535.2009},
	doi = {10.1152/jn.00535.2009},
	language = {en},
	number = {6},
	urldate = {2018-09-10},
	journal = {Journal of Neurophysiology},
	author = {Jones, Stephanie R. and Pritchett, Dominique L. and Sikora, Michael A. and Stufflebeam, Steven M. and Hämäläinen, Matti and Moore, Christopher I.},
	month = dec,
	year = {2009},
	keywords = {MEG, readme, beta, neural oscillations, motor cortex},
	pages = {3554--3572},
}

@article{sherman_neural_2016,
	title = {Neural mechanisms of transient neocortical beta rhythms: {Converging} evidence from humans, computational modeling, monkeys, and mice},
	volume = {113},
	issn = {0027-8424, 1091-6490},
	shorttitle = {Neural mechanisms of transient neocortical beta rhythms},
	url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1604135113},
	doi = {10.1073/pnas.1604135113},
	language = {en},
	number = {33},
	urldate = {2018-09-10},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Sherman, Maxwell A. and Lee, Shane and Law, Robert and Haegens, Saskia and Thorn, Catherine A. and Hämäläinen, Matti S. and Moore, Christopher I. and Jones, Stephanie R.},
	month = aug,
	year = {2016},
	keywords = {readme, beta, neural oscillations, LFP},
	pages = {E4885--E4894},
}

@book{pollock_open_2018,
	title = {The open revolution},
	isbn = {978-1-983033-22-3},
	language = {English},
	publisher = {a/e/t Press},
	author = {Pollock, Rufus},
	year = {2018},
	note = {OCLC: 1044700824},
	file = {Pollock - 2018 - The open revolution.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Pollock - 2018 - The open revolution.pdf:application/pdf},
}

@article{cohen_where_2017,
	title = {Where does {EEG} come from and what does it mean?},
	volume = {40},
	issn = {01662236},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0166223617300243},
	doi = {10.1016/j.tins.2017.02.004},
	language = {en},
	number = {4},
	urldate = {2018-08-25},
	journal = {Trends in Neurosciences},
	author = {Cohen, Michael X},
	month = apr,
	year = {2017},
	keywords = {MEG, EEG, explanation, read},
	pages = {208--218},
}

@article{cohen_five_2014,
	title = {Five methodological challenges in cognitive electrophysiology},
	volume = {85},
	issn = {10538119},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S1053811913008641},
	doi = {10.1016/j.neuroimage.2013.08.010},
	language = {en},
	urldate = {2018-08-25},
	journal = {NeuroImage},
	author = {Cohen, Michael X and Gulbinaite, Rasa},
	month = jan,
	year = {2014},
	keywords = {MEG, EEG, methods, fixme, neural oscillations, thesis.introduction, electrophysiology, terminology},
	pages = {702--710},
}

@article{keitel_perceptually_2018,
	title = {Perceptually relevant speech tracking in auditory and motor cortex reflects distinct linguistic features},
	volume = {16},
	issn = {1545-7885},
	url = {http://dx.plos.org/10.1371/journal.pbio.2004473},
	doi = {10.1371/journal.pbio.2004473},
	language = {en},
	number = {3},
	urldate = {2018-09-07},
	journal = {PLOS Biology},
	author = {Keitel, Anne and Gross, Joachim and Kayser, Christoph},
	editor = {Bizley, Jennifer},
	month = mar,
	year = {2018},
	keywords = {MEG, project.streams, readme, neural oscillations},
	pages = {e2004473},
}

@article{tennant_we_2018,
	title = {Do we need an open science coalition?},
	url = {https://zenodo.org/record/1407963},
	doi = {10.5281/zenodo.1407963},
	abstract = {This is an opinion piece published on {\textless}em{\textgreater}Elephant in the lab{\textless}/em{\textgreater} – a blog journal on science policy. It is part of a series of articles analysing Open Science.},
	urldate = {2018-09-05},
	author = {Tennant, Jon},
	month = sep,
	year = {2018},
	keywords = {open science},
}

@article{frank_interactions_2001,
	title = {Interactions between frontal cortex and basal ganglia in working memory: {A} computational model},
	volume = {1},
	issn = {1530-7026, 1531-135X},
	shorttitle = {Interactions between frontal cortex and basal ganglia in working memory},
	url = {http://www.springerlink.com/index/10.3758/CABN.1.2.137},
	doi = {10.3758/CABN.1.2.137},
	language = {en},
	number = {2},
	urldate = {2018-09-04},
	journal = {Cognitive, Affective, \& Behavioral Neuroscience},
	author = {Frank, M. J. and Loughry, B. and O'Reilly, R. C.},
	month = jun,
	year = {2001},
	keywords = {working memory, readme, project.lsnn, lsnn, 1-2-AX, computational cognitive neuroscience, computational implementation},
	pages = {137--160},
}

@article{burkitt_review_2006,
	title = {A review of the integrate-and-fire neuron model: {I}. homogeneous synaptic input},
	volume = {95},
	issn = {0340-1200, 1432-0770},
	shorttitle = {A {Review} of the {Integrate}-and-fire {Neuron} {Model}},
	doi = {10.1007/s00422-006-0068-6},
	language = {en},
	number = {1},
	journal = {Biological Cybernetics},
	author = {Burkitt, A. N.},
	month = jul,
	year = {2006},
	keywords = {readme, project.lsnn, LIF, computational neuroscience},
	pages = {1--19},
}

@article{kriegeskorte_cognitive_2018,
	title = {Cognitive computational neuroscience},
	volume = {21},
	issn = {1097-6256, 1546-1726},
	url = {http://www.nature.com/articles/s41593-018-0210-5},
	doi = {10.1038/s41593-018-0210-5},
	language = {en},
	number = {9},
	urldate = {2018-09-01},
	journal = {Nature Neuroscience},
	author = {Kriegeskorte, Nikolaus and Douglas, Pamela K.},
	month = sep,
	year = {2018},
	keywords = {explanation, read, computation, model-based, theory},
	pages = {1148--1160},
}

@article{hasson_hierarchy_2008,
	title = {A hierarchy of temporal receptive windows in human cortex},
	volume = {28},
	copyright = {Copyright © 2008 Society for Neuroscience 0270-6474/08/282539-12\$15.00/0},
	issn = {0270-6474, 1529-2401},
	doi = {10.1523/JNEUROSCI.5487-07.2008},
	abstract = {Real-world events unfold at different time scales and, therefore, cognitive and neuronal processes must likewise occur at different time scales. We present a novel procedure that identifies brain regions responsive to sensory information accumulated over different time scales. We measured functional magnetic resonance imaging activity while observers viewed silent films presented forward, backward, or piecewise-scrambled in time. Early visual areas (e.g., primary visual cortex and the motion-sensitive area MT+) exhibited high response reliability regardless of disruptions in temporal structure. In contrast, the reliability of responses in several higher brain areas, including the superior temporal sulcus (STS), precuneus, posterior lateral sulcus (LS), temporal parietal junction (TPJ), and frontal eye field (FEF), was affected by information accumulated over longer time scales. These regions showed highly reproducible responses for repeated forward, but not for backward or piecewise-scrambled presentations. Moreover, these regions exhibited marked differences in temporal characteristics, with LS, TPJ, and FEF responses depending on information accumulated over longer durations (∼36 s) than STS and precuneus (∼12 s). We conclude that, similar to the known cortical hierarchy of spatial receptive fields, there is a hierarchy of progressively longer temporal receptive windows in the human brain.},
	language = {en},
	number = {10},
	urldate = {2017-12-05},
	journal = {Journal of Neuroscience},
	author = {Hasson, Uri and Yang, Eunice and Vallines, Ignacio and Heeger, David J. and Rubin, Nava},
	month = mar,
	year = {2008},
	pmid = {18322098},
	keywords = {fMRI, project.lstmMEG, read, TRW},
	pages = {2539--2550},
	file = {Hasson et al. - 2008 - A hierarchy of temporal receptive windows in human.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Hasson et al. - 2008 - A hierarchy of temporal receptive windows in human.pdf:application/pdf},
}

@article{christiansen_now-or-never_2016,
	title = {The now-or-never bottleneck: {A} fundamental constraint on language},
	volume = {39},
	issn = {0140-525X, 1469-1825},
	shorttitle = {The {Now}-or-{Never} bottleneck},
	url = {https://www.cambridge.org/core/product/identifier/S0140525X1500031X/type/journal_article},
	doi = {10.1017/S0140525X1500031X},
	language = {en},
	urldate = {2018-08-27},
	journal = {Behavioral and Brain Sciences},
	author = {Christiansen, Morten H. and Chater, Nick},
	year = {2016},
	keywords = {project.lstmMEG, read},
}

@article{mcclelland_place_2009,
	title = {The place of modeling in cognitive science},
	volume = {1},
	issn = {17568757, 17568765},
	url = {http://doi.wiley.com/10.1111/j.1756-8765.2008.01003.x},
	doi = {10.1111/j.1756-8765.2008.01003.x},
	language = {en},
	number = {1},
	urldate = {2018-08-27},
	journal = {Topics in Cognitive Science},
	author = {McClelland, James L.},
	month = jan,
	year = {2009},
	keywords = {explanation, read, computation, neural networks, model-based, cognitive science},
	pages = {11--38},
}

@incollection{frank_getting_2014,
	title = {Getting real about systematicity},
	isbn = {978-0-262-02723-6},
	url = {https://repository.ubn.ru.nl/handle/2066/131345},
	urldate = {2018-08-27},
	booktitle = {The architecture of cognition: rethinking {Fodor} and {Pylyshyn}'s systematicity challenge},
	publisher = {MIT Press},
	author = {Frank, S. L.},
	year = {2014},
	keywords = {computation, neural networks, systematicity},
	pages = {147--164},
}

@misc{teytelman_no_2018,
	type = {News},
	title = {No more excuses for non-reproducible methods},
	copyright = {2018 Nature},
	url = {http://www.nature.com/articles/d41586-018-06008-w},
	abstract = {Online technologies make it easy to share precise experimental protocols — and doing so is essential to modern science.},
	language = {EN},
	urldate = {2018-08-23},
	journal = {Nature},
	author = {Teytelman, Lenny},
	month = aug,
	year = {2018},
	doi = {10.1038/d41586-018-06008-w},
	keywords = {open science, reproducibility, protocol},
	file = {Snapshot:/Users/kriarm/Zotero/storage/9ZWHS2JK/d41586-018-06008-w.html:text/html},
}

@book{bechtel_mental_2008,
	address = {New York},
	title = {Mental mechanisms: philosophical perspectives on cognitive neuroscience},
	isbn = {978-0-8058-6333-8 978-0-8058-6334-5},
	shorttitle = {Mental mechanisms},
	publisher = {Routledge},
	author = {Bechtel, William},
	year = {2008},
	note = {OCLC: ocn123332139},
	keywords = {explanation, philosophy of neuroscience, philosophy of science, computation, mechanism, project.6},
	file = {Bechtel - 2008 - Mental mechanisms philosophical perspectives on c.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Bechtel - 2008 - Mental mechanisms philosophical perspectives on c.pdf:application/pdf},
}

@article{gelder_what_1995,
	title = {What might cognition be if not computation?},
	volume = {92},
	number = {7},
	journal = {Journal of Philosophy},
	author = {Gelder, Tim van},
	year = {1995},
	keywords = {read},
	pages = {345--81},
}

@book{marr_vision:_2010,
	address = {Cambridge, Mass},
	title = {Vision: a computational investigation into the human representation and processing of visual information},
	isbn = {978-0-262-51462-0},
	shorttitle = {Vision},
	publisher = {MIT Press},
	author = {Marr, David},
	year = {2010},
	note = {OCLC: ocn472791457},
	keywords = {explanation, philosophy of science, computation, vision, thesis.introduction},
}

@incollection{poeppel_david_cartographic_2008,
	address = {Berlin, Boston},
	title = {The cartographic imperative: confusing localization and explanation in human brain mapping},
	isbn = {978-3-11-054877-8},
	url = {http://www.degruyter.com/view/books/9783110548778/9783110548778/9783110548778.xml},
	urldate = {2018-07-30},
	booktitle = {Ikonographie des {Gehirns}},
	publisher = {De Gruyter},
	author = {Poeppel, David},
	year = {2008},
	doi = {10.1515/9783110548778},
	keywords = {explanation, read, computation, project.6, functional decomposition, language},
	file = {Poeppel, David - 2008 - The cartographic imperative confusing localizatio.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Poeppel, David - 2008 - The cartographic imperative confusing localizatio.pdf:application/pdf},
}

@article{barrett_future_2009,
	title = {The future of psychology: connecting mind to brain},
	volume = {4},
	issn = {1745-6916, 1745-6924},
	shorttitle = {The {Future} of {Psychology}},
	url = {http://journals.sagepub.com/doi/10.1111/j.1745-6924.2009.01134.x},
	doi = {10.1111/j.1745-6924.2009.01134.x},
	language = {en},
	number = {4},
	urldate = {2018-08-11},
	journal = {Perspectives on Psychological Science},
	author = {Barrett, Lisa Feldman},
	month = jul,
	year = {2009},
	keywords = {readme, explanation, philosophy of science, project.6},
	pages = {326--339},
}

@incollection{barrett_mental_2014,
	title = {Mental mechanisms and psychological construction},
	booktitle = {The {Psychological} {Construction} of {Emotion}},
	publisher = {Guilford Press},
	author = {Herschbach, Mitchell and Bechtel, William},
	editor = {Barrett, Lisa Feldman and Russell, James},
	year = {2014},
	keywords = {explanation, philosophy of neuroscience, philosophy of science, read, computation, thesis.introduction, mechanism, project.6, construction, emotion},
	pages = {21--44},
}

@article{fresco_explaining_2010,
	title = {Explaining computation without semantics: keeping it simple},
	volume = {20},
	issn = {0924-6495, 1572-8641},
	shorttitle = {Explaining {Computation} {Without} {Semantics}},
	url = {http://link.springer.com/10.1007/s11023-010-9199-6},
	doi = {10.1007/s11023-010-9199-6},
	language = {en},
	number = {2},
	urldate = {2018-04-08},
	journal = {Minds and Machines},
	author = {Fresco, Nir},
	month = jul,
	year = {2010},
	keywords = {readme, explanation, computation, cognitive science, computational explanation, philsophy of mind, philsophy of science, semantics},
	pages = {165--181},
}

@book{danziger_naming_2008,
	address = {London},
	title = {Naming the mind: how psychology found its language},
	isbn = {978-0-8039-7762-4},
	shorttitle = {Naming the mind},
	language = {eng},
	publisher = {Sage Publ},
	author = {Danziger, Kurt},
	year = {2008},
	note = {OCLC: 254876541},
	keywords = {explanation, philosophy of science, epistemology, philosophy of mind},
	file = {Danziger - 2008 - Naming the mind how psychology found its language.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Danziger - 2008 - Naming the mind how psychology found its language.pdf:application/pdf},
}

@article{brette_philosophy_2015,
	title = {Philosophy of the spike: {Rate}-based vs. spike-based theories of the brain},
	volume = {9},
	issn = {1662-5137},
	shorttitle = {Philosophy of the {Spike}},
	url = {https://www.frontiersin.org/articles/10.3389/fnsys.2015.00151/full},
	doi = {10.3389/fnsys.2015.00151},
	abstract = {Does the brain use a firing rate code or a spike timing code? Considering this controversial question from an epistemological perspective, I argue that progress has been hampered by its problematic phrasing. It takes the perspective of an external observer looking at whether those two observables vary with stimuli, and thereby misses the relevant question: which one has a causal role in neural activity? When rephrased in a more meaningful way, the rate-based view appears as an ad hoc methodological postulate, one that is practical but with virtually no empirical or theoretical support.},
	language = {English},
	urldate = {2018-07-11},
	journal = {Frontiers in Systems Neuroscience},
	author = {Brette, Romain},
	year = {2015},
	keywords = {readme, explanation, philosophy of neuroscience, project.lsnn, philsophy of science, rate-based models, spiking neurons},
}

@misc{delahunty_scholarly_2018,
	title = {Scholarly publishing: {Addicted} to the brand},
	url = {https://www.inspiringstem.org/rooms/340-archive/posts/33675-addicted-to-the-brand-the-hypocrisy-of-a-publishing-academic},
	urldate = {2018-07-26},
	journal = {Inspiring STEM},
	author = {Delahunty, Martin},
	month = may,
	year = {2018},
}

@article{herz_modeling_2006,
	title = {Modeling single-neuron dynamics and computations: {A} balance of detail and abstraction},
	volume = {314},
	issn = {0036-8075, 1095-9203},
	shorttitle = {Modeling {Single}-{Neuron} {Dynamics} and {Computations}},
	url = {http://www.sciencemag.org/cgi/doi/10.1126/science.1127240},
	doi = {10.1126/science.1127240},
	language = {en},
	number = {5796},
	urldate = {2018-07-26},
	journal = {Science},
	author = {Herz, A. V. M. and Gollisch, T. and Machens, C. K. and Jaeger, D.},
	month = oct,
	year = {2006},
	keywords = {read, LIF, lsnn, computational neuroscience},
	pages = {80--85},
}

@article{oreilly_six_1998,
	title = {Six principles for biologically based computational models of cortical cognition},
	volume = {2},
	issn = {13646613},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S1364661398012418},
	doi = {10.1016/S1364-6613(98)01241-8},
	language = {en},
	number = {11},
	urldate = {2018-07-25},
	journal = {Trends in Cognitive Sciences},
	author = {O'Reilly, Randall C.},
	month = nov,
	year = {1998},
	keywords = {readme, project.lsnn, project.6, biological realism},
	pages = {455--462},
}

@article{oreilly_biologically_2006,
	title = {Biologically based computational models of high-level cognition},
	volume = {314},
	issn = {0036-8075, 1095-9203},
	url = {http://www.sciencemag.org/cgi/doi/10.1126/science.1127242},
	doi = {10.1126/science.1127242},
	language = {en},
	number = {5796},
	urldate = {2018-07-25},
	journal = {Science},
	author = {O'Reilly, R. C.},
	month = oct,
	year = {2006},
	keywords = {project.lstmMEG, explanation, read, computation, project.lsnn, bistability, gating},
	pages = {91--94},
}

@article{protasi_love_2018,
	title = {Love your frenemy},
	url = {https://aeon.co/essays/envy-can-poison-love-but-it-can-also-nourish-growth},
	abstract = {Envy is the dark side of love, but love is the luminous side of envy. Is there a way to harness envy wisely, for growth?},
	language = {en},
	urldate = {2018-07-22},
	journal = {Aeon},
	author = {Protasi, Sara},
	month = jul,
	year = {2018},
	keywords = {read, emotion, envy, mood, social psychology},
	file = {Snapshot:/Users/kriarm/Zotero/storage/SAITVX86/envy-can-poison-love-but-it-can-also-nourish-growth.html:text/html},
}

@article{murray_hierarchy_2014,
	title = {A hierarchy of intrinsic timescales across primate cortex},
	volume = {17},
	copyright = {2014 Nature Publishing Group},
	issn = {1546-1726},
	doi = {10.1038/nn.3862},
	abstract = {{\textless}p{\textgreater}Primate cortex can be organized with specialization and hierarchical principles, but presently there is little evidence for how it is organized temporally. Across six separate datasets, the authors find a hierarchical ordering of intrinsic fluctuation of spiking activity, with timescales that increase from sensory to prefrontal areas.{\textless}/p{\textgreater}},
	language = {En},
	number = {12},
	urldate = {2017-11-27},
	journal = {Nature Neuroscience},
	author = {Murray, John D. and Bernacchia, Alberto and Padoa-Schioppa, Camillo and Lee, Daeyeol and Freedman, David J. and Seo, Hyojung and Wallis, Jonathan D. and Romo, Ranulfo and Pasternak, Tatiana and Wang, Xiao-Jing and Cai, Xinying},
	month = dec,
	year = {2014},
	keywords = {hierarchy, project.lstmMEG, read, time},
	pages = {1661},
	file = {Bernacchia et al. - 2014 - A hierarchy of intrinsic timescales across primate.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Bernacchia et al. - 2014 - A hierarchy of intrinsic timescales across primate.pdf:application/pdf},
}

@article{robinson_integrating_1989,
	title = {Integrating with neurons},
	volume = {12},
	issn = {0147-006X, 1545-4126},
	url = {http://www.annualreviews.org/doi/10.1146/annurev.ne.12.030189.000341},
	doi = {10.1146/annurev.ne.12.030189.000341},
	language = {en},
	number = {1},
	urldate = {2018-07-19},
	journal = {Annual Review of Neuroscience},
	author = {Robinson, D A},
	month = mar,
	year = {1989},
	keywords = {readme, project.lsnn, neural integration},
	pages = {33--45},
}

@incollection{goldman_neural_2009,
	address = {Oxford},
	title = {Neural integrator models},
	isbn = {978-0-08-045046-9},
	url = {https://www.sciencedirect.com/science/article/pii/B9780080450469014340},
	abstract = {Integration of information across time is a neural computation of critical importance to a variety of brain functions. Examples include oculomotor neural integrators and head direction cells that integrate velocity signals into positional or directional signals, parametric working memory circuits which convert transient input pulses into self-sustained persistent neural activity patterns, and linear ramping neural activity underlying the accumulation of information during decision making. How is integration over long timescales realized in neural circuits? This article reviews experimental and theoretical work related to this fundamental question, with a focus on the idea that recurrent synaptic or cellular mechanisms can instantiate an integration time much longer than intrinsic biophysical time constants of the system. We first introduce some basic concepts and present two types of codes used by neural integrators – the location code and the rate code. Then we summarize models that implement a variety of candidate mechanisms for neural integration in the brain, and we discuss the problem of fine-tuning of model parameters and possible solutions to this problem. Finally, we outline challenges for future research.},
	urldate = {2017-12-05},
	booktitle = {Encyclopedia of {Neuroscience}},
	publisher = {Academic Press},
	author = {Goldman, M. S. and Compte, A. and Wang, X. -J.},
	editor = {Squire, Larry R.},
	year = {2009},
	doi = {10.1016/B978-008045046-9.01434-0},
	keywords = {project.lstmMEG, fixme, project.lsnn, computational neuroscience, integration},
	pages = {165--178},
	file = {Goldman et al. - 2009 - Neural Integrator Models.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Goldman et al. - 2009 - Neural Integrator Models.pdf:application/pdf},
}

@article{simon_architecture_1962,
	title = {The architecture of complexity},
	volume = {106},
	issn = {0003-049X},
	url = {http://www.jstor.org/stable/985254},
	number = {6},
	urldate = {2017-11-27},
	journal = {Proceedings of the American Philosophical Society},
	author = {Simon, Herbert A.},
	year = {1962},
	keywords = {hierarchy, project.lstmMEG, readme, complexity},
	pages = {467--482},
	file = {Simon - 1962 - The architecture of complexity.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Simon - 1962 - The architecture of complexity.pdf:application/pdf},
}

@article{himberger_principles_2018,
	title = {Principles of temporal processing across the cortical hierarchy},
	issn = {03064522},
	doi = {10.1016/j.neuroscience.2018.04.030},
	language = {en},
	journal = {Neuroscience},
	author = {Himberger, Kevin D. and Chien, Hsiang-Yun and Honey, Christopher J.},
	month = may,
	year = {2018},
	keywords = {project.lstmMEG, fixme, TRW, project.lsnn},
	file = {Himberger et al. - 2018 - Principles of temporal processing across the corti.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Himberger et al. - 2018 - Principles of temporal processing across the corti.pdf:application/pdf},
}

@article{ralston_incorporating_2016,
	title = {Incorporating spike-rate adaptation into a rate code in mathematical and biological neurons},
	volume = {115},
	issn = {0022-3077, 1522-1598},
	url = {http://www.physiology.org/doi/10.1152/jn.00993.2015},
	doi = {10.1152/jn.00993.2015},
	language = {en},
	number = {5},
	urldate = {2018-07-12},
	journal = {Journal of Neurophysiology},
	author = {Ralston, Bridget N. and Flagg, Lucas Q. and Faggin, Eric and Birmingham, John T.},
	month = may,
	year = {2016},
	keywords = {project.lsnn, LIF, computational neuroscience, spike-rate adaptation},
	pages = {2501--2518},
}

@article{pascanu_difficulty_2012,
	title = {On the difficulty of training recurrent neural networks},
	url = {http://arxiv.org/abs/1211.5063},
	abstract = {There are two widely known issues with properly training Recurrent Neural Networks, the vanishing and the exploding gradient problems detailed in Bengio et al. (1994). In this paper we attempt to improve the understanding of the underlying issues by exploring these problems from an analytical, a geometric and a dynamical systems perspective. Our analysis is used to justify a simple yet effective solution. We propose a gradient norm clipping strategy to deal with exploding gradients and a soft constraint for the vanishing gradients problem. We validate empirically our hypothesis and proposed solutions in the experimental section.},
	urldate = {2018-07-12},
	journal = {arXiv:1211.5063 [cs]},
	author = {Pascanu, Razvan and Mikolov, Tomas and Bengio, Yoshua},
	month = nov,
	year = {2012},
	note = {arXiv: 1211.5063},
	keywords = {RNN, project.lstmMEG, readme, machine learning, project.lsnn, vanishing gradient problem},
}

@article{shi_junxing_deep_2018,
	title = {Deep recurrent neural network reveals a hierarchy of process memory during dynamic natural vision},
	volume = {0},
	issn = {1065-9471},
	url = {https://onlinelibrary-wiley-com.ru.idm.oclc.org/doi/10.1002/hbm.24006},
	doi = {10.1002/hbm.24006},
	abstract = {Abstract The human visual cortex extracts both spatial and temporal visual features to support perception and guide behavior. Deep convolutional neural networks (CNNs) provide a computational framework to model cortical representation and organization for spatial visual processing, but unable to explain how the brain processes temporal information. To overcome this limitation, we extended a CNN by adding recurrent connections to different layers of the CNN to allow spatial representations to be remembered and accumulated over time. The extended model, or the recurrent neural network (RNN), embodied a hierarchical and distributed model of process memory as an integral part of visual processing. Unlike the CNN, the RNN learned spatiotemporal features from videos to enable action recognition. The RNN better predicted cortical responses to natural movie stimuli than the CNN, at all visual areas, especially those along the dorsal stream. As a fully observable model of visual processing, the RNN also revealed a cortical hierarchy of temporal receptive window, dynamics of process memory, and spatiotemporal representations. These results support the hypothesis of process memory, and demonstrate the potential of using the RNN for in?depth computational understanding of dynamic natural vision.},
	number = {0},
	urldate = {2018-03-24},
	journal = {Human Brain Mapping},
	author = {{Shi Junxing} and {Wen Haiguang} and {Zhang Yizhen} and {Han Kuan} and {Liu Zhongming}},
	month = feb,
	year = {2018},
	keywords = {deep learning, fMRI, project.lstmMEG, read, vision, project.lsnn, ANN, project.lstmMRI, recurrent neural network},
	file = {Shi Junxing et al. - 2018 - Deep recurrent neural network reveals a hierarchy .pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Shi Junxing et al. - 2018 - Deep recurrent neural network reveals a hierarchy .pdf:application/pdf},
}

@article{shi_deep_2017,
	title = {Deep recurrent neural network reveals a hierarchy of process memory during dynamic natural vision},
	copyright = {© 2017, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial 4.0 International), CC BY-NC 4.0, as described at http://creativecommons.org/licenses/by-nc/4.0/},
	url = {https://www.biorxiv.org/content/early/2017/08/17/177196},
	doi = {10.1101/177196},
	abstract = {The human visual cortex extracts both spatial and temporal visual features to support perception and guide behavior. Deep convolutional neural networks (CNNs) provide a computational framework to model cortical representation and organization for spatial visual processing, but unable to explain how the brain processes temporal information. To overcome this limitation, we extended a CNN by adding recurrent connections to different layers of the CNN to allow spatial representations to be remembered and accumulated over time. The extended model, or the recurrent neural network (RNN), embodied a hierarchical and distributed model of process memory as an integral part of visual processing. Unlike the CNN, the RNN learned spatiotemporal features from videos to enable action recognition. The RNN better predicted cortical responses to natural movie stimuli than the CNN, at all visual areas especially those along the dorsal stream. As a fully-observable model of visual processing, the RNN also revealed a cortical hierarchy of temporal receptive window, dynamics of process memory, and spatiotemporal representations. These results support the hypothesis of process memory, and demonstrate the potential of using the RNN for in-depth computational understanding of dynamic natural vision.},
	language = {en},
	urldate = {2017-11-20},
	journal = {bioRxiv},
	author = {Shi, Junxing and Wen, Haiguang and Zhang, Yizhen and Han, Kuan and Liu, Zhongming},
	month = aug,
	year = {2017},
	keywords = {project.lstmMEG, read, TRW, computation, processing memory, ANN, memory},
	pages = {177196},
	file = {Shi et al. - 2017 - Deep Recurrent Neural Network Reveals a Hierarchy .pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Shi et al. - 2017 - Deep Recurrent Neural Network Reveals a Hierarchy .pdf:application/pdf},
}

@article{sundermeyer_feedforward_2015,
	title = {From feedforward to recurrent {LSTM} neural networks for language modeling},
	volume = {23},
	issn = {2329-9290},
	doi = {10.1109/TASLP.2015.2400218},
	number = {3},
	journal = {IEEE/ACM Trans. Audio, Speech and Lang. Proc.},
	author = {Sundermeyer, Martin and Ney, Hermann and Schlüter, Ralf},
	month = mar,
	year = {2015},
	keywords = {LSTM, RNN, project.lstmMEG, readme, language modeling, project.lsnn, feedforward neural network, Kneser-Ney smoothing},
	pages = {517--529},
}

@article{linzen_assessing_2016-1,
	title = {Assessing the ability of {LSTMs} to learn syntax-sensitive dependencies},
	url = {http://arxiv.org/abs/1611.01368},
	abstract = {The success of long short-term memory (LSTM) neural networks in language processing is typically attributed to their ability to capture long-distance statistical regularities. Linguistic regularities are often sensitive to syntactic structure; can such dependencies be captured by LSTMs, which do not have explicit structural representations? We begin addressing this question using number agreement in English subject-verb dependencies. We probe the architecture's grammatical competence both using training objectives with an explicit grammatical target (number prediction, grammaticality judgments) and using language models. In the strongly supervised settings, the LSTM achieved very high overall accuracy (less than 1\% errors), but errors increased when sequential and structural information conflicted. The frequency of such errors rose sharply in the language-modeling setting. We conclude that LSTMs can capture a non-trivial amount of grammatical structure given targeted supervision, but stronger architectures may be required to further reduce errors; furthermore, the language modeling signal is insufficient for capturing syntax-sensitive dependencies, and should be supplemented with more direct supervision if such dependencies need to be captured.},
	urldate = {2018-07-10},
	journal = {arXiv:1611.01368 [cs]},
	author = {Linzen, Tal and Dupoux, Emmanuel and Goldberg, Yoav},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.01368},
	keywords = {syntax, LSTM, project.lstmMEG, readme, computational linguistics, neural networks},
}

@inproceedings{mikolov_recurrent_2010,
	title = {Recurrent neural network based language model},
	url = {http://www.isca-speech.org/archive/interspeech_2010/i10_1045.html},
	booktitle = {{INTERSPEECH} 2010, 11th {Annual} {Conference} of the {International} {Speech} {Communication} {Association}, {Makuhari}, {Chiba}, {Japan}, {September} 26-30, 2010},
	author = {Mikolov, Tomas and Karafiát, Martin and Burget, Lukás and Cernocký, Jan and Khudanpur, Sanjeev},
	year = {2010},
	keywords = {language model, project.lstmMEG, readme, computational linguistics, project.lsnn, recurrent neural network, SRN},
	pages = {1045--1048},
}

@article{hochreiter_long_1997,
	title = {Long short-term memory},
	volume = {9},
	issn = {0899-7667, 1530-888X},
	url = {http://www.mitpressjournals.org/doi/10.1162/neco.1997.9.8.1735},
	doi = {10.1162/neco.1997.9.8.1735},
	language = {en},
	number = {8},
	urldate = {2018-01-20},
	journal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	month = nov,
	year = {1997},
	keywords = {LSTM, RNN, project.lstmMEG, readme, TRW, machine learning, project.lsnn, ANN},
	pages = {1735--1780},
	file = {Hochreiter and Schmidhuber - 1997 - Long short-term memory.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Hochreiter and Schmidhuber - 1997 - Long short-term memory.pdf:application/pdf},
}

@article{greff_lstm:_2017,
	title = {{LSTM}: {A} search space odyssey},
	volume = {28},
	issn = {2162-237X, 2162-2388},
	shorttitle = {{LSTM}},
	doi = {10.1109/TNNLS.2016.2582924},
	abstract = {Several variants of the Long Short-Term Memory (LSTM) architecture for recurrent neural networks have been proposed since its inception in 1995. In recent years, these networks have become the state-of-the-art models for a variety of machine learning problems. This has led to a renewed interest in understanding the role and utility of various computational components of typical LSTM variants. In this paper, we present the first large-scale analysis of eight LSTM variants on three representative tasks: speech recognition, handwriting recognition, and polyphonic music modeling. The hyperparameters of all LSTM variants for each task were optimized separately using random search, and their importance was assessed using the powerful fANOVA framework. In total, we summarize the results of 5400 experimental runs (\${\textbackslash}approx 15\$ years of CPU time), which makes our study the largest of its kind on LSTM networks. Our results show that none of the variants can improve upon the standard LSTM architecture significantly, and demonstrate the forget gate and the output activation function to be its most critical components. We further observe that the studied hyperparameters are virtually independent and derive guidelines for their efficient adjustment.},
	number = {10},
	urldate = {2018-05-28},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Greff, Klaus and Srivastava, Rupesh Kumar and Koutník, Jan and Steunebrink, Bas R. and Schmidhuber, Jürgen},
	month = oct,
	year = {2017},
	note = {arXiv: 1503.04069},
	keywords = {LSTM, project.lstmMEG, readme, neural networks, machine learning, project.lsnn, ANN, project.lstmMRI},
	pages = {2222--2232},
	file = {Greff et al. - 2017 - LSTM A search space odyssey.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Greff et al. - 2017 - LSTM A search space odyssey.pdf:application/pdf},
}

@article{kass_computational_2018,
	title = {Computational neuroscience: mathematical and statistical perspectives},
	volume = {5},
	issn = {2326-8298, 2326-831X},
	shorttitle = {Computational {Neuroscience}},
	doi = {10.1146/annurev-statistics-041715-033733},
	language = {en},
	number = {1},
	journal = {Annual Review of Statistics and Its Application},
	author = {Kass, Robert E. and Amari, Shun-Ichi and Arai, Kensuke and Brown, Emery N. and Diekman, Casey O. and Diesmann, Markus and Doiron, Brent and Eden, Uri T. and Fairhall, Adrienne L. and Fiddyment, Grant M. and Fukai, Tomoki and Grün, Sonja and Harrison, Matthew T. and Helias, Moritz and Nakahara, Hiroyuki and Teramae, Jun-nosuke and Thomas, Peter J. and Reimers, Mark and Rodu, Jordan and Rotstein, Horacio G. and Shea-Brown, Eric and Shimazaki, Hideaki and Shinomoto, Shigeru and Yu, Byron M. and Kramer, Mark A.},
	month = mar,
	year = {2018},
	keywords = {readme, statistics, fixme, project.lsnn, LIF, computational neuroscience, spiking neurons, brain-as-computer metaphor},
	pages = {183--214},
	file = {Kass et al. - 2018 - Computational neuroscience mathematical and stati.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Kass et al. - 2018 - Computational neuroscience mathematical and stati.pdf:application/pdf},
}

@article{hopfield_neural_1982,
	title = {Neural networks and physical systems with emergent collective computational abilities.},
	volume = {79},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/cgi/doi/10.1073/pnas.79.8.2554},
	doi = {10.1073/pnas.79.8.2554},
	language = {en},
	number = {8},
	urldate = {2018-07-05},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Hopfield, J. J.},
	month = apr,
	year = {1982},
	keywords = {readme, neural networks, machine learning, ANN, Hopfield, hopfield network},
	pages = {2554--2558},
}

@article{naselaris_encoding_2011,
	series = {Multivariate {Decoding} and {Brain} {Reading}},
	title = {Encoding and decoding in {fMRI}},
	volume = {56},
	issn = {1053-8119},
	url = {http://www.sciencedirect.com/science/article/pii/S1053811910010657},
	doi = {10.1016/j.neuroimage.2010.07.073},
	abstract = {Over the past decade fMRI researchers have developed increasingly sensitive techniques for analyzing the information represented in BOLD activity. The most popular of these techniques is linear classification, a simple technique for decoding information about experimental stimuli or tasks from patterns of activity across an array of voxels. A more recent development is the voxel-based encoding model, which describes the information about the stimulus or task that is represented in the activity of single voxels. Encoding and decoding are complementary operations: encoding uses stimuli to predict activity while decoding uses activity to predict information about the stimuli. However, in practice these two operations are often confused, and their respective strengths and weaknesses have not been made clear. Here we use the concept of a linearizing feature space to clarify the relationship between encoding and decoding. We show that encoding and decoding operations can both be used to investigate some of the most common questions about how information is represented in the brain. However, focusing on encoding models offers two important advantages over decoding. First, an encoding model can in principle provide a complete functional description of a region of interest, while a decoding model can provide only a partial description. Second, while it is straightforward to derive an optimal decoding model from an encoding model it is much more difficult to derive an encoding model from a decoding model. We propose a systematic modeling approach that begins by estimating an encoding model for every voxel in a scan and ends by using the estimated encoding models to perform decoding.},
	number = {2},
	urldate = {2018-02-16},
	journal = {NeuroImage},
	author = {Naselaris, Thomas and Kay, Kendrick N. and Nishimoto, Shinji and Gallant, Jack L.},
	month = may,
	year = {2011},
	keywords = {fMRI, project.lstmMEG, read, computational neuroscience, decoding, encoding},
	pages = {400--410},
}

@article{hill_goldilocks_2015,
	title = {The goldilocks principle: reading children's books with explicit memory representations},
	shorttitle = {The {Goldilocks} {Principle}},
	url = {http://arxiv.org/abs/1511.02301},
	abstract = {We introduce a new test of how well language models capture meaning in children's books. Unlike standard language modelling benchmarks, it distinguishes the task of predicting syntactic function words from that of predicting lower-frequency words, which carry greater semantic content. We compare a range of state-of-the-art models, each with a different way of encoding what has been previously read. We show that models which store explicit representations of long-term contexts outperform state-of-the-art neural language models at predicting semantic content words, although this advantage is not observed for syntactic function words. Interestingly, we find that the amount of text encoded in a single memory representation is highly influential to the performance: there is a sweet-spot, not too big and not too small, between single words and full sentences that allows the most meaningful information in a text to be effectively retained and recalled. Further, the attention over such window-based memories can be trained effectively through self-supervision. We then assess the generality of this principle by applying it to the CNN QA benchmark, which involves identifying named entities in paraphrased summaries of news articles, and achieve state-of-the-art performance.},
	journal = {arXiv:1511.02301 [cs]},
	author = {Hill, Felix and Bordes, Antoine and Chopra, Sumit and Weston, Jason},
	month = nov,
	year = {2015},
	note = {arXiv: 1511.02301},
	keywords = {LSTM, project.lstmMEG, read, narrative, recurrent neural network},
}

@article{titley_toward_2017,
	title = {Toward a neurocentric view of learning},
	volume = {95},
	issn = {08966273},
	doi = {10.1016/j.neuron.2017.05.021},
	language = {en},
	number = {1},
	urldate = {2018-06-16},
	journal = {Neuron},
	author = {Titley, Heather K. and Brunel, Nicolas and Hansel, Christian},
	month = jul,
	year = {2017},
	keywords = {learning, read, project.lsnn, LTP, membrane excitability, membrane potential, memory engram},
	pages = {19--32},
	file = {Titley et al. - 2017 - Toward a neurocentric view of learning.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Titley et al. - 2017 - Toward a neurocentric view of learning.pdf:application/pdf},
}

@article{karpathy_visualizing_2015,
	title = {Visualizing and understanding recurrent networks},
	url = {http://arxiv.org/abs/1506.02078},
	abstract = {Recurrent Neural Networks (RNNs), and specifically a variant with Long Short-Term Memory (LSTM), are enjoying renewed interest as a result of successful applications in a wide range of machine learning problems that involve sequential data. However, while LSTMs provide exceptional results in practice, the source of their performance and their limitations remain rather poorly understood. Using character-level language models as an interpretable testbed, we aim to bridge this gap by providing an analysis of their representations, predictions and error types. In particular, our experiments reveal the existence of interpretable cells that keep track of long-range dependencies such as line lengths, quotes and brackets. Moreover, our comparative analysis with finite horizon n-gram models traces the source of the LSTM improvements to long-range structural dependencies. Finally, we provide analysis of the remaining errors and suggests areas for further study.},
	urldate = {2018-05-08},
	journal = {arXiv:1506.02078 [cs]},
	author = {Karpathy, Andrej and Johnson, Justin and Fei-Fei, Li},
	month = jun,
	year = {2015},
	note = {arXiv: 1506.02078},
	keywords = {RNN, RNN interpretability, project.lstmMEG, fixme, read, neural networks, machine learning, ANN},
	file = {Karpathy et al. - 2015 - Visualizing and understanding recurrent networks.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Karpathy et al. - 2015 - Visualizing and understanding recurrent networks.pdf:application/pdf},
}

@article{hupkes_visualisation_2017,
	title = {Visualisation and 'diagnostic classifiers' reveal how recurrent and recursive neural networks process hierarchical structure},
	url = {http://arxiv.org/abs/1711.10203},
	abstract = {We investigate how neural networks can learn and process languages with hierarchical, compositional semantics. To this end, we define the artificial task of processing nested arithmetic expressions, and study whether different types of neural networks can learn to compute their meaning. We find that recursive neural networks can find a generalising solution to this problem, and we visualise this solution by breaking it up in three steps: project, sum and squash. As a next step, we investigate recurrent neural networks, and show that a gated recurrent unit, that processes its input incrementally, also performs very well on this task. To develop an understanding of what the recurrent network encodes, visualisation techniques alone do not suffice. Therefore, we develop an approach where we formulate and test multiple hypotheses on the information encoded and processed by the network. For each hypothesis, we derive predictions about features of the hidden state representations at each time step, and train 'diagnostic classifiers' to test those predictions. Our results indicate that the networks follow a strategy similar to our hypothesised 'cumulative strategy', which explains the high accuracy of the network on novel expressions, the generalisation to longer expressions than seen in training, and the mild deterioration with increasing length. This is turn shows that diagnostic classifiers can be a useful technique for opening up the black box of neural networks. We argue that diagnostic classification, unlike most visualisation techniques, does scale up from small networks in a toy domain, to larger and deeper recurrent networks dealing with real-life data, and may therefore contribute to a better understanding of the internal dynamics of current state-of-the-art models in natural language processing.},
	urldate = {2018-06-27},
	journal = {arXiv:1711.10203 [cs]},
	author = {Hupkes, Dieuwke and Veldhoen, Sara and Zuidema, Willem},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.10203},
	keywords = {syntax, LSTM, RNN, RNN interpretability, project.lstmMEG, readme, machine learning, ANN, GRU},
}

@article{bastiaansen_syntactic_2002,
	title = {Syntactic processing modulates the theta {Rhythm} of the human {EEG}},
	volume = {17},
	issn = {10538119},
	doi = {10.1006/nimg.2002.1275},
	language = {en},
	number = {3},
	urldate = {2018-03-21},
	journal = {NeuroImage},
	author = {Bastiaansen, Marcel C.M. and van Berkum, Jos J.A. and Hagoort, Peter},
	month = nov,
	year = {2002},
	keywords = {project.streams, sentence comprehension, oscillations},
	pages = {1479--1492},
}

@article{marslen-wilson_functional_1987,
	title = {Functional parallelism in spoken word-recognition},
	volume = {25},
	issn = {00100277},
	doi = {10.1016/0010-0277(87)90005-9},
	language = {en},
	number = {1-2},
	journal = {Cognition},
	author = {Marslen-Wilson, William D.},
	month = mar,
	year = {1987},
	keywords = {project.streams, readme, auditory comprehension, cohort model, lexical competition, word recognition},
	pages = {71--102},
}

@article{elman_finding_1990,
	title = {Finding structure in time},
	volume = {14},
	issn = {03640213},
	doi = {10.1016/0364-0213(90)90002-E},
	language = {en},
	number = {2},
	journal = {Cognitive Science},
	author = {Elman, J},
	month = jun,
	year = {1990},
	keywords = {RNN, project.lstmMEG, readme, neural networks, machine learning, project.lsnn, ANN, time, core},
	pages = {179--211},
	file = {Elman - 1990 - Finding structure in time.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Elman - 1990 - Finding structure in time.pdf:application/pdf},
}

@article{gulordava_colorless_2018-1,
	title = {Colorless green recurrent networks dream hierarchically},
	url = {http://arxiv.org/abs/1803.11138},
	abstract = {Recurrent neural networks (RNNs) have achieved impressive results in a variety of linguistic processing tasks, suggesting that they can induce non-trivial properties of language. We investigate here to what extent RNNs learn to track abstract hierarchical syntactic structure. We test whether RNNs trained with a generic language modeling objective in four languages (Italian, English, Hebrew, Russian) can predict long-distance number agreement in various constructions. We include in our evaluation nonsensical sentences where RNNs cannot rely on semantic or lexical cues ("The colorless green ideas I ate with the chair sleep furiously"), and, for Italian, we compare model performance to human intuitions. Our language-model-trained RNNs make reliable predictions about long-distance agreement, and do not lag much behind human performance. We thus bring support to the hypothesis that RNNs are not just shallow-pattern extractors, but they also acquire deeper grammatical competence.},
	urldate = {2018-06-27},
	journal = {arXiv:1803.11138 [cs]},
	author = {Gulordava, Kristina and Bojanowski, Piotr and Grave, Edouard and Linzen, Tal and Baroni, Marco},
	month = mar,
	year = {2018},
	note = {arXiv: 1803.11138},
	keywords = {syntax, LSTM, RNN, project.lstmMEG, readme, computational linguistics, machine learning},
	file = {Gulordava et al. - 2018 - Colorless green recurrent networks dream hierarchi.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Gulordava et al. - 2018 - Colorless green recurrent networks dream hierarchi.pdf:application/pdf},
}

@article{ettinger_role_2014,
	title = {The role of morphology in phoneme prediction: {Evidence} from {MEG}},
	volume = {129},
	issn = {0093934X},
	shorttitle = {The role of morphology in phoneme prediction},
	doi = {10.1016/j.bandl.2013.11.004},
	language = {en},
	journal = {Brain and Language},
	author = {Ettinger, Allyson and Linzen, Tal and Marantz, Alec},
	month = feb,
	year = {2014},
	keywords = {MEG, project.streams, entropy, surprisal, auditory comprehension, word recognition, morphology},
	pages = {14--23},
}

@article{holdgraf_encoding_2017,
	title = {Encoding and decoding models in cognitive electrophysiology},
	volume = {11},
	issn = {1662-5137},
	url = {http://journal.frontiersin.org/article/10.3389/fnsys.2017.00061/full},
	doi = {10.3389/fnsys.2017.00061},
	urldate = {2018-06-25},
	journal = {Frontiers in Systems Neuroscience},
	author = {Holdgraf, Christopher R. and Rieger, Jochem W. and Micheli, Cristiano and Martin, Stephanie and Knight, Robert T. and Theunissen, Frederic E.},
	month = sep,
	year = {2017},
	keywords = {MEG, EEG, project.lstmMEG, readme, project.vsmMEG, decoding models, ecoding models},
}

@article{wu_complete_2006,
	title = {Complete functional characterization of sensory neurons by system identification},
	volume = {29},
	doi = {10.1146/annurev.neuro.29.051605.113024},
	abstract = {AbstractSystem identification is a growing approach to sensory neurophysiology that facilitates the development of quantitative functional models of sensory processing. This approach provides a clear set of guidelines for combining experimental data with other knowledge about sensory function to obtain a description that optimally predicts the way that neurons process sensory information. This prediction paradigm provides an objective method for evaluating and comparing computational models. In this chapter we review many of the system identification algorithms that have been used in sensory neurophysiology, and we show how they can be viewed as variants of a single statistical inference problem. We then review many of the practical issues that arise when applying these methods to neurophysiological experiments: stimulus selection, behavioral control, model visualization, and validation. Finally we discuss several problems to which system identification has been applied recently, including one important long-term goal of sensory neuroscience: developing models of sensory systems that accurately predict neuronal responses under completely natural conditions.},
	number = {1},
	journal = {Annual Review of Neuroscience},
	author = {Wu, Michael C.-K. and David, Stephen V. and Gallant, Jack L.},
	year = {2006},
	pmid = {16776594},
	keywords = {project.lstmMEG, readme, project.vsmMEG, encoding models, computational neuroscience, sensory neuroscience},
	pages = {477--505},
}

@article{kriegeskorte_deep_2015,
	title = {Deep neural networks: {A} new framework for modeling biological vision and brain information processing},
	volume = {1},
	issn = {2374-4642, 2374-4650},
	shorttitle = {Deep {Neural} {Networks}},
	doi = {10.1146/annurev-vision-082114-035447},
	language = {en},
	number = {1},
	journal = {Annual Review of Vision Science},
	author = {Kriegeskorte, Nikolaus},
	month = nov,
	year = {2015},
	keywords = {deep learning, project.lstmMEG, readme, CNN, computation, vision, ANN},
	pages = {417--446},
}

@article{van_gerven_primer_2017,
	title = {A primer on encoding models in sensory neuroscience},
	volume = {76},
	issn = {00222496},
	doi = {10.1016/j.jmp.2016.06.009},
	language = {en},
	urldate = {2018-06-25},
	journal = {Journal of Mathematical Psychology},
	author = {van Gerven, Marcel A.J.},
	month = feb,
	year = {2017},
	keywords = {project.lstmMEG, project.vsmMEG, read, neural networks, ANN, encoding models, model-based},
	pages = {172--183},
}

@article{tinbergen_aims_1963,
	title = {On aims and methods of ethology},
	volume = {20},
	issn = {00443573},
	url = {http://doi.wiley.com/10.1111/j.1439-0310.1963.tb01161.x},
	doi = {10.1111/j.1439-0310.1963.tb01161.x},
	language = {en},
	number = {4},
	urldate = {2018-06-03},
	journal = {Zeitschrift für Tierpsychologie},
	author = {Tinbergen, N.},
	year = {1963},
	keywords = {readme, explanation, philosophy of science, induction, biology, ethology, phenomenon},
	pages = {410--433},
	file = {Tinbergen - 1963 - On aims and methods of ethology.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Tinbergen - 1963 - On aims and methods of ethology.pdf:application/pdf},
}

@article{wang_neurophysiological_2010,
	title = {Neurophysiological and computational principles of cortical rhythms in cognition},
	volume = {90},
	issn = {0031-9333, 1522-1210},
	doi = {10.1152/physrev.00035.2008},
	language = {en},
	number = {3},
	journal = {Physiological Reviews},
	author = {Wang, Xiao-Jing},
	month = jul,
	year = {2010},
	keywords = {project.streams, readme, fixme, computation, neural oscillations, cortical circuits},
	pages = {1195--1268},
}

@article{kielar_localization_2015,
	title = {Localization of electrophysiological responses to semantic and syntactic anomalies in language comprehension with {MEG}},
	volume = {105},
	issn = {10538119},
	doi = {10.1016/j.neuroimage.2014.11.016},
	language = {en},
	journal = {NeuroImage},
	author = {Kielar, Aneta and Panamsky, Lilia and Links, Kira A. and Meltzer, Jed A.},
	month = jan,
	year = {2015},
	keywords = {MEG, cognitive neuroscience, project.streams, oscillations},
	pages = {507--524},
}

@article{jenkinson_fsl_2012,
	title = {{FSL}},
	volume = {62},
	issn = {10538119},
	doi = {10.1016/j.neuroimage.2011.09.015},
	language = {en},
	number = {2},
	journal = {NeuroImage},
	author = {Jenkinson, Mark and Beckmann, Christian F. and Behrens, Timothy E.J. and Woolrich, Mark W. and Smith, Stephen M.},
	month = aug,
	year = {2012},
	keywords = {methods, project.streams, MRI, preprocessing},
	pages = {782--790},
}

@article{hasson_neurobiology_2017,
	title = {The neurobiology of uncertainty: implications for statistical learning},
	volume = {372},
	copyright = {© 2016 The Author(s). http://royalsocietypublishing.org/licencePublished by the Royal Society. All rights reserved.},
	issn = {0962-8436, 1471-2970},
	shorttitle = {The neurobiology of uncertainty},
	doi = {10.1098/rstb.2016.0048},
	abstract = {The capacity for assessing the degree of uncertainty in the environment relies on estimating statistics of temporally unfolding inputs. This, in turn, allows calibration of predictive and bottom-up processing, and signalling changes in temporally unfolding environmental features. In the last decade, several studies have examined how the brain codes for and responds to input uncertainty. Initial neurobiological experiments implicated frontoparietal and hippocampal systems, based largely on paradigms that manipulated distributional features of visual stimuli. However, later work in the auditory domain pointed to different systems, whose activation profiles have interesting implications for computational and neurobiological models of statistical learning (SL). This review begins by briefly recapping the historical development of ideas pertaining to the sensitivity to uncertainty in temporally unfolding inputs. It then discusses several issues at the interface of studies of uncertainty and SL. Following, it presents several current treatments of the neurobiology of uncertainty and reviews recent findings that point to principles that serve as important constraints on future neurobiological theories of uncertainty, and relatedly, SL. This review suggests it may be useful to establish closer links between neurobiological research on uncertainty and SL, considering particularly mechanisms sensitive to local and global structure in inputs, the degree of input uncertainty, the complexity of the system generating the input, learning mechanisms that operate on different temporal scales and the use of learnt information for online prediction.
This article is part of the themed issue ‘New frontiers for statistical learning in the cognitive sciences’.},
	language = {en},
	number = {1711},
	journal = {Phil. Trans. R. Soc. B},
	author = {Hasson, Uri},
	month = jan,
	year = {2017},
	pmid = {27872367},
	keywords = {project.streams, read, entropy, information theory, probability theory, surprisal, neuroscience, mutual information, perception, uncertainty},
	pages = {20160048},
	file = {Hasson - 2017 - The neurobiology of uncertainty implications for .pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Hasson - 2017 - The neurobiology of uncertainty implications for .pdf:application/pdf},
}

@article{frank_word_2017,
	title = {Word predictability and semantic similarity show distinct patterns of brain activity during language comprehension},
	volume = {32},
	issn = {2327-3798},
	doi = {10.1080/23273798.2017.1323109},
	abstract = {We investigate the effects of two types of relationship between the words of a sentence or text – predictability and semantic similarity – by reanalysing electroencephalography (EEG) and functional magnetic resonance imaging (fMRI) data from studies in which participants comprehend naturalistic stimuli. Each content word's predictability given previous words is quantified by a probabilistic language model, and semantic similarity to previous words is quantified by a distributional semantics model. Brain activity time-locked to each word is regressed on the two model-derived measures. Results show that predictability and semantic similarity have near identical N400 effects but are dissociated in the fMRI data, with word predictability related to activity in, among others, the visual word-form area, and semantic similarity related to activity in areas associated with the semantic network. This indicates that both predictability and similarity play a role during natural language comprehension and modulate distinct cortical regions.},
	number = {9},
	journal = {Language, Cognition and Neuroscience},
	author = {Frank, Stefan L. and Willems, Roel M.},
	month = oct,
	year = {2017},
	keywords = {language model, project.streams, distributional semantics, project.vsmMEG, VSM, read, surprisal, n-gram, word embedding},
	pages = {1192--1203},
	file = {Frank and Willems - 2017 - Word predictability and semantic similarity show d.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Frank and Willems - 2017 - Word predictability and semantic similarity show d.pdf:application/pdf},
}

@article{ferstl_extended_2008,
	title = {The extended language network: {A} meta-analysis of neuroimaging studies on text comprehension},
	volume = {29},
	issn = {10659471, 10970193},
	shorttitle = {The extended language network},
	doi = {10.1002/hbm.20422},
	language = {en},
	number = {5},
	journal = {Human Brain Mapping},
	author = {Ferstl, Evelyn C. and Neumann, Jane and Bogler, Carsten and von Cramon, D. Yves},
	month = may,
	year = {2008},
	keywords = {cognitive neuroscience, project.streams, readme, narrative comprehension},
	pages = {581--593},
}

@article{chater_probabilistic_2006-1,
	title = {Probabilistic models of language processing and acquisition},
	volume = {10},
	issn = {1364-6613, 1879-307X},
	doi = {10.1016/j.tics.2006.05.006},
	language = {English},
	number = {7},
	journal = {Trends in Cognitive Sciences},
	author = {Chater, Nick and Manning, Christopher D.},
	month = jul,
	year = {2006},
	pmid = {16784883},
	keywords = {project.streams, read, probabilistic models of cognition, probability theory, computational linguistics, language modeling},
	pages = {335--344},
	file = {Chater and Manning - 2006 - Probabilistic models of language processing and ac.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Chater and Manning - 2006 - Probabilistic models of language processing and ac.pdf:application/pdf},
}

@article{price_degeneracy_2002,
	title = {Degeneracy and cognitive anatomy},
	volume = {6},
	issn = {13646613},
	doi = {10.1016/S1364-6613(02)01976-9},
	language = {en},
	number = {10},
	urldate = {2018-06-21},
	journal = {Trends in Cognitive Sciences},
	author = {Price, Cathy J and Friston, Karl J},
	month = oct,
	year = {2002},
	keywords = {project.streams, degeneracy, lesion symptom mapping, localization},
	pages = {416--421},
}

@article{cromwell_rethinking_2011,
	series = {Pioneering {Research} in {Affective} {Neuroscience}: {Celebrating} the {Work} of {Dr}. {Jaak} {Panksepp}},
	title = {Rethinking the cognitive revolution from a neural perspective: {How} overuse/misuse of the term ‘cognition’ and the neglect of affective controls in behavioral neuroscience could be delaying progress in understanding the {BrainMind}},
	volume = {35},
	issn = {0149-7634},
	shorttitle = {Rethinking the cognitive revolution from a neural perspective},
	url = {http://www.sciencedirect.com/science/article/pii/S0149763411000273},
	doi = {10.1016/j.neubiorev.2011.02.008},
	abstract = {Words such as cognition, motivation and emotion powerfully guide theory development and the overall aims and goals of behavioral neuroscience research. Once such concepts are accepted generally as natural aspects of the brain, their influence can be pervasive and long lasting. Importantly, the choice of conceptual terms used to describe and study mental/neural functions can also constrain research by forcing the results into seemingly useful ‘conceptual’ categories that have no discrete reality in the brain. Since the popularly named ‘cognitive revolution’ in psychological science came to fruition in the early 1970s, the term cognitive or cognition has been perhaps the most widely used conceptual term in behavioral neuroscience. These terms, similar to other conceptual terms, have potential value if utilized appropriately. We argue that recently the term cognition has been both overused and misused. This has led to problems in developing a usable shared definition for the term and to promotion of possible misdirections in research within behavioral neuroscience. In addition, we argue that cognitive-guided research influenced primarily by top-down (cortical toward subcortical) perspectives without concurrent non-cognitive modes of bottom-up developmental thinking, could hinder progress in the search for new treatments and medications for psychiatric illnesses and neurobehavioral disorders. Overall, linkages of animal research insights to human psychology may be better served by bottom-up (subcortical to cortical) affective and motivational ‘state-control’ perspectives, simply because the lower networks of the brain are foundational for the construction of higher ‘information-processing’ aspects of mind. Moving forward, rapidly expanding new techniques and creative methods in neuroscience along with more accurate brain concepts, may help guide the development of new therapeutics and hopefully more accurate ways to describe and explain brain-behavior relationships.},
	number = {9},
	urldate = {2018-06-11},
	journal = {Neuroscience \& Biobehavioral Reviews},
	author = {Cromwell, Howard Casey and Panksepp, Jaak},
	month = oct,
	year = {2011},
	keywords = {readme, explanation, philosophy of neuroscience, philosophy of mind},
	pages = {2026--2035},
	file = {Cromwell and Panksepp - 2011 - Rethinking the cognitive revolution from a neural .pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Cromwell and Panksepp - 2011 - Rethinking the cognitive revolution from a neural .pdf:application/pdf},
}

@article{lima_roles_2016,
	title = {Roles of supplementary motor areas in auditory processing and auditory imagery},
	volume = {39},
	issn = {01662236},
	doi = {10.1016/j.tins.2016.06.003},
	language = {en},
	number = {8},
	urldate = {2018-06-20},
	journal = {Trends in Neurosciences},
	author = {Lima, César F. and Krishnan, Saloni and Scott, Sophie K.},
	month = aug,
	year = {2016},
	keywords = {project.streams, read, speech comprehension, motor cortex, pre-SMA, SMA},
	pages = {527--542},
}

@article{akram_dynamic_2017,
	title = {Dynamic estimation of the auditory temporal response function from {MEG} in competing-speaker environments},
	volume = {64},
	issn = {0018-9294},
	doi = {10.1109/TBME.2016.2628884},
	abstract = {Objective: A central problem in computational neuroscience is to characterize brain function using neural activity recorded from the brain in response to sensory inputs with statistical confidence. Most of existing estimation techniques, such as those based on reverse correlation, exhibit two main limitations: first, they are unable to produce dynamic estimates of the neural activity at a resolution comparable with that of the recorded data, and second, they often require heavy averaging across time as well as multiple trials in order to construct statistical confidence intervals for a precise interpretation of data. In this paper, we address the above-mentioned issues for estimating auditory temporal response function (TRF) as a parametric computational model for selective auditory attention in competing-speaker environments. Methods: The TRF is a sparse kernel which regresses auditory MEG data with respect to the envelopes of the speech streams. We develop an efficient estimation technique by exploiting the sparsity of the TRF and adopting an ℓ1-regularized least squares estimator which is capable of producing dynamic TRF estimates as well as confidence intervals at sampling resolution from single-trial MEG data. Results: We evaluate the performance of our proposed estimator using evoked MEG responses from the human brain in an auditory attention experiment with two competing speakers. The TRFs are estimated dynamically over time using the proposed technique with multisecond resolution, which is a significant improvement over previous results with a temporal resolution of the order of a minute. Conclusion: Application of our method to MEG data reveals a precise characterization of the modulation of M50 and M100 evoked responses with respect to the attentional state of the subject at multisecond resolution. Significance: Our proposed estimation technique provides a high resolution real-time attention decoding framework in multispeaker- environments with potential application in smart hearing aid technology.},
	number = {8},
	journal = {IEEE Transactions on Biomedical Engineering},
	author = {Akram, S. and Simon, J. Z. and Babadi, B.},
	month = aug,
	year = {2017},
	keywords = {MEG, trf, project.vsmMEG, speech comprehension, audio envelope},
	pages = {1896--1905},
}

@article{haegens_rhythmic_2018,
	title = {Rhythmic facilitation of sensory processing: {A} critical review},
	volume = {86},
	issn = {01497634},
	shorttitle = {Rhythmic facilitation of sensory processing},
	doi = {10.1016/j.neubiorev.2017.12.002},
	language = {en},
	urldate = {2018-06-19},
	journal = {Neuroscience \& Biobehavioral Reviews},
	author = {Haegens, Saskia and Zion Golumbic, Elana},
	month = mar,
	year = {2018},
	keywords = {readme, beta, neural oscillations, theta, speech comprehension, delta, project.TE, transfer entropy},
	pages = {150--165},
}

@article{luo_electroencephalogram_2010,
	title = {Electroencephalogram oscillations differentiate semantic and prosodic processes during sentence reading},
	volume = {169},
	issn = {03064522},
	doi = {10.1016/j.neuroscience.2010.05.032},
	language = {en},
	number = {2},
	urldate = {2018-06-18},
	journal = {Neuroscience},
	author = {Luo, Y. and Zhang, Y. and Feng, X. and Zhou, X.},
	month = aug,
	year = {2010},
	keywords = {project.streams, readme, beta, neural oscillations, semantics},
	pages = {654--664},
}

@article{pfurtscheller_existence_1997,
	title = {On the existence of different types of central beta rhythms below 30 {Hz}},
	volume = {102},
	issn = {00134694},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0013469496966122},
	doi = {10.1016/S0013-4694(96)96612-2},
	language = {en},
	number = {4},
	urldate = {2018-06-18},
	journal = {Electroencephalography and Clinical Neurophysiology},
	author = {Pfurtscheller, G. and Stancák, A. and Edlinger, G.},
	month = apr,
	year = {1997},
	keywords = {cognitive neuroscience, project.streams, readme, beta, neural oscillations, motor cortex},
	pages = {316--325},
}

@article{craver_when_2006,
	title = {When mechanistic models explain},
	volume = {153},
	issn = {0039-7857, 1573-0964},
	url = {http://link.springer.com/10.1007/s11229-006-9097-x},
	doi = {10.1007/s11229-006-9097-x},
	language = {en},
	number = {3},
	urldate = {2018-06-17},
	journal = {Synthese},
	author = {Craver, Carl F.},
	month = nov,
	year = {2006},
	keywords = {readme, explanation, philosophy of neuroscience, philosophy of science, computation, project.6, philosophy},
	pages = {355--376},
}

@article{lowder_lexical_nodate,
	title = {Lexical predictability during natural reading: effects of surprisal and entropy reduction},
	issn = {1551-6709},
	shorttitle = {Lexical {Predictability} {During} {Natural} {Reading}},
	doi = {10.1111/cogs.12597},
	abstract = {What are the effects of word-by-word predictability on sentence processing times during the natural reading of a text? Although information complexity metrics such as surprisal and entropy reduction have been useful in addressing this question, these metrics tend to be estimated using computational language models, which require some degree of commitment to a particular theory of language processing. Taking a different approach, this study implemented a large-scale cumulative cloze task to collect word-by-word predictability data for 40 passages and compute surprisal and entropy reduction values in a theory-neutral manner. A separate group of participants read the same texts while their eye movements were recorded. Results showed that increases in surprisal and entropy reduction were both associated with increases in reading times. Furthermore, these effects did not depend on the global difficulty of the text. The findings suggest that surprisal and entropy reduction independently contribute to variation in reading times, as these metrics seem to capture different aspects of lexical predictability.},
	language = {en},
	urldate = {2018-03-09},
	journal = {Cognitive Science},
	author = {Lowder, Matthew W. and Choi, Wonil and Ferreira, Fernanda and Henderson, John M.},
	keywords = {fMRI, project.streams, readme, surprisal, entropy reduction},
	pages = {n/a--n/a},
	file = {Lowder et al_Lexical Predictability During Natural Reading.pdf:/Users/kriarm/Zotero/storage/LUQUTWP7/Lowder et al_Lexical Predictability During Natural Reading.pdf:application/pdf;Snapshot:/Users/kriarm/Zotero/storage/J2RIRPSF/abstract.html:text/html},
}

@article{luce_computational_1986,
	title = {A computational analysis of uniqueness points in auditory word recognition},
	volume = {39},
	issn = {0031-5117, 1532-5962},
	doi = {10.3758/BF03212485},
	language = {en},
	number = {3},
	urldate = {2018-06-14},
	journal = {Perception \& Psychophysics},
	author = {Luce, Paul A.},
	month = may,
	year = {1986},
	keywords = {project.streams, readme, cohort model, word recognition, uniqueness point},
	pages = {155--158},
}

@article{murthy_coherent_1992,
	title = {Coherent 25- to 35-{Hz} oscillations in the sensorimotor cortex of awake behaving monkeys.},
	volume = {89},
	issn = {0027-8424, 1091-6490},
	doi = {10.1073/pnas.89.12.5670},
	language = {en},
	number = {12},
	urldate = {2018-06-14},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Murthy, V. N. and Fetz, E. E.},
	month = jun,
	year = {1992},
	keywords = {project.streams, beta, motor cortex, oscillations},
	pages = {5670--5674},
}

@article{van_elk_functional_2010,
	title = {The functional role of motor activation in language processing: {Motor} cortical oscillations support lexical-semantic retrieval},
	volume = {50},
	issn = {10538119},
	shorttitle = {The functional role of motor activation in language processing},
	doi = {10.1016/j.neuroimage.2009.12.123},
	language = {en},
	number = {2},
	urldate = {2018-06-14},
	journal = {NeuroImage},
	author = {van Elk, M. and van Schie, H.T. and Zwaan, R.A. and Bekkering, H.},
	month = apr,
	year = {2010},
	keywords = {project.streams, readme, beta, oscillations, embodied cognition, simulation},
	pages = {665--677},
}

@article{arnal_deltabeta_2015,
	title = {Delta–beta coupled oscillations underlie temporal prediction accuracy},
	volume = {25},
	issn = {1047-3211, 1460-2199},
	doi = {10.1093/cercor/bhu103},
	language = {en},
	number = {9},
	urldate = {2018-06-14},
	journal = {Cerebral Cortex},
	author = {Arnal, Luc H. and Doelling, Keith B. and Poeppel, David},
	month = sep,
	year = {2015},
	keywords = {project.streams, readme, beta, motor cortex, time, oscillations, auditory perception},
	pages = {3077--3085},
}

@article{farmer_rhythmicity_1998,
	title = {Rhythmicity, synchronization and binding in human and primate motor systems},
	volume = {509},
	issn = {00223751},
	doi = {10.1111/j.1469-7793.1998.003bo.x},
	language = {en},
	number = {1},
	urldate = {2018-06-14},
	journal = {The Journal of Physiology},
	author = {Farmer, S. F.},
	month = may,
	year = {1998},
	keywords = {project.streams, readme, beta, motor cortex, oscillations},
	pages = {3--14},
}

@article{hari_activation_1998,
	title = {Activation of human primary motor cortex during action observation: {A} neuromagnetic study},
	volume = {95},
	copyright = {Copyright © 1998, The National Academy of Sciences},
	issn = {0027-8424, 1091-6490},
	shorttitle = {Activation of human primary motor cortex during action observation},
	abstract = {The monkey premotor cortex contains neurons that discharge during action execution and during observation of actions made by others. Transcranial magnetic stimulation experiments suggest that a similar observation/execution matching system also is present in humans. We recorded neuromagnetic oscillatory activity of the human precentral cortex from 10 healthy volunteers while (i) they had no task to perform, (ii) they were manipulating a small object, and (iii) they were observing another individual performing the same task. The left and right median nerves were stimulated alternately (interstimulus interval, 1.5 s) at intensities exceeding motor threshold, and the poststimulus rebound of the rolandic 15- to 25-Hz activity was quantified. In agreement with previous studies, the rebound was strongly suppressed bilaterally during object manipulation. Most interestingly, the rebound also was significantly diminished during action observation (31–46\% of the suppression during object manipulation). Control experiments, in which subjects were instructed to observe stationary or moving stimuli, confirmed the specificity of the suppression effect. Because the recorded 15- to 25-Hz activity is known to originate mainly in the precentral motor cortex, we concluded that the human primary motor cortex is activated during observation as well as execution of motor tasks. These findings have implications for a better understanding of the machinery underlying action recognition in humans.},
	language = {en},
	number = {25},
	urldate = {2018-06-14},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Hari, R. and Forss, N. and Avikainen, S. and Kirveskari, E. and Salenius, S. and Rizzolatti, G.},
	month = dec,
	year = {1998},
	pmid = {9844015},
	keywords = {project.streams, beta, motor cortex, oscillations},
	pages = {15061--15065},
}

@article{schapiro_complementary_2017,
	title = {Complementary learning systems within the hippocampus: a neural network modelling approach to reconciling episodic memory with statistical learning},
	volume = {372},
	issn = {0962-8436, 1471-2970},
	shorttitle = {Complementary learning systems within the hippocampus},
	doi = {10.1098/rstb.2016.0049},
	language = {en},
	number = {1711},
	urldate = {2018-06-13},
	journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	author = {Schapiro, Anna C. and Turk-Browne, Nicholas B. and Botvinick, Matthew M. and Norman, Kenneth A.},
	month = jan,
	year = {2017},
	keywords = {hippocampus, statistical learning},
	pages = {20160049},
}

@article{lisman_memory_2018,
	title = {Memory formation depends on both synapse-specific modifications of synaptic strength and cell-specific increases in excitability},
	volume = {21},
	copyright = {2018 The Publisher},
	issn = {1546-1726},
	url = {https://www.nature.com/articles/s41593-018-0076-6},
	doi = {10.1038/s41593-018-0076-6},
	abstract = {The authors discuss newly emerging evidence for the role of the transcription factor CREB in memory, including its role in modulating changes in excitability that are critical for neural assembly formation and linking of memories across time.},
	language = {en},
	number = {3},
	urldate = {2018-06-13},
	journal = {Nature Neuroscience},
	author = {Lisman, John and Cooper, Katherine and Sehgal, Megha and Silva, Alcino J.},
	month = mar,
	year = {2018},
	keywords = {readme, project.lsnn, processing memory, memory},
	pages = {309--314},
	file = {Lisman et al. - 2018 - Memory formation depends on both synapse-specific .pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Lisman et al. - 2018 - Memory formation depends on both synapse-specific .pdf:application/pdf},
}

@article{bengio_neural_2003,
	title = {A neural probabilistic language model},
	volume = {3},
	issn = {ISSN 1533-7928},
	url = {http://www.jmlr.org/papers/v3/bengio03a.html},
	number = {Feb},
	urldate = {2018-06-12},
	journal = {Journal of Machine Learning Research},
	author = {Bengio, Yoshua and Ducharme, Réjean and Vincent, Pascal and Jauvin, Christian},
	year = {2003},
	keywords = {readme},
	pages = {1137--1155},
	file = {Bengio et al. - 2003 - A neural probabilistic language model.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Bengio et al. - 2003 - A neural probabilistic language model.pdf:application/pdf},
}

@article{bengio_learning_1994,
	title = {Learning long-term dependencies with gradient descent is difficult},
	volume = {5},
	issn = {1045-9227},
	doi = {10.1109/72.279181},
	abstract = {Recurrent neural networks can be used to map input sequences to output sequences, such as for recognition, production or prediction problems. However, practical difficulties have been reported in training recurrent neural networks to perform tasks in which the temporal contingencies present in the input/output sequences span long intervals. We show why gradient based learning algorithms face an increasingly difficult problem as the duration of the dependencies to be captured increases. These results expose a trade-off between efficient learning by gradient descent and latching on information for long periods. Based on an understanding of this problem, alternatives to standard gradient descent are considered.},
	language = {eng},
	number = {2},
	journal = {IEEE transactions on neural networks},
	author = {Bengio, Y. and Simard, P. and Frasconi, P.},
	year = {1994},
	pmid = {18267787},
	keywords = {LSTM, project.lstmMEG, readme, neural networks, machine learning},
	pages = {157--166},
}

@article{morillon_motor_2017,
	title = {Motor origin of temporal predictions in auditory attention},
	volume = {114},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1705373114},
	doi = {10.1073/pnas.1705373114},
	language = {en},
	number = {42},
	urldate = {2018-06-11},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Morillon, Benjamin and Baillet, Sylvain},
	month = oct,
	year = {2017},
	keywords = {project.streams, readme, speech comprehension, motor cortex, oscillations},
	pages = {E8913--E8921},
}

@article{schoffelen_frequency-specific_2017,
	title = {Frequency-specific directed interactions in the human brain network for language},
	volume = {114},
	copyright = {©  . http://www.pnas.org/site/misc/userlicense.xhtml},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/content/114/30/8083},
	doi = {10.1073/pnas.1703155114},
	abstract = {The brain’s remarkable capacity for language requires bidirectional interactions between functionally specialized brain regions. We used magnetoencephalography to investigate interregional interactions in the brain network for language while 102 participants were reading sentences. Using Granger causality analysis, we identified inferior frontal cortex and anterior temporal regions to receive widespread input and middle temporal regions to send widespread output. This fits well with the notion that these regions play a central role in language processing. Characterization of the functional topology of this network, using data-driven matrix factorization, which allowed for partitioning into a set of subnetworks, revealed directed connections at distinct frequencies of interaction. Connections originating from temporal regions peaked at alpha frequency, whereas connections originating from frontal and parietal regions peaked at beta frequency. These findings indicate that the information flow between language-relevant brain areas, which is required for linguistic processing, may depend on the contributions of distinct brain rhythms.},
	language = {en},
	number = {30},
	urldate = {2018-06-08},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Schoffelen, Jan-Mathijs and Hultén, Annika and Lam, Nietzsche and Marquand, André F. and Uddén, Julia and Hagoort, Peter},
	month = jul,
	year = {2017},
	pmid = {28698376},
	keywords = {MEG, project.streams, neural oscillations, connectivity, directed connectivity, granger causality},
	pages = {8083--8088},
}

@incollection{kintsch_role_1991,
	title = {The role of knowledge in discourse comprehension: {A} construction-integration model},
	volume = {79},
	isbn = {978-0-444-88484-8},
	shorttitle = {The {Role} of {Knowledge} in {Discourse} {Comprehension}},
	language = {en},
	urldate = {2018-06-08},
	booktitle = {Advances in {Psychology}},
	publisher = {Elsevier},
	author = {Kintsch, Walter},
	year = {1991},
	doi = {10.1016/S0166-4115(08)61551-4},
	keywords = {project.streams, situation model},
	pages = {107--153},
}

@article{skipper_hearing_2017,
	title = {The hearing ear is always found close to the speaking tongue: {Review} of the role of the motor system in speech perception},
	volume = {164},
	issn = {0093-934X},
	shorttitle = {The hearing ear is always found close to the speaking tongue},
	doi = {10.1016/j.bandl.2016.10.004},
	abstract = {Does “the motor system” play “a role” in speech perception? If so, where, how, and when? We conducted a systematic review that addresses these questions using both qualitative and quantitative methods. The qualitative review of behavioural, computational modelling, non-human animal, brain damage/disorder, electrical stimulation/recording, and neuroimaging research suggests that distributed brain regions involved in producing speech play specific, dynamic, and contextually determined roles in speech perception. The quantitative review employed region and network based neuroimaging meta-analyses and a novel text mining method to describe relative contributions of nodes in distributed brain networks. Supporting the qualitative review, results show a specific functional correspondence between regions involved in non-linguistic movement of the articulators, covertly and overtly producing speech, and the perception of both nonword and word sounds. This distributed set of cortical and subcortical speech production regions are ubiquitously active and form multiple networks whose topologies dynamically change with listening context. Results are inconsistent with motor and acoustic only models of speech perception and classical and contemporary dual-stream models of the organization of language and the brain. Instead, results are more consistent with complex network models in which multiple speech production related networks and subnetworks dynamically self-organize to constrain interpretation of indeterminant acoustic patterns as listening context requires.},
	urldate = {2018-06-08},
	journal = {Brain and Language},
	author = {Skipper, Jeremy I. and Devlin, Joseph T. and Lametti, Daniel R.},
	month = jan,
	year = {2017},
	keywords = {project.streams, readme, speech comprehension, speech production},
	pages = {77--105},
	file = {ScienceDirect Snapshot:/Users/kriarm/Zotero/storage/PNX3ETWM/S0093934X16301420.html:text/html},
}

@article{zwaan_situation_1998,
	title = {Situation models in language comprehension and memory.},
	volume = {123},
	issn = {1939-1455, 0033-2909},
	doi = {10.1037/0033-2909.123.2.162},
	language = {en},
	number = {2},
	urldate = {2018-06-08},
	journal = {Psychological Bulletin},
	author = {Zwaan, Rolf A. and Radvansky, Gabriel A.},
	year = {1998},
	keywords = {project.streams, language comprehension, memory, embodied cognition, situation model},
	pages = {162--185},
}

@article{ranganath_two_2012,
	title = {Two cortical systems for memory-guided behaviour},
	volume = {13},
	copyright = {2012 Nature Publishing Group},
	issn = {1471-0048},
	doi = {10.1038/nrn3338},
	abstract = {Although the perirhinal cortex (PRC), parahippocampal cortex (PHC) and retrosplenial cortex (RSC) have an essential role in memory, the precise functions of these areas are poorly understood. Here, we review the anatomical and functional characteristics of these areas based on studies in humans, monkeys and rats. Our Review suggests that the PRC and PHC–RSC are core components of two separate large-scale cortical networks that are dissociable by neuroanatomy, susceptibility to disease and function. These networks not only support different types of memory but also appear to support different aspects of cognition.},
	language = {en},
	number = {10},
	urldate = {2018-06-08},
	journal = {Nature Reviews Neuroscience},
	author = {Ranganath, Charan and Ritchey, Maureen},
	month = oct,
	year = {2012},
	keywords = {project.streams, memory, hippocampus, episodic memory},
	pages = {713--726},
}

@article{khaligh-razavi_deep_2014,
	title = {Deep supervised, but not unsupervised, models may explain {IT} cortical representation},
	volume = {10},
	issn = {1553-7358},
	doi = {10.1371/journal.pcbi.1003915},
	language = {en},
	number = {11},
	urldate = {2018-06-06},
	journal = {PLoS Computational Biology},
	author = {Khaligh-Razavi, Seyed-Mahdi and Kriegeskorte, Nikolaus},
	editor = {Diedrichsen, Jörn},
	month = nov,
	year = {2014},
	keywords = {deep learning, project.lstmMEG, readme, project.vsmMEG, neural networks},
	pages = {e1003915},
}

@article{baldassano_discovering_2017,
	title = {Discovering event structure in continuous narrative perception and memory},
	volume = {95},
	issn = {0896-6273},
	doi = {10.1016/j.neuron.2017.06.041},
	abstract = {Summary
During realistic, continuous perception, humans automatically segment experiences into discrete events. Using a novel model of cortical event dynamics, we investigate how cortical structures generate event representations during narrative perception and how these events are stored to and retrieved from memory. Our data-driven approach allows us to detect event boundaries as shifts between stable patterns of brain activity without relying on stimulus annotations and reveals a nested hierarchy from short events in sensory regions to long events in high-order areas (including angular gyrus and posterior medial cortex), which represent abstract, multimodal situation models. High-order event boundaries are coupled to increases in hippocampal activity, which predict pattern reinstatement during later free recall. These areas also show evidence of anticipatory reinstatement as subjects listen to a familiar narrative. Based on these results, we propose that brain activity is naturally structured into nested events, which form the basis of long-term memory representations.},
	number = {3},
	urldate = {2017-12-01},
	journal = {Neuron},
	author = {Baldassano, Christopher and Chen, Janice and Zadbood, Asieh and Pillow, Jonathan W. and Hasson, Uri and Norman, Kenneth A.},
	month = aug,
	year = {2017},
	keywords = {project.lstmMEG, project.streams, readme, fixme, event structure, hidden markov model},
	pages = {709--721.e5},
	file = {Baldassano et al. - 2017 - Discovering Event Structure in Continuous Narrativ.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Baldassano et al. - 2017 - Discovering Event Structure in Continuous Narrativ.pdf:application/pdf},
}

@article{broderick_electrophysiological_2018,
	title = {Electrophysiological correlates of semantic dissimilarity reflect the comprehension of natural, narrative speech},
	volume = {0},
	issn = {0960-9822},
	url = {http://www.cell.com/current-biology/abstract/S0960-9822(18)30146-5},
	doi = {10.1016/j.cub.2018.01.080},
	language = {English},
	number = {0},
	urldate = {2018-02-24},
	journal = {Current Biology},
	author = {Broderick, Michael P. and Anderson, Andrew J. and Liberto, Giovanni M. Di and Crosse, Michael J. and Lalor, Edmund C.},
	month = feb,
	year = {2018},
	keywords = {EEG, project.lstmMEG, readme, distributional semantics, project.vsmMEG, VSM, narrative},
	file = {Broderick et al. - 2018 - Electrophysiological correlates of semantic dissim.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Broderick et al. - 2018 - Electrophysiological correlates of semantic dissim.pdf:application/pdf;mmc1.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/mmc1.pdf:application/pdf},
}

@article{huth_continuous_2012,
	title = {A continuous semantic space describes the representation of thousands of object and action categories across the human brain},
	volume = {76},
	issn = {0896-6273},
	url = {http://www.cell.com/neuron/abstract/S0896-6273(12)00934-8},
	doi = {10.1016/j.neuron.2012.10.014},
	language = {English},
	number = {6},
	urldate = {2018-02-20},
	journal = {Neuron},
	author = {Huth, Alexander G. and Nishimoto, Shinji and Vu, An T. and Gallant, Jack L.},
	month = dec,
	year = {2012},
	pmid = {23259955},
	keywords = {cognitive neuroscience, fMRI, project.lstmMEG, readme, project.vsmMEG, narrative},
	pages = {1210--1224},
}

@inproceedings{wehbe_aligning_2014,
	title = {Aligning context-based statistical models of language with brain activity during reading},
	booktitle = {{EMNLP}},
	publisher = {ACL},
	author = {Wehbe, Leila and Vaswani, Ashish and Knight, Kevin and Mitchell, Tom M.},
	year = {2014},
	keywords = {MEG, project.lstmMEG, project.vsmMEG, fixme, read, neural networks, reading comprehension},
	pages = {233--243},
}

@article{wehbe_simultaneously_2014,
	title = {Simultaneously uncovering the patterns of brain regions involved in different story reading subprocesses},
	volume = {9},
	issn = {1932-6203},
	url = {http://dx.plos.org/10.1371/journal.pone.0112575},
	doi = {10.1371/journal.pone.0112575},
	language = {en},
	number = {11},
	urldate = {2018-01-25},
	journal = {PLoS ONE},
	author = {Wehbe, Leila and Murphy, Brian and Talukdar, Partha and Fyshe, Alona and Ramdas, Aaditya and Mitchell, Tom},
	editor = {Paterson, Kevin},
	month = nov,
	year = {2014},
	keywords = {fMRI, project.lstmMEG, distributional semantics, project.vsmMEG, VSM, read, narrative, semantics},
	pages = {e112575},
}

@article{mitchell_predicting_2008,
	title = {Predicting human brain activity associated with the meanings of nouns},
	volume = {320},
	issn = {0036-8075, 1095-9203},
	url = {http://www.sciencemag.org/cgi/doi/10.1126/science.1152876},
	doi = {10.1126/science.1152876},
	language = {en},
	number = {5880},
	urldate = {2018-01-25},
	journal = {Science},
	author = {Mitchell, T. M. and Shinkareva, S. V. and Carlson, A. and Chang, K.-M. and Malave, V. L. and Mason, R. A. and Just, M. A.},
	month = may,
	year = {2008},
	keywords = {fMRI, project.lstmMEG, distributional semantics, project.vsmMEG, VSM, read},
	pages = {1191--1195},
	file = {Mitchell et al. - 2008 - Predicting human brain activity associated with th.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Mitchell et al. - 2008 - Predicting human brain activity associated with th.pdf:application/pdf},
}

@article{cadieu_deep_2014,
	title = {Deep neural networks rival the representation of primate {IT} cortex for core visual object recognition},
	volume = {10},
	issn = {1553-7358},
	doi = {10.1371/journal.pcbi.1003963},
	language = {en},
	number = {12},
	urldate = {2018-06-06},
	journal = {PLoS Computational Biology},
	author = {Cadieu, Charles F. and Hong, Ha and Yamins, Daniel L. K. and Pinto, Nicolas and Ardila, Diego and Solomon, Ethan A. and Majaj, Najib J. and DiCarlo, James J.},
	editor = {Bethge, Matthias},
	month = dec,
	year = {2014},
	keywords = {deep learning, project.lstmMEG, vision, visual cortex},
	pages = {e1003963},
}

@article{yamins_performance-optimized_2014,
	title = {Performance-optimized hierarchical models predict neural responses in higher visual cortex},
	volume = {111},
	issn = {0027-8424, 1091-6490},
	doi = {10.1073/pnas.1403112111},
	language = {en},
	number = {23},
	urldate = {2018-06-06},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Yamins, D. L. K. and Hong, H. and Cadieu, C. F. and Solomon, E. A. and Seibert, D. and DiCarlo, J. J.},
	month = jun,
	year = {2014},
	keywords = {deep learning, project.lstmMEG, readme, neural networks, vision, visual cortex},
	pages = {8619--8624},
}

@article{cichy_comparison_2016,
	title = {Comparison of deep neural networks to spatio-temporal cortical dynamics of human visual object recognition reveals hierarchical correspondence},
	volume = {6},
	issn = {2045-2322},
	doi = {10.1038/srep27755},
	language = {en},
	number = {1},
	urldate = {2018-06-05},
	journal = {Scientific Reports},
	author = {Cichy, Radoslaw Martin and Khosla, Aditya and Pantazis, Dimitrios and Torralba, Antonio and Oliva, Aude},
	month = sep,
	year = {2016},
	keywords = {MEG, deep learning, project.lstmMEG, readme, neural networks, neuroscience},
}

@article{eickenberg_seeing_2017,
	title = {Seeing it all: {Convolutional} network layers map the function of the human visual system},
	volume = {152},
	issn = {10538119},
	shorttitle = {Seeing it all},
	doi = {10.1016/j.neuroimage.2016.10.001},
	language = {en},
	urldate = {2018-06-05},
	journal = {NeuroImage},
	author = {Eickenberg, Michael and Gramfort, Alexandre and Varoquaux, Gaël and Thirion, Bertrand},
	month = may,
	year = {2017},
	keywords = {project.lstmMEG, readme, vision, recurrent neural network},
	pages = {184--194},
}

@article{guclu_modeling_2017,
	title = {Modeling the dynamics of human brain activity with recurrent neural networks},
	volume = {11},
	issn = {1662-5188},
	doi = {10.3389/fncom.2017.00007},
	urldate = {2018-06-05},
	journal = {Frontiers in Computational Neuroscience},
	author = {Güçlü, Umut and van Gerven, Marcel A. J.},
	month = feb,
	year = {2017},
	keywords = {fMRI, project.lstmMEG, read, neural networks, ANN, encoding, word embedding, ridge regression},
}

@article{kell_task-optimized_2018,
	title = {A task-optimized neural network replicates human auditory behavior, predicts brain responses, and reveals a cortical processing hierarchy},
	volume = {98},
	issn = {0896-6273},
	doi = {10.1016/j.neuron.2018.03.044},
	abstract = {Summary
A core goal of auditory neuroscience is to build quantitative models that predict cortical responses to natural sounds. Reasoning that a complete model of auditory cortex must solve ecologically relevant tasks, we optimized hierarchical neural networks for speech and music recognition. The best-performing network contained separate music and speech pathways following early shared processing, potentially replicating human cortical organization. The network performed both tasks as well as humans and exhibited human-like errors despite not being optimized to do so, suggesting common constraints on network and human performance. The network predicted fMRI voxel responses substantially better than traditional spectrotemporal filter models throughout auditory cortex. It also provided a quantitative signature of cortical representational hierarchy—primary and non-primary responses were best predicted by intermediate and late network layers, respectively. The results suggest that task optimization provides a powerful set of tools for modeling sensory systems.},
	number = {3},
	urldate = {2018-06-05},
	journal = {Neuron},
	author = {Kell, Alexander J. E. and Yamins, Daniel L. K. and Shook, Erica N. and Norman-Haignere, Sam V. and McDermott, Josh H.},
	month = may,
	year = {2018},
	keywords = {deep learning, fMRI, project.lstmMEG, read, neural networks, speech comprehension, auditory perception},
	pages = {630--644.e16},
}

@article{zaremba_recurrent_2014,
	title = {Recurrent neural network regularization},
	url = {http://arxiv.org/abs/1409.2329},
	abstract = {We present a simple regularization technique for Recurrent Neural Networks (RNNs) with Long Short-Term Memory (LSTM) units. Dropout, the most successful technique for regularizing neural networks, does not work well with RNNs and LSTMs. In this paper, we show how to correctly apply dropout to LSTMs, and show that it substantially reduces overfitting on a variety of tasks. These tasks include language modeling, speech recognition, image caption generation, and machine translation.},
	urldate = {2018-05-28},
	journal = {arXiv:1409.2329 [cs]},
	author = {Zaremba, Wojciech and Sutskever, Ilya and Vinyals, Oriol},
	month = sep,
	year = {2014},
	note = {arXiv: 1409.2329},
}

@book{trappenberg_fundamentals_2010,
	address = {Oxford ; New York},
	edition = {2nd ed},
	title = {Fundamentals of computational neuroscience},
	isbn = {978-0-19-956841-3},
	abstract = {"Computational neuroscience is the theoretical study of the brain to uncover the principles and mechanisms that guide the development, organization, information processing, and mental functions of the nervous system. Although not a new area, it is only recently that enough knowledge has been gathered to establish computational neuroscience as a scientific discipline in its own right. Given the complexity of the field, and its increasing importance in progressing our understanding of how the brain works, there has long been a need for an introductory text on what is often assumed to be an impenetrable topic. The new edition of Fundamentals of Computational Neuroscience build on the success and strengths of the first edition. It introduces the theoretical foundations of neuroscience with a focus on the nature of information processing in the brain. The book covers the introduction and motivation of simplified models of neurons that are suitable for exploring information processing in large brain-like networks. Additionally, it introduces several fundamental network architectures and discusses their relevance for information processing in the brain, giving some examples of models of higher-order cognitive functions to demonstrate the advanced insight that can be gained with such studies. Each chapter starts by introducing its topic with experimental facts and conceptual questions related to the study of brain function. An additional feature is the inclusion of simple Matlab programs that can be used to explore many of the mechanisms explained in the book. An accompanying webpage includes programs for download. The book is aimed at those within the brain and cognitive sciences, from graduate level and upwards"--Provided by publisher},
	publisher = {Oxford University Press},
	author = {Trappenberg, Thomas P.},
	year = {2010},
	note = {OCLC: ocn444383805},
	keywords = {explanation, LIF, computational neuroscience, leaky-integrate-and-fire, textbook},
}

@article{veen_localization_1997,
	title = {Localization of brain electrical activity via linearly constrained minimum variance spatial filtering},
	volume = {44},
	issn = {0018-9294},
	doi = {10.1109/10.623056},
	abstract = {A spatial filtering method for localizing sources of brain electrical activity from surface recordings is described and analyzed. The spatial filters are implemented as a weighted sum of the data recorded at different sites. The weights are chosen to minimize the filter output power subject to a linear constraint. The linear constraint forces the filter to pass brain electrical activity from a specified location, while the power minimization attenuates activity originating at other locations. The estimated output power as a function of location is normalized by the estimated noise power as a function of location to obtain a neural activity index map. Locations of source activity correspond to maxima in the neural activity index map. The method does not require any prior assumptions about the number of active sources of their geometry because it exploits the spatial covariance of the source electrical activity. This paper presents a development and analysis of the method and explores its sensitivity to deviations between actual and assumed data models. The effect on the algorithm of covariance matrix estimation, correlation between sources, and choice of reference is discussed. Simulated and measured data is used to illustrate the efficacy of the approach.},
	number = {9},
	journal = {IEEE Transactions on Biomedical Engineering},
	author = {Veen, B. D. Van and Drongelen, W. Van and Yuchtman, M. and Suzuki, A.},
	month = sep,
	year = {1997},
	keywords = {methods, project.lstmMEG, project.streams, project.vsmMEG, beamformer, lcmv, linear model, spatial filter},
	pages = {867--880},
}

@article{qian_bridging_2016,
	title = {Bridging {LSTM} architecture and the neural dynamics during reading},
	url = {http://arxiv.org/abs/1604.06635},
	abstract = {Recently, the long short-term memory neural network (LSTM) has attracted wide interest due to its success in many tasks. LSTM architecture consists of a memory cell and three gates, which looks similar to the neuronal networks in the brain. However, there still lacks the evidence of the cognitive plausibility of LSTM architecture as well as its working mechanism. In this paper, we study the cognitive plausibility of LSTM by aligning its internal architecture with the brain activity observed via fMRI when the subjects read a story. Experiment results show that the artificial memory vector in LSTM can accurately predict the observed sequential brain activities, indicating the correlation between LSTM architecture and the cognitive process of story reading.},
	urldate = {2018-05-29},
	journal = {arXiv:1604.06635 [cs]},
	author = {Qian, Peng and Qiu, Xipeng and Huang, Xuanjing},
	month = apr,
	year = {2016},
	note = {arXiv: 1604.06635},
	keywords = {fMRI, LSTM, project.lstmMEG, readme, neural networks, project.lstmMRI, narrative comprehension},
	file = {Qian et al. - 2016 - Bridging LSTM architecture and the neural dynamics.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Qian et al. - 2016 - Bridging LSTM architecture and the neural dynamics.pdf:application/pdf},
}

@article{xu_language_2005,
	title = {Language in context: emergent features of word, sentence, and narrative comprehension},
	volume = {25},
	issn = {10538119},
	shorttitle = {Language in context},
	doi = {10.1016/j.neuroimage.2004.12.013},
	language = {en},
	number = {3},
	urldate = {2018-05-27},
	journal = {NeuroImage},
	author = {Xu, Jiang and Kemeny, Stefan and Park, Grace and Frattali, Carol and Braun, Allen},
	month = apr,
	year = {2005},
	keywords = {fMRI, project.streams, narrative comprehension},
	pages = {1002--1015},
}

@article{lerner_temporal_2014,
	title = {Temporal scaling of neural responses to compressed and dilated natural speech},
	volume = {111},
	copyright = {Copyright © 2014 the American Physiological Society},
	issn = {0022-3077, 1522-1598},
	doi = {10.1152/jn.00497.2013},
	abstract = {Different brain areas integrate information over different timescales, and this capacity to accumulate information increases from early sensory areas to higher order perceptual and cognitive areas. It is currently unknown whether the timescale capacity of each brain area is fixed or whether it adaptively rescales depending on the rate at which information arrives from the world. Here, using functional MRI, we measured brain responses to an auditory narrative presented at different rates. We asked whether neural responses to slowed (speeded) versions of the narrative could be compressed (stretched) to match neural responses to the original narrative. Temporal rescaling was observed in early auditory regions (which accumulate information over short timescales) as well as linguistic and extra-linguistic brain areas (which can accumulate information over long timescales). The temporal rescaling phenomenon started to break down for stimuli presented at double speed, and intelligibility was also impaired for these stimuli. These data suggest that 1) the rate of neural information processing can be rescaled according to the rate of incoming information, both in early sensory regions as well as in higher order cortexes, and 2) the rescaling of neural dynamics is confined to a range of rates that match the range of behavioral performance.},
	language = {en},
	number = {12},
	urldate = {2017-12-04},
	journal = {Journal of Neurophysiology},
	author = {Lerner, Y. and Honey, C. J. and Katkov, M. and Hasson, U.},
	month = jun,
	year = {2014},
	pmid = {24647432},
	keywords = {fMRI, project.lstmMEG, fixme, read, TRW},
	pages = {2433--2444},
	file = {Lerner et al. - 2014 - Temporal scaling of neural responses to compressed.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Lerner et al. - 2014 - Temporal scaling of neural responses to compressed.pdf:application/pdf},
}

@book{dienes_understanding_2008,
	address = {New York},
	title = {Understanding psychology as a science: an introduction to scientific and statistical inference},
	isbn = {978-0-230-54230-3 978-0-230-54231-0},
	shorttitle = {Understanding psychology as a science},
	publisher = {Palgrave Macmillan},
	author = {Dienes, Zoltan},
	year = {2008},
	keywords = {explanation, philosophy of science, thesis.introduction, Kuhn, Lakatos, Popper, psychology},
	file = {Dienes - 2008 - Understanding psychology as a science an introduc.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Dienes - 2008 - Understanding psychology as a science an introduc.pdf:application/pdf},
}

@book{gerstner_neuronal_2014,
	address = {Cambridge, United Kingdom},
	title = {Neuronal dynamics: from single neurons to networks and models of cognition},
	isbn = {978-1-107-06083-8 978-1-107-63519-7},
	shorttitle = {Neuronal dynamics},
	publisher = {Cambridge University Press},
	author = {Gerstner, Wulfram and Kistler, Werner M. and Naud, Richard and Paninski, Liam},
	year = {2014},
	keywords = {project.lsnn, computational neuroscience, textbook},
}

@incollection{schapiro_statistical_2015,
	title = {Statistical learning},
	isbn = {978-0-12-397316-0},
	url = {http://linkinghub.elsevier.com/retrieve/pii/B9780123970251002761},
	language = {en},
	urldate = {2018-03-26},
	booktitle = {Brain {Mapping}},
	publisher = {Elsevier},
	author = {Schapiro, A. and Turk-Browne, N.},
	year = {2015},
	doi = {10.1016/B978-0-12-397025-1.00276-1},
	keywords = {hippocampus, statistical learning},
	pages = {501--506},
}

@article{smith_small_2018,
	title = {Small is beautiful: {In} defense of the small-{N} design},
	issn = {1069-9384, 1531-5320},
	shorttitle = {Small is beautiful},
	doi = {10.3758/s13423-018-1451-8},
	abstract = {The dominant paradigm for inference in psychology is a null-hypothesis significance testing one. Recently, the foundations of this paradigm have been shaken by several notable replication failures. One recommendation to remedy the replication crisis is to collect larger samples of participants. We argue that this recommendation misses a critical point, which is that increasing sample size will not remedy psychology’s lack of strong measurement, lack of strong theories and models, and lack of effective experimental control over error variance. In contrast, there is a long history of research in psychology employing small-N designs that treats the individual participant as the replication unit, which addresses each of these failings, and which produces results that are robust and readily replicated. We illustrate the properties of small-N and large-N designs using a simulated paradigm investigating the stage structure of response times. Our simulations highlight the high power and inferential validity of the small-N design, in contrast to the lower power and inferential indeterminacy of the large-N design. We argue that, if psychology is to be a mature quantitative science, then its primary theoretical aim should be to investigate systematic, functional relationships as they are manifested at the individual participant level and that, wherever possible, it should use methods that are optimized to identify relationships of this kind.},
	language = {en},
	urldate = {2018-05-08},
	journal = {Psychonomic Bulletin \& Review},
	author = {Smith, Philip L. and Little, Daniel R.},
	month = mar,
	year = {2018},
	keywords = {project.lstmMEG, statistics, fixme, read, experimental design, small N},
	pages = {1--19},
	file = {Smith and Little - 2018 - Small is beautiful In defense of the small-Empha.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Smith and Little - 2018 - Small is beautiful In defense of the small-Empha.pdf:application/pdf},
}

@article{fong_using_2018,
	title = {Using human brain activity to guide machine learning},
	volume = {8},
	copyright = {2018 The Author(s)},
	issn = {2045-2322},
	doi = {10.1038/s41598-018-23618-6},
	abstract = {Machine learning is a field of computer science that builds algorithms that learn. In many cases, machine learning algorithms are used to recreate a human ability like adding a caption to a photo, driving a car, or playing a game. While the human brain has long served as a source of inspiration for machine learning, little effort has been made to directly use data collected from working brains as a guide for machine learning algorithms. Here we demonstrate a new paradigm of “neurally-weighted” machine learning, which takes fMRI measurements of human brain activity from subjects viewing images, and infuses these data into the training process of an object recognition learning algorithm to make it more consistent with the human brain. After training, these neurally-weighted classifiers are able to classify images without requiring any additional neural data. We show that our neural-weighting approach can lead to large performance gains when used with traditional machine vision features, as well as to significant improvements with already high-performing convolutional neural network features. The effectiveness of this approach points to a path forward for a new class of hybrid machine learning algorithms which take both inspiration and direct constraints from neuronal data.},
	language = {en},
	number = {1},
	urldate = {2018-05-17},
	journal = {Scientific Reports},
	author = {Fong, Ruth C. and Scheirer, Walter J. and Cox, David D.},
	month = mar,
	year = {2018},
	keywords = {fMRI, project.lstmMEG, readme, neural networks, machine learning, computational neuroscience},
	pages = {5397},
}

@article{meehl_theory-testing_1967,
	title = {Theory-testing in psychology and physics: {A} methodological paradox},
	volume = {34},
	issn = {0031-8248, 1539-767X},
	shorttitle = {Theory-{Testing} in {Psychology} and {Physics}},
	doi = {10.1086/288135},
	language = {en},
	number = {2},
	urldate = {2018-05-08},
	journal = {Philosophy of Science},
	author = {Meehl, Paul E.},
	month = jun,
	year = {1967},
	keywords = {methods, readme, statistics, physics, epistemology, psychology, science practice},
	pages = {103--115},
}

@article{abbott_building_2016,
	title = {Building functional networks of spiking model neurons},
	volume = {19},
	copyright = {2016 Nature Publishing Group},
	issn = {1546-1726},
	url = {https://www.nature.com/articles/nn.4241},
	doi = {10.1038/nn.4241},
	abstract = {Most of the networks used by computer scientists and many of those studied by modelers in neuroscience represent unit activities as continuous variables. Neurons, however, communicate primarily through discontinuous spiking. We review methods for transferring our ability to construct interesting networks that perform relevant tasks from the artificial continuous domain to more realistic spiking network models. These methods raise a number of issues that warrant further theoretical and experimental study.},
	language = {en},
	number = {3},
	urldate = {2018-04-30},
	journal = {Nature Neuroscience},
	author = {Abbott, L. F. and DePasquale, Brian and Memmesheimer, Raoul-Martin},
	month = mar,
	year = {2016},
	keywords = {readme, neural networks, project.lsnn, computational neuroscience, SNN},
	pages = {350--355},
	file = {Abbott et al. - 2016 - Building functional networks of spiking model neur.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Abbott et al. - 2016 - Building functional networks of spiking model neur.pdf:application/pdf},
}

@incollection{mayo_new_2009,
	address = {Cambridge},
	title = {New {Perspectives} on ({Some} {Old}) {Problems} of {Frequentist} {Statistics}},
	isbn = {978-0-511-65752-8},
	url = {https://www.cambridge.org/core/product/identifier/CBO9780511657528A015/type/book_part},
	language = {en},
	urldate = {2018-04-28},
	booktitle = {Error and {Inference}},
	publisher = {Cambridge University Press},
	author = {Mayo, Deborah G. and Cox, David and Spanos, Aris},
	editor = {Mayo, Deborah G. and Spanos, Aris},
	year = {2009},
	doi = {10.1017/CBO9780511657528.009},
	pages = {247--330},
	file = {Mayo et al. - 2009 - New Perspectives on (Some Old) Problems of Frequen.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Mayo et al. - 2009 - New Perspectives on (Some Old) Problems of Frequen.pdf:application/pdf},
}

@book{levelt_speaking:_1989,
	address = {Cambridge, Mass},
	series = {{ACL}-{MIT} {Press} series in natural-language processing},
	title = {Speaking: from intention to articulation},
	isbn = {978-0-262-12137-8},
	shorttitle = {Speaking},
	publisher = {MIT Press},
	author = {Levelt, W. J. M.},
	year = {1989},
	keywords = {project.streams, cognitive science, speech production, speech, theory of lexical access},
}

@article{vu_shared_2018,
	title = {A shared vision for machine learning in neuroscience},
	volume = {38},
	issn = {0270-6474, 1529-2401},
	doi = {10.1523/JNEUROSCI.0508-17.2018},
	language = {en},
	number = {7},
	urldate = {2018-04-25},
	journal = {The Journal of Neuroscience},
	author = {Vu, Mai-Anh T. and Adalı, Tülay and Ba, Demba and Buzsáki, György and Carlson, David and Heller, Katherine and Liston, Conor and Rudin, Cynthia and Sohal, Vikaas S. and Widge, Alik S. and Mayberg, Helen S. and Sapiro, Guillermo and Dzirasa, Kafui},
	month = feb,
	year = {2018},
	keywords = {read},
	pages = {1601--1607},
}

@article{bellec_long_2018,
	title = {Long short-term memory and learning-to-learn in networks of spiking neurons},
	url = {http://arxiv.org/abs/1803.09574},
	abstract = {Networks of spiking neurons (SNNs) are frequently studied as models for networks of neurons in the brain, but also as paradigm for novel energy efficient computing hardware. In principle they are especially suitable for computations in the temporal domain, such as speech processing, because their computations are carried out via events in time and space. But so far they have been lacking the capability to preserve information for longer time spans during a computation, until it is updated or needed - like a register of a digital computer. This function is provided to artificial neural networks through Long Short-Term Memory (LSTM) units. We show here that SNNs attain similar capabilities if one includes adapting neurons in the network. Adaptation denotes an increase of the firing threshold of a neuron after preceding firing. A substantial fraction of neurons in the neocortex of rodents and humans has been found to be adapting. It turns out that if adapting neurons are integrated in a suitable manner into the architecture of SNNs, the performance of these enhanced SNNs, which we call LSNNs, for computation in the temporal domain approaches that of artificial neural networks with LSTM-units. In addition, the computing and learning capabilities of LSNNs can be substantially enhanced through learning-to-learn (L2L) methods from machine learning, that have so far been applied primarily to LSTM networks and apparently never to SSNs. This preliminary report on arXiv will be replaced by a more detailed version in about a month.},
	urldate = {2018-04-24},
	journal = {arXiv:1803.09574 [cs, q-bio]},
	author = {Bellec, Guillaume and Salaj, Darjan and Subramoney, Anand and Legenstein, Robert and Maass, Wolfgang},
	month = mar,
	year = {2018},
	note = {arXiv: 1803.09574},
	keywords = {LSTM, neural networks, project.lsnn, SNN, reading},
	file = {Bellec et al. - 2018 - Long short-term memory and learning-to-learn in ne.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Bellec et al. - 2018 - Long short-term memory and learning-to-learn in ne.pdf:application/pdf},
}

@article{brennan_naturalistic_2016,
	title = {Naturalistic sentence comprehension in the brain: naturalistic comprehension},
	volume = {10},
	issn = {1749818X},
	shorttitle = {Naturalistic sentence comprehension in the brain},
	doi = {10.1111/lnc3.12198},
	language = {en},
	number = {7},
	urldate = {2018-03-16},
	journal = {Language and Linguistics Compass},
	author = {Brennan, Jonathan R.},
	month = jul,
	year = {2016},
	keywords = {project.streams, read, computational linguistics, langauge models},
	pages = {299--313},
}

@incollection{mayo_frequentist_2006,
	address = {Beachwood, Ohio, USA},
	title = {Frequentist statistics as a theory of inductive inference},
	url = {http://projecteuclid.org/euclid.lnms/1196283956},
	language = {en},
	urldate = {2018-04-21},
	booktitle = {Institute of {Mathematical} {Statistics} {Lecture} {Notes} - {Monograph} {Series}},
	publisher = {Institute of Mathematical Statistics},
	author = {Mayo, Deborah G. and Cox, D. R.},
	year = {2006},
	doi = {10.1214/074921706000000400},
	keywords = {methods, readme, statistics, probability theory, induction, reading, error statistics, frequentist statistics, inductive inference},
	pages = {77--97},
	file = {Mayo and Cox - 2006 - Frequentist statistics as a theory of inductive in.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Mayo and Cox - 2006 - Frequentist statistics as a theory of inductive in.pdf:application/pdf},
}

@article{thurston_proof_1994,
	title = {On proof and progress in mathematics},
	url = {http://arxiv.org/abs/math/9404236},
	abstract = {In response to Jaffe and Quinn [math.HO/9307227], the author discusses forms of progress in mathematics that are not captured by formal proofs of theorems, especially in his own work in the theory of foliations and geometrization of 3-manifolds and dynamical systems.},
	urldate = {2018-04-21},
	journal = {arXiv:math/9404236},
	author = {Thurston, William P.},
	month = mar,
	year = {1994},
	note = {arXiv: math/9404236},
	keywords = {readme, explanation, education, interpretation, mathematics, proof, science communication, understanding},
	file = {Thurston - 1994 - On proof and progress in mathematics.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Thurston - 1994 - On proof and progress in mathematics.pdf:application/pdf},
}

@article{olah_research_2017,
	title = {Research debt},
	volume = {2},
	issn = {2476-0757},
	url = {https://distill.pub/2017/research-debt},
	doi = {10.23915/distill.00005},
	abstract = {Science is a human activity. When we fail to distill and explain research, we accumulate a kind of debt...},
	language = {en},
	number = {3},
	urldate = {2018-04-21},
	journal = {Distill},
	author = {Olah, Chris and Carter, Shan},
	month = mar,
	year = {2017},
	keywords = {technical debt, read, science practice, science communication, research debt},
	pages = {e5},
	file = {Snapshot:/Users/kriarm/Zotero/storage/DI9XR7JI/research-debt.html:text/html},
}

@article{marblestone_toward_2016,
	title = {Toward an integration of deep learning and neuroscience},
	volume = {10},
	issn = {1662-5188},
	doi = {10.3389/fncom.2016.00094},
	abstract = {Neuroscience has focused on the detailed implementation of computation, studying neural codes, dynamics and circuits. In machine learning, however, artificial neural networks tend to eschew precisely designed codes, dynamics or circuits in favor of brute force optimization of a cost function, often using simple and relatively uniform initial architectures. Two recent developments have emerged within machine learning that create an opportunity to connect these seemingly divergent perspectives. First, structured architectures are used, including dedicated systems for attention, recursion and various forms of short- and long-term memory storage. Second, cost functions and training procedures have become more complex and are varied across layers and over time. Here we think about the brain in terms of these ideas. We hypothesize that (1) the brain optimizes cost functions, (2) the cost functions are diverse and differ across brain locations and over development, and (3) optimization operates within a pre-structured architecture matched to the computational problems posed by behavior. In support of these hypotheses, we argue that a range of implementations of credit assignment through multiple layers of neurons are compatible with our current knowledge of neural circuitry, and that the brain's specialized systems can be interpreted as enabling efficient optimization for specific problem classes. Such a heterogeneously optimized system, enabled by a series of interacting cost functions, serves to make learning data-efficient and precisely targeted to the needs of the organism. We suggest directions by which neuroscience could seek to refine and test these hypotheses.},
	language = {English},
	urldate = {2018-04-20},
	journal = {Frontiers in Computational Neuroscience},
	author = {Marblestone, Adam H. and Wayne, Greg and Kording, Konrad P.},
	year = {2016},
	keywords = {deep learning, neuroscience, machine learning, ANN, reading, cost function},
	file = {Marblestone et al. - 2016 - Toward an integration of deep learning and neurosc.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Marblestone et al. - 2016 - Toward an integration of deep learning and neurosc.pdf:application/pdf},
}

@article{barak_recurrent_2017,
	title = {Recurrent neural networks as versatile tools of neuroscience research},
	volume = {46},
	issn = {09594388},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0959438817300429},
	doi = {10.1016/j.conb.2017.06.003},
	language = {en},
	urldate = {2018-04-19},
	journal = {Current Opinion in Neurobiology},
	author = {Barak, Omri},
	month = oct,
	year = {2017},
	keywords = {RNN, project.lstmMEG, readme, project.vsmMEG, explanation, computation, neural networks, machine learning, ANN, thesis.introduction, reading},
	pages = {1--6},
}

@inproceedings{smith_cloze_2011,
	title = {Cloze but no cigar: {The} complex relationship between cloze, corpus, and subjective probabilities in language processing},
	booktitle = {Proceedings of the 33rd {Annual} {Meeting} of the {Cognitive} {Science} {Society}},
	author = {Smith, Nathaniel J. and Levy, Roger},
	year = {2011},
	keywords = {language model, project.streams, probability theory, sentence comprehension, cloze probability, suprisal},
	pages = {1637--1642},
}

@article{friston_modalities_2009,
	title = {Modalities, modes, and models in functional neuroimaging},
	volume = {326},
	issn = {0036-8075, 1095-9203},
	url = {http://www.sciencemag.org/cgi/doi/10.1126/science.1174521},
	doi = {10.1126/science.1174521},
	language = {en},
	number = {5951},
	urldate = {2018-04-17},
	journal = {Science},
	author = {Friston, K. J.},
	month = oct,
	year = {2009},
	pages = {399--403},
}

@article{stolk_online_2013,
	title = {Online and offline tools for head movement compensation in {MEG}},
	volume = {68},
	issn = {1053-8119},
	url = {http://www.sciencedirect.com/science/article/pii/S1053811912011597},
	doi = {10.1016/j.neuroimage.2012.11.047},
	abstract = {Magnetoencephalography (MEG) is measured above the head, which makes it sensitive to variations of the head position with respect to the sensors. Head movements blur the topography of the neuronal sources of the MEG signal, increase localization errors, and reduce statistical sensitivity. Here we describe two novel and readily applicable methods that compensate for the detrimental effects of head motion on the statistical sensitivity of MEG experiments. First, we introduce an online procedure that continuously monitors head position. Second, we describe an offline analysis method that takes into account the head position time-series. We quantify the performance of these methods in the context of three different experimental settings, involving somatosensory, visual and auditory stimuli, assessing both individual and group-level statistics. The online head localization procedure allowed for optimal repositioning of the subjects over multiple sessions, resulting in a 28\% reduction of the variance in dipole position and an improvement of up to 15\% in statistical sensitivity. Offline incorporation of the head position time-series into the general linear model resulted in improvements of group-level statistical sensitivity between 15\% and 29\%. These tools can substantially reduce the influence of head movement within and between sessions, increasing the sensitivity of many cognitive neuroscience experiments.},
	number = {Supplement C},
	urldate = {2017-12-14},
	journal = {NeuroImage},
	author = {Stolk, Arjen and Todorovic, Ana and Schoffelen, Jan-Mathijs and Oostenveld, Robert},
	month = mar,
	year = {2013},
	keywords = {MEG, methods, project.streams, project.vsmMEG, common, general linear model, GLM},
	pages = {39--48},
}

@inproceedings{stolcke_srilm_2002,
	address = {Denver, Colorado, USA},
	title = {{SRILM} - an {Extensible} {Language} {Modeling} {Toolkit}},
	url = {http://www.isca-speech.org/archive/icslp_2002/i02_0901.html},
	abstract = {SRILM is a collection of C++ libraries, executable programs, and helper scripts designed to allow both production of and experimentation with statistical language models for speech recognition and other applications. SRILM is freely available for noncommercial purposes. The toolkit supports creation and evaluation of a variety of language model types based on N-gram statistics, as well as several related tasks, such as statistical tagging and manipulation of N-best lists and word lattices. This paper summarizes the functionality of the toolkit and discusses its design and implementation, highlighting ease of rapid prototyping, reusability, and combinability of tools.},
	booktitle = {{ICSLP}-2002},
	author = {Stolcke, Andreas},
	month = sep,
	year = {2002},
	keywords = {language model, LM, methods, project.streams, project.vsmMEG, language},
	pages = {901--904},
}

@article{smith_fast_2002,
	title = {Fast robust automated brain extraction},
	volume = {17},
	issn = {1065-9471, 1097-0193},
	url = {http://doi.wiley.com/10.1002/hbm.10062},
	doi = {10.1002/hbm.10062},
	language = {en},
	number = {3},
	urldate = {2017-12-14},
	journal = {Human Brain Mapping},
	author = {Smith, Stephen M.},
	month = nov,
	year = {2002},
	keywords = {methods, project.streams, project.vsmMEG, MRI, source reconstruction},
	pages = {143--155},
}

@article{oostenveld_fieldtrip:_2011,
	title = {{FieldTrip}: {Open} {Source} {Software} for {Advanced} {Analysis} of {MEG}, {EEG}, and {Invasive} {Electrophysiological} {Data}},
	shorttitle = {{FieldTrip}},
	doi = {10.1155/2011/156869},
	abstract = {This paper describes FieldTrip, an open source software package that we developed for the analysis of MEG, EEG, and other electrophysiological data. The software is implemented as a MATLAB toolbox and includes a complete set of consistent and user-friendly high-level functions that allow experimental neuroscientists to analyze experimental data. It includes algorithms for simple and advanced analysis, such as time-frequency analysis using multitapers, source reconstruction using dipoles, distributed sources and beamformers, connectivity analysis, and nonparametric statistical permutation tests at the channel and source level. The implementation as toolbox allows the user to perform elaborate and structured analyses of large data sets using the MATLAB command line and batch scripting. Furthermore, users and developers can easily extend the functionality and implement new algorithms. The modular design facilitates the reuse in other software packages.},
	language = {1},
	urldate = {2017-12-14},
	journal = {Computational Intelligence and Neuroscience},
	author = {Oostenveld, Robert and Fries, Pascal and Maris, Eric and Schoffelen, Jan-Mathijs},
	year = {2011},
	pmid = {21253357},
	doi = {10.1155/2011/156869},
	keywords = {MEG, EEG, methods, project.lstmMEG, project.streams, project.vsmMEG, read, common},
}

@article{kay_principles_2017,
	title = {Principles for models of neural information processing},
	issn = {10538119},
	doi = {10.1016/j.neuroimage.2017.08.016},
	language = {en},
	urldate = {2018-04-14},
	journal = {NeuroImage},
	author = {Kay, Kendrick N.},
	month = aug,
	year = {2017},
	keywords = {explanation, fixme, read, computation, project.6, computational modeling, information processing},
}

@article{friston_models_2005,
	title = {Models of brain function in neuroimaging},
	volume = {56},
	issn = {0066-4308, 1545-2085},
	doi = {10.1146/annurev.psych.56.091103.070311},
	language = {en},
	number = {1},
	urldate = {2018-04-14},
	journal = {Annual Review of Psychology},
	author = {Friston, Karl J.},
	month = feb,
	year = {2005},
	keywords = {fMRI, readme, explanation, neuroimaging, computation, model-based, computational modeling, model},
	pages = {57--87},
}

@article{dolsak_climate_2018,
	chapter = {Opinion},
	title = {The climate change hypocrisy of jet-setting academics},
	url = {https://www.huffingtonpost.com/entry/opinion-dolsak-prakash-carbon-tax_us_5abe746ae4b055e50acd5c80},
	abstract = {Universities should pay a carbon tax on their faculty's air travel.},
	language = {en\_US},
	urldate = {2018-04-11},
	journal = {Huffington Post},
	author = {Dolšak, Nives and Prakash, Aseem},
	month = mar,
	year = {2018},
	keywords = {air travel, carbon footprint, carbon offset, climate change, environment, science policy},
	file = {Snapshot:/Users/kriarm/Zotero/storage/X7MNHJXU/opinion-dolsak-prakash-carbon-tax_us_5abe746ae4b055e50acd5c80.html:text/html},
}

@book{milkowski_explaining_2013,
	address = {Cambrid, Massachusetts},
	title = {Explaining the computational mind},
	isbn = {978-0-262-01886-9},
	publisher = {The MIT Press},
	author = {Miłkowski, Marcin},
	year = {2013},
	keywords = {cognitive neuroscience, readme, explanation, philosophy of science, computation, computational cognitive neuroscience, cognitive science, project.6, computational explanation},
}

@article{tennant_multi-disciplinary_2017,
	title = {A multi-disciplinary perspective on emergent and future innovations in peer review},
	volume = {6},
	issn = {2046-1402},
	url = {https://f1000research.com/articles/6-1151/v3},
	doi = {10.12688/f1000research.12037.3},
	language = {en},
	urldate = {2018-03-03},
	journal = {F1000Research},
	author = {Tennant, Jonathan P. and Dugan, Jonathan M. and Graziotin, Daniel and Jacques, Damien C. and Waldner, François and Mietchen, Daniel and Elkhatib, Yehia and B. Collister, Lauren and Pikas, Christina K. and Crick, Tom and Masuzzo, Paola and Caravaggi, Anthony and Berg, Devin R. and Niemeyer, Kyle E. and Ross-Hellauer, Tony and Mannheimer, Sara and Rigling, Lillian and Katz, Daniel S. and Greshake Tzovaras, Bastian and Pacheco-Mendoza, Josmel and Fatima, Nazeefa and Poblet, Marta and Isaakidis, Marios and Irawan, Dasapta Erwin and Renaut, Sébastien and Madan, Christopher R. and Matthias, Lisa and Nørgaard Kjær, Jesper and O'Donnell, Daniel Paul and Neylon, Cameron and Kearns, Sarah and Selvaraju, Manojkumar and Colomb, Julien},
	month = nov,
	year = {2017},
	pages = {1151},
	file = {Tennant et al. - 2017 - A multi-disciplinary perspective on emergent and f.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Tennant et al. - 2017 - A multi-disciplinary perspective on emergent and f.pdf:application/pdf},
}

@article{brodbeck_neural_2018,
	title = {Neural source dynamics of brain responses to continuous stimuli: {Speech} processing from acoustics to comprehension},
	volume = {172},
	issn = {1053-8119},
	shorttitle = {Neural source dynamics of brain responses to continuous stimuli},
	url = {http://www.sciencedirect.com/science/article/pii/S1053811918300429},
	doi = {10.1016/j.neuroimage.2018.01.042},
	abstract = {Human experience often involves continuous sensory information that unfolds over time. This is true in particular for speech comprehension, where continuous acoustic signals are processed over seconds or even minutes. We show that brain responses to such continuous stimuli can be investigated in detail, for magnetoencephalography (MEG) data, by combining linear kernel estimation with minimum norm source localization. Previous research has shown that the requirement to average data over many trials can be overcome by modeling the brain response as a linear convolution of the stimulus and a kernel, or response function, and estimating a kernel that predicts the response from the stimulus. However, such analysis has been typically restricted to sensor space. Here we demonstrate that this analysis can also be performed in neural source space. We first computed distributed minimum norm current source estimates for continuous MEG recordings, and then computed response functions for the current estimate at each source element, using the boosting algorithm with cross-validation. Permutation tests can then assess the significance of individual predictor variables, as well as features of the corresponding spatio-temporal response functions. We demonstrate the viability of this technique by computing spatio-temporal response functions for speech stimuli, using predictor variables reflecting acoustic, lexical and semantic processing. Results indicate that processes related to comprehension of continuous speech can be differentiated anatomically as well as temporally: acoustic information engaged auditory cortex at short latencies, followed by responses over the central sulcus and inferior frontal gyrus, possibly related to somatosensory/motor cortex involvement in speech perception; lexical frequency was associated with a left-lateralized response in auditory cortex and subsequent bilateral frontal activity; and semantic composition was associated with bilateral temporal and frontal brain activity. We conclude that this technique can be used to study the neural processing of continuous stimuli in time and anatomical space with the millisecond temporal resolution of MEG. This suggests new avenues for analyzing neural processing of naturalistic stimuli, without the necessity of averaging over artificially short or truncated stimuli.},
	urldate = {2018-02-27},
	journal = {NeuroImage},
	author = {Brodbeck, Christian and Presacco, Alessandro and Simon, Jonathan Z.},
	month = may,
	year = {2018},
	keywords = {MEG, methods, readme, project.vsmMEG, model-based, speech, MNE},
	pages = {162--174},
}

@article{dehghani_decoding_2017,
	title = {Decoding the neural representation of story meanings across languages},
	volume = {38},
	issn = {1097-0193},
	url = {http://onlinelibrary.wiley.com/doi/10.1002/hbm.23814/abstract},
	doi = {10.1002/hbm.23814},
	abstract = {Drawing from a common lexicon of semantic units, humans fashion narratives whose meaning transcends that of their individual utterances. However, while brain regions that represent lower-level semantic units, such as words and sentences, have been identified, questions remain about the neural representation of narrative comprehension, which involves inferring cumulative meaning. To address these questions, we exposed English, Mandarin, and Farsi native speakers to native language translations of the same stories during fMRI scanning. Using a new technique in natural language processing, we calculated the distributed representations of these stories (capturing the meaning of the stories in high-dimensional semantic space), and demonstrate that using these representations we can identify the specific story a participant was reading from the neural data. Notably, this was possible even when the distributed representations were calculated using stories in a different language than the participant was reading. Our results reveal that identification relied on a collection of brain regions most prominently located in the default mode network. These results demonstrate that neuro-semantic encoding of narratives happens at levels higher than individual semantic units and that this encoding is systematic across both individuals and languages. Hum Brain Mapp 38:6096–6106, 2017. © 2017 Wiley Periodicals, Inc.},
	language = {en},
	number = {12},
	urldate = {2018-02-20},
	journal = {Human Brain Mapping},
	author = {Dehghani, Morteza and Boghrati, Reihane and Man, Kingson and Hoover, Joe and Gimbel, Sarah I. and Vaswani, Ashish and Zevin, Jason D. and Immordino-Yang, Mary Helen and Gordon, Andrew S. and Damasio, Antonio and Kaplan, Jonas T.},
	month = dec,
	year = {2017},
	keywords = {fMRI, project.vsmMEG, fixme, read, narrative, doc2vec},
	pages = {6096--6106},
}

@article{poeppel_maps_2012,
	title = {The maps problem and the mapping problem: {Two} challenges for a cognitive neuroscience of speech and language},
	volume = {29},
	issn = {0264-3294, 1464-0627},
	shorttitle = {The maps problem and the mapping problem},
	url = {http://www.tandfonline.com/doi/abs/10.1080/02643294.2012.710600},
	doi = {10.1080/02643294.2012.710600},
	language = {en},
	number = {1-2},
	urldate = {2018-02-17},
	journal = {Cognitive Neuropsychology},
	author = {Poeppel, David},
	month = mar,
	year = {2012},
	keywords = {readme, explanation, philosophy of neuroscience, computation, project.6, mapping},
	pages = {34--55},
	file = {Poeppel - 2012 - The maps problem and the mapping problem Two chal.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Poeppel - 2012 - The maps problem and the mapping problem Two chal.pdf:application/pdf},
}

@article{embick_towards_2015,
	title = {Towards a computational(ist) neurobiology of language: correlational, integrated and explanatory neurolinguistics},
	volume = {30},
	issn = {2327-3798, 2327-3801},
	shorttitle = {Towards a computational(ist) neurobiology of language},
	url = {http://www.tandfonline.com/doi/abs/10.1080/23273798.2014.980750},
	doi = {10.1080/23273798.2014.980750},
	language = {en},
	number = {4},
	urldate = {2018-02-17},
	journal = {Language, Cognition and Neuroscience},
	author = {Embick, David and Poeppel, David},
	month = apr,
	year = {2015},
	keywords = {cognitive neuroscience, explanation, read, computation, thesis.introduction, theory, linguistics},
	pages = {357--366},
}

@article{klein_images_2010,
	title = {Images are not the evidence in neuroimaging},
	volume = {61},
	issn = {0007-0882},
	url = {https://academic.oup.com/bjps/article/61/2/265/1459012},
	doi = {10.1093/bjps/axp035},
	abstract = {fMRI promises to uncover the functional structure of the brain. I argue, however, that pictures of ‘brain activity' associated with fMRI experiments are poor evidence for functional claims. These neuroimages present the results of null hypothesis significance tests performed on fMRI data. Significance tests alone cannot provide evidence about the functional structure of causally dense systems, including the brain. Instead, neuroimages should be seen as indicating regions where further data analysis is warranted. This additional analysis rarely involves simple significance testing, and so justified skepticism about neuroimages does not provide reason for skepticism about fMRI more generally. 1Introduction2Neuroimages Are Statistical Maps3The Skeptical Argument 3.1Evidence and neuroimages3.2The problem of causal density3.3The problem of arbitrary thresholds3.4The problem of vague alternatives4Skepticism Is Due to NHST5Neuroimages versus Neuroimaging},
	number = {2},
	urldate = {2017-12-12},
	journal = {The British Journal for the Philosophy of Science},
	author = {Klein, Colin},
	month = jun,
	year = {2010},
	keywords = {fMRI, explanation, philosophy of neuroscience, philosophy of science, read, epistemology},
	pages = {265--278},
	file = {Klein - 2010 - Images are not the evidence in neuroimaging.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Klein - 2010 - Images are not the evidence in neuroimaging.pdf:application/pdf},
}

@article{piccinini_neural_2013,
	title = {Neural computation and the computational theory of cognition},
	volume = {37},
	issn = {1551-6709},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/cogs.12012/abstract},
	doi = {10.1111/cogs.12012},
	abstract = {We begin by distinguishing computationalism from a number of other theses that are sometimes conflated with it. We also distinguish between several important kinds of computation: computation in a generic sense, digital computation, and analog computation. Then, we defend a weak version of computationalism—neural processes are computations in the generic sense. After that, we reject on empirical grounds the common assimilation of neural computation to either analog or digital computation, concluding that neural computation is sui generis. Analog computation requires continuous signals; digital computation requires strings of digits. But current neuroscientific evidence indicates that typical neural signals, such as spike trains, are graded like continuous signals but are constituted by discrete functional elements (spikes); thus, typical neural signals are neither continuous signals nor strings of digits. It follows that neural computation is sui generis. Finally, we highlight three important consequences of a proper understanding of neural computation for the theory of cognition. First, understanding neural computation requires a specially designed mathematical theory (or theories) rather than the mathematical theories of analog or digital computation. Second, several popular views about neural computation turn out to be incorrect. Third, computational theories of cognition that rely on non-neural notions of computation ought to be replaced or reinterpreted in terms of neural computation.},
	language = {en},
	number = {3},
	urldate = {2017-12-18},
	journal = {Cognitive Science},
	author = {Piccinini, Gualtiero and Bahar, Sonya},
	month = apr,
	year = {2013},
	keywords = {project.streams, explanation, philosophy of neuroscience, philosophy of science, read, information, information theory, computation, thesis.introduction, computational neuroscience, thesis},
	pages = {453--488},
	file = {Piccinini and Bahar - 2013 - Neural computation and the computational theory of.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Piccinini and Bahar - 2013 - Neural computation and the computational theory of.pdf:application/pdf},
}

@article{mitchell_composition_2010,
	title = {Composition in distributional models of semantics},
	volume = {34},
	issn = {03640213},
	url = {http://doi.wiley.com/10.1111/j.1551-6709.2010.01106.x},
	doi = {10.1111/j.1551-6709.2010.01106.x},
	language = {en},
	number = {8},
	urldate = {2018-02-15},
	journal = {Cognitive Science},
	author = {Mitchell, Jeff and Lapata, Mirella},
	month = nov,
	year = {2010},
	keywords = {readme, project.vsmMEG, VSM, computational linguistics},
	pages = {1388--1429},
}

@article{goldberg_primer_2015,
	title = {A {Primer} on neural network models for natural language processing},
	url = {http://arxiv.org/abs/1510.00726},
	abstract = {Over the past few years, neural networks have re-emerged as powerful machine-learning models, yielding state-of-the-art results in fields such as image recognition and speech processing. More recently, neural network models started to be applied also to textual natural language signals, again with very promising results. This tutorial surveys neural network models from the perspective of natural language processing research, in an attempt to bring natural-language researchers up to speed with the neural techniques. The tutorial covers input encoding for natural language tasks, feed-forward networks, convolutional networks, recurrent networks and recursive networks, as well as the computation graph abstraction for automatic gradient computation.},
	urldate = {2018-02-16},
	journal = {arXiv:1510.00726 [cs]},
	author = {Goldberg, Yoav},
	month = oct,
	year = {2015},
	note = {arXiv: 1510.00726},
	keywords = {NLP, project.lstmMEG, readme, ANN, word embedding, primer},
	file = {Goldberg - 2015 - A Primer on neural network models for natural lang.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Goldberg - 2015 - A Primer on neural network models for natural lang.pdf:application/pdf},
}

@article{liang_bringing_2015,
	title = {Bringing machine learning and compositional semantics together},
	volume = {1},
	url = {https://doi.org/10.1146/annurev-linguist-030514-125312},
	doi = {10.1146/annurev-linguist-030514-125312},
	abstract = {Computational semantics has long been considered a field divided between logical and statistical approaches, but this divide is rapidly eroding with the development of statistical models that learn compositional semantic theories from corpora and databases. This review presents a simple discriminative learning framework for defining such models and relating them to logical theories. Within this framework, we discuss the task of learning to map utterances to logical forms (semantic parsing) and the task of learning from denotations with logical forms as latent variables. We also consider models that use distributed (e.g., vector) representations rather than logical ones, showing that these can be considered part of the same overall framework for understanding meaning and structural complexity.},
	number = {1},
	urldate = {2018-02-16},
	journal = {Annual Review of Linguistics},
	author = {Liang, Percy and Potts, Christopher},
	year = {2015},
	keywords = {readme, project.vsmMEG, VSM, computational linguistics, semantics, 2},
	pages = {355--376},
}

@article{lenci_distributional_2018,
	title = {Distributional models of word meaning},
	volume = {4},
	url = {https://doi.org/10.1146/annurev-linguistics-030514-125254},
	doi = {10.1146/annurev-linguistics-030514-125254},
	abstract = {Distributional semantics is a usage-based model of meaning, based on the assumption that the statistical distribution of linguistic items in context plays a key role in characterizing their semantic behavior. Distributional models build semantic representations by extracting co-occurrences from corpora and have become a mainstream research paradigm in computational linguistics. In this review, I present the state of the art in distributional semantics, focusing on its assets and limits as a model of meaning and as a method for semantic analysis.},
	number = {1},
	urldate = {2018-02-16},
	journal = {Annual Review of Linguistics},
	author = {Lenci, Alessandro},
	year = {2018},
	keywords = {readme, distributional semantics, project.vsmMEG, semantics},
	pages = {151--171},
}

@article{buzsaki_neuronal_2004,
	title = {Neuronal oscillations in cortical networks},
	volume = {304},
	copyright = {American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	url = {http://science.sciencemag.org/content/304/5679/1926},
	doi = {10.1126/science.1099745},
	abstract = {Clocks tick, bridges and skyscrapers vibrate, neuronal networks oscillate. Are neuronal oscillations an inevitable by-product, similar to bridge vibrations, or an essential part of the brain's design? Mammalian cortical neurons form behavior-dependent oscillating networks of various sizes, which span five orders of magnitude in frequency. These oscillations are phylogenetically preserved, suggesting that they are functionally relevant. Recent findings indicate that network oscillations bias input selection, temporally link neurons into assemblies, and facilitate synaptic plasticity, mechanisms that cooperatively support temporal representation and long-term consolidation of information.},
	language = {en},
	number = {5679},
	urldate = {2018-01-10},
	journal = {Science},
	author = {Buzsáki, György and Draguhn, Andreas},
	month = jun,
	year = {2004},
	pmid = {15218136},
	keywords = {project.streams, read, neural oscillations, thesis.introduction},
	pages = {1926--1929},
}

@article{heer_hierarchical_2017,
	title = {The hierarchical cortical organization of human speech processing},
	volume = {37},
	copyright = {Copyright © 2017 the authors 0270-6474/17/376539-19\$15.00/0},
	issn = {0270-6474, 1529-2401},
	doi = {10.1523/JNEUROSCI.3267-16.2017},
	abstract = {Speech comprehension requires that the brain extract semantic meaning from the spectral features represented at the cochlea. To investigate this process, we performed an fMRI experiment in which five men and two women passively listened to several hours of natural narrative speech. We then used voxelwise modeling to predict BOLD responses based on three different feature spaces that represent the spectral, articulatory, and semantic properties of speech. The amount of variance explained by each feature space was then assessed using a separate validation dataset. Because some responses might be explained equally well by more than one feature space, we used a variance partitioning analysis to determine the fraction of the variance that was uniquely explained by each feature space. Consistent with previous studies, we found that speech comprehension involves hierarchical representations starting in primary auditory areas and moving laterally on the temporal lobe: spectral features are found in the core of A1, mixtures of spectral and articulatory in STG, mixtures of articulatory and semantic in STS, and semantic in STS and beyond. Our data also show that both hemispheres are equally and actively involved in speech perception and interpretation. Further, responses as early in the auditory hierarchy as in STS are more correlated with semantic than spectral representations. These results illustrate the importance of using natural speech in neurolinguistic research. Our methodology also provides an efficient way to simultaneously test multiple specific hypotheses about the representations of speech without using block designs and segmented or synthetic speech.
SIGNIFICANCE STATEMENT To investigate the processing steps performed by the human brain to transform natural speech sound into meaningful language, we used models based on a hierarchical set of speech features to predict BOLD responses of individual voxels recorded in an fMRI experiment while subjects listened to natural speech. Both cerebral hemispheres were actively involved in speech processing in large and equal amounts. Also, the transformation from spectral features to semantic elements occurs early in the cortical speech-processing stream. Our experimental and analytical approaches are important alternatives and complements to standard approaches that use segmented speech and block designs, which report more laterality in speech processing and associated semantic processing to higher levels of cortex than reported here.},
	language = {en},
	number = {27},
	urldate = {2017-11-21},
	journal = {Journal of Neuroscience},
	author = {Heer, Wendy A. de and Huth, Alexander G. and Griffiths, Thomas L. and Gallant, Jack L. and Theunissen, Frédéric E.},
	month = jul,
	year = {2017},
	pmid = {28588065},
	keywords = {project.lstmMEG, distributional semantics, project.vsmMEG, VSM, read, semantics, speech},
	pages = {6539--6557},
	file = {Heer et al. - 2017 - The Hierarchical Cortical Organization of Human Sp.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Heer et al. - 2017 - The Hierarchical Cortical Organization of Human Sp.pdf:application/pdf},
}

@article{lopopolo_using_2017,
	title = {Using stochastic language models ({SLM}) to map lexical, syntactic, and phonological information processing in the brain},
	volume = {12},
	issn = {1932-6203},
	url = {http://dx.plos.org/10.1371/journal.pone.0177794},
	doi = {10.1371/journal.pone.0177794},
	language = {en},
	number = {5},
	urldate = {2018-02-05},
	journal = {PLOS ONE},
	author = {Lopopolo, Alessandro and Frank, Stefan L. and van den Bosch, Antal and Willems, Roel M.},
	editor = {Allen, Philip},
	month = may,
	year = {2017},
	keywords = {fMRI, language model, LM, project.lstmMEG, project.streams, read, entropy, information theory, surprisal},
	pages = {e0177794},
}

@article{brennan_abstract_2016,
	title = {Abstract linguistic structure correlates with temporal activity during naturalistic comprehension},
	volume = {157-158},
	issn = {0093-934X},
	url = {http://www.sciencedirect.com/science/article/pii/S0093934X15300687},
	doi = {10.1016/j.bandl.2016.04.008},
	abstract = {Neurolinguistic accounts of sentence comprehension identify a network of relevant brain regions, but do not detail the information flowing through them. We investigate syntactic information. Does brain activity implicate a computation over hierarchical grammars or does it simply reflect linear order, as in a Markov chain? To address this question, we quantify the cognitive states implied by alternative parsing models. We compare processing-complexity predictions from these states against fMRI timecourses from regions that have been implicated in sentence comprehension. We find that hierarchical grammars independently predict timecourses from left anterior and posterior temporal lobe. Markov models are predictive in these regions and across a broader network that includes the inferior frontal gyrus. These results suggest that while linear effects are wide-spread across the language network, certain areas in the left temporal lobe deal with abstract, hierarchical syntactic representations.},
	number = {Supplement C},
	urldate = {2017-11-22},
	journal = {Brain and Language},
	author = {Brennan, Jonathan R. and Stabler, Edward P. and Van Wagenen, Sarah E. and Luh, Wen-Ming and Hale, John T.},
	month = jun,
	year = {2016},
	keywords = {syntax, fMRI, language model, LM, project.lstmMEG, project.streams, read},
	pages = {81--94},
}

@article{henderson_language_2016,
	title = {Language structure in the brain: {A} fixation-related {fMRI} study of syntactic surprisal in reading},
	volume = {132},
	issn = {1053-8119},
	shorttitle = {Language structure in the brain},
	doi = {10.1016/j.neuroimage.2016.02.050},
	abstract = {How is syntactic analysis implemented by the human brain during language comprehension? The current study combined methods from computational linguistics, eyetracking, and fMRI to address this question. Subjects read passages of text presented as paragraphs while their eye movements were recorded in an MRI scanner. We parsed the text using a probabilistic context-free grammar to isolate syntactic difficulty. Syntactic difficulty was quantified as syntactic surprisal, which is related to the expectedness of a given word's syntactic category given its preceding context. We compared words with high and low syntactic surprisal values that were equated for length, frequency, and lexical surprisal, and used fixation-related (FIRE) fMRI to measure neural activity associated with syntactic surprisal for each fixated word. We observed greater neural activity for high than low syntactic surprisal in two predicted cortical regions previously identified with syntax: left inferior frontal gyrus (IFG) and less robustly, left anterior superior temporal lobe (ATL). These results support the hypothesis that left IFG and ATL play a central role in syntactic analysis during language comprehension. More generally, the results suggest a broader cortical network associated with syntactic prediction that includes increased activity in bilateral IFG and insula, as well as fusiform and right lingual gyri.},
	number = {Supplement C},
	urldate = {2017-12-06},
	journal = {NeuroImage},
	author = {Henderson, John M. and Choi, Wonil and Lowder, Matthew W. and Ferreira, Fernanda},
	month = may,
	year = {2016},
	keywords = {fMRI, project.streams, read, eye-tracking, surprisal, PSG},
	pages = {293--300},
}

@article{smith_chimaeric_2002,
	title = {Chimaeric sounds reveal dichotomies in auditory perception},
	volume = {416},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/416087a},
	doi = {10.1038/416087a},
	language = {en},
	number = {6876},
	urldate = {2017-12-14},
	journal = {Nature},
	author = {Smith, Zachary M. and Delgutte, Bertrand and Oxenham, Andrew J.},
	month = mar,
	year = {2002},
	keywords = {methods, project.streams},
	pages = {87--90},
}

@article{nolte_magnetic_2003,
	title = {The magnetic lead field theorem in the quasi-static approximation and its use for magnetoencephalography forward calculation in realistic volume conductors},
	volume = {48},
	doi = {10.1088/0031-9155/48/22/002},
	number = {22},
	journal = {Physics in Medicine and Biology},
	author = {Nolte, Guido},
	month = nov,
	year = {2003},
	keywords = {MEG, methods, project.streams, source reconstruction},
	pages = {3637--3652},
}

@inproceedings{oostdijk_spoken_2000,
	title = {The {Spoken} {Dutch} {Corpus}: {Overview} and first evaluation},
	abstract = {The Spoken Dutch Corpus that is currently under construction will constitute a 10-million-word corpus of contemporary Dutch as spoken in Flanders and the Netherlands. A collection of extremely varied data for extremely varied users, the Spoken Dutch Corpus constitutes an ideal case study for evaluating proposals for encoding standards. The paper discusses the nature of the meta-data that are deemed to be relevant for the various user groups of the Spoken Dutch Corpus. It also addresses issues such as how - from a users' point of view - these meta-data should preferably be structured. In addition, the paper evaluates the extent to which available standard proposals are adequate or need to be adapted to suit these needs.},
	booktitle = {Proceedings of {Second} {International} {Conference} on {Language} {Resources} and {Evaluation} ({LREC})},
	author = {Oostdijk, Nelleke},
	year = {2000},
	keywords = {methods, project.streams, corpus, lingustics},
	pages = {887--894},
}

@inproceedings{schafer_building_2012,
	title = {Building large corpora from the web using a new efficient tool chain},
	isbn = {978-2-9517408-7-7},
	url = {http://dblp.uni-trier.de/db/conf/lrec/lrec2012.html#SchaferB12},
	abstract = {Over the last decade,methods of web corpus construction and the evaluation of web corpora have been actively researched. Prominently, the WaCky initiative has provided both theoretical results and a set of web corpora for selected European languages. We present a software toolkit for web corpus construction and a set of siginificantly larger corpora (up to over 9 billion tokens) built using this software. First, we discuss how the data should be collected to ensure that it is not biased towards certain hosts. Then, we describe our software toolkit which performs basic cleanups as well as boilerplate removal, simple connected text detection as well as shingling to remove duplicates from the corpora. We finally report evaluation results of the corpora built so far, for examplew. r. t. the amount of duplication contained and the text type/genre distribution. Where applicable, we compare our corpora to the WaCky corpora, since it is inappropriate, in our view, to compare web corpora to traditional or balanced corpora. While we use some methods applied by theWaCky initiative, we can show that we have introduced incremental improvements},
	booktitle = {Proceedings of the {Eighth} {International} {Conference} on {Language} {Resources} and {Evaluation} ({LREC}'12)},
	publisher = {European Language Resources Association (ELRA)},
	author = {Schäfer, Roland and Bildhauer, Felix},
	year = {2012},
	keywords = {methods, project.streams, computational linguistics, langauge},
	pages = {486--493},
}

@incollection{bosch_efficient_2007,
	address = {Utrecht},
	title = {An efficient memory-based morphosyntactic tagger and parser for {Dutch}},
	url = {http://www.cnts.ua.ac.be/papers/2007/tad07.pdf},
	booktitle = {Computational {Linguistics} in the {Netherlands} 2006: {Selected} papers from the seventeenth {CLIN} {Meeting}},
	publisher = {LOT},
	author = {Bosch, Antal van den and Busser, B. and Canisius, S. and Daelemans, Walter},
	editor = {Dirix, P.},
	year = {2007},
	keywords = {methods, project.streams, computational linguistics, parser},
	pages = {191--206},
}

@article{balkenius_spaces_2016,
	title = {Spaces in the brain: from neurons to meanings},
	volume = {7},
	issn = {1664-1078},
	shorttitle = {Spaces in the {Brain}},
	url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2016.01820/full},
	doi = {10.3389/fpsyg.2016.01820},
	abstract = {Spaces in the brain can refer either to psychological spaces, which are derived from similarity judgments, or to neurocognitive spaces, which are based on the activities of neural structures. We want to show how psychological spaces naturally emerge from the underlying neural spaces by dimension reductions that preserve similarity structures and the relevant categorizations. Some neuronal representational formats that may generate the psychological spaces are presented, compared and discussed in relation to the mathematical principles of monotonicity, continuity and convexity. In particular, we discuss the spatial structures involved in the connections between perception and action, for example eye-hand coordination, and argue that spatial organization of information makes such mappings more efficient.},
	language = {English},
	urldate = {2018-01-31},
	journal = {Frontiers in Psychology},
	author = {Balkenius, Christian and Gärdenfors, Peter},
	year = {2016},
	keywords = {readme, project.vsmMEG, VSM, coding, conceptual spaces, similarity},
	file = {Balkenius and Gärdenfors - 2016 - Spaces in the Brain From Neurons to Meanings.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Balkenius and Gärdenfors - 2016 - Spaces in the Brain From Neurons to Meanings.pdf:application/pdf},
}

@article{parviz_using_2011,
	title = {Using language models and latent semantic analysis to characterise the {N400m} neural response},
	url = {https://aclanthology.coli.uni-saarland.de/papers/U11-1007/u11-1007},
	language = {en},
	urldate = {2018-02-01},
	journal = {Proceedings of the Australasian Language Technology Association Workshop 2011},
	author = {Parviz, Mehdi and Johnson, Mark and Johnson, Blake and Brock, Jon},
	year = {2011},
	pages = {38--46},
}

@article{bullinaria_extracting_2007,
	title = {Extracting semantic representations from word co-occurrence statistics: {A} computational study},
	volume = {39},
	issn = {1554-351X, 1554-3528},
	shorttitle = {Extracting semantic representations from word co-occurrence statistics},
	url = {https://link.springer.com/article/10.3758/BF03193020},
	doi = {10.3758/BF03193020},
	abstract = {The idea that at least some aspects of word meaning can be induced from patterns of word co-occurrence is becoming increasingly popular. However, there is less agreement about the precise computations involved, and the appropriate tests to distinguish between the various possibilities. It is important that the effect of the relevant design choices and parameter values are understood if psychological models using these methods are to be reliably evaluated and compared. In this article, we present a systematic exploration of the principal computational possibilities for formulating and validating representations of word meanings from word co-occurrence statistics. We find that, once we have identified the best procedures, a very simple approach is surprisingly successful and robust over a range of psychologically relevant evaluation measures.},
	language = {en},
	number = {3},
	urldate = {2018-02-01},
	journal = {Behavior Research Methods},
	author = {Bullinaria, John A. and Levy, Joseph P.},
	month = aug,
	year = {2007},
	keywords = {methods, readme, distributional semantics, project.vsmMEG, VSM},
	pages = {510--526},
	file = {Bullinaria and Levy - 2007 - Extracting semantic representations from word co-o.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Bullinaria and Levy - 2007 - Extracting semantic representations from word co-o.pdf:application/pdf},
}

@article{huth_natural_2016,
	title = {Natural speech reveals the semantic maps that tile human cerebral cortex},
	volume = {532},
	copyright = {2016 Nature Publishing Group},
	issn = {1476-4687},
	doi = {10.1038/nature17637},
	abstract = {{\textless}p{\textgreater}It has been proposed that language meaning is represented throughout the cerebral cortex in a distributed ‘semantic system’, but little is known about the details of this network; here, voxel-wise modelling of functional MRI data collected while subjects listened to natural stories is used to create a detailed atlas that maps representations of word meaning in the human brain.{\textless}/p{\textgreater}},
	language = {En},
	number = {7600},
	urldate = {2018-01-08},
	journal = {Nature},
	author = {Huth, Alexander G. and Heer, Wendy A. de and Griffiths, Thomas L. and Theunissen, Frédéric E. and Gallant, Jack L.},
	month = apr,
	year = {2016},
	keywords = {fMRI, project.lstmMEG, readme, project.vsmMEG, fixme, computational neuroscience, semantics},
	pages = {453},
	file = {Huth et al. - 2016 - Natural speech reveals the semantic maps that tile.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Huth et al. - 2016 - Natural speech reveals the semantic maps that tile.pdf:application/pdf},
}

@incollection{landauer_computational_2002,
	title = {On the computational basis of learning and cognition: {Arguments} from {LSA}},
	volume = {41},
	shorttitle = {On the computational basis of learning and cognition},
	url = {http://www.sciencedirect.com/science/article/pii/S0079742102800044},
	abstract = {This chapter discusses the computational basis of learning and cognition. To deal with a continuously changing environment, living things have three choices: (1) evolve unvarying processes that usually succeed, (2) evolve genetically fixed effector, perceptual, and computational functions that are contingent on the environment, and (3) learn adaptive functions during their lifetimes. The theme of this chapter is the relation between (2) and (3): the nature of evolutionarily determined computational processes that support learning. The principal goal of this chapter has been to suggest that high-dimensional vector space computations based on empirical associations among very large numbers of components could be a close model of a fundamental computational basis of most learning in both verbal and perceptual domains. More powerful representational effects can be brought about by linear inductive combinations of the elements of very large vocabularies than has often been realized. Success of one such model to demonstrate many natural properties of language commonly assumed to be essentially more complex, nonlinear, and/or unlearned, along with evidence and argument that similar computations may serve similar roles in object recognition, are taken to reaffirm the possibility that a single underlying associational mechanism lies behind many more special and complex appearing cognitive phenomena.},
	urldate = {2018-01-30},
	booktitle = {Psychology of {Learning} and {Motivation}},
	publisher = {Academic Press},
	author = {Landauer, Thomas K},
	month = jan,
	year = {2002},
	doi = {10.1016/S0079-7421(02)80004-4},
	keywords = {readme, project.vsmMEG, VSM, computation, semantics, learing, LSA},
	pages = {43--84},
	file = {Landauer - 2002 - On the computational basis of learning and cogniti.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Landauer - 2002 - On the computational basis of learning and cogniti.pdf:application/pdf},
}

@inproceedings{chung_hierarchical_2017,
	title = {Hierarchical multiscale recurrent neural networks},
	url = {http://arxiv.org/abs/1609.01704v7},
	abstract = {Learning both hierarchical and temporal representation has been among the long-standing challenges of recurrent neural networks. Multiscale recurrent neural networks have been considered as a promising approach to resolve this issue, yet there has been a lack of empirical evidence showing that this type of models can actually capture the temporal dependencies by discovering the latent hierarchical structure of the sequence. In this paper, we propose a novel multiscale approach, called the hierarchical multiscale recurrent neural networks, which can capture the latent hierarchical structure in the sequence by encoding the temporal dependencies with different timescales using a novel update mechanism. We show some evidence that our proposed multiscale architecture can discover underlying hierarchical structure in the sequences without using explicit boundary information. We evaluate our proposed model on character-level language modelling and handwriting sequence modelling.},
	urldate = {2018-01-22},
	booktitle = {{arXiv}:1609.01704 [cs]},
	author = {Chung, Junyoung and Ahn, Sungjin and Bengio, Yoshua},
	month = mar,
	year = {2017},
	note = {arXiv:1609.01704v7},
	keywords = {readme},
	file = {Chung et al. - 2017 - Hierarchical multiscale recurrent neural networks.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Chung et al. - 2017 - Hierarchical multiscale recurrent neural networks.pdf:application/pdf},
}

@article{turney_frequency_2010,
	title = {From frequency to meaning: vector space models of semantics},
	shorttitle = {From frequency to meaning},
	url = {http://arxiv.org/abs/1003.1141},
	doi = {10.1613/jair.2934},
	abstract = {Computers understand very little of the meaning of human language. This profoundly limits our ability to give instructions to computers, the ability of computers to explain their actions to us, and the ability of computers to analyse and process text. Vector space models (VSMs) of semantics are beginning to address these limits. This paper surveys the use of VSMs for semantic processing of text. We organize the literature on VSMs according to the structure of the matrix in a VSM. There are currently three broad classes of VSMs, based on term-document, word-context, and pair-pattern matrices, yielding three classes of applications. We survey a broad range of applications in these three categories and we take a detailed look at a specific open source project in each category. Our goal in this survey is to show the breadth of applications of VSMs for semantics, to provide a new perspective on VSMs for those who are already familiar with the area, and to provide pointers into the literature for those who are less familiar with the field.},
	urldate = {2018-01-26},
	journal = {arXiv:1003.1141 [cs]},
	author = {Turney, Peter D. and Pantel, Patrick},
	month = mar,
	year = {2010},
	note = {arXiv: 1003.1141},
	keywords = {project.vsmMEG, VSM, read, computational linguistics, thesis.introduction, semantics, bag of words, family resemblance, latent relation hypothesis, linear algebra, semantic similarity},
	file = {Turney and Pantel - 2010 - From frequency to meaning vector space models of .pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Turney and Pantel - 2010 - From frequency to meaning vector space models of .pdf:application/pdf},
}

@article{eliasmith_moving_2003,
	title = {Moving beyond metaphors: understanding the mind for what it is},
	volume = {100},
	issn = {0022-362X},
	shorttitle = {Moving {Beyond} {Metaphors}},
	url = {http://www.pdcnet.org/oom/service?url_ver=Z39.88-2004&rft_val_fmt=&rft.imuse_id=jphil_2003_0100_0010_0493_0520&svc_id=info:www.pdcnet.org/collection},
	doi = {10.5840/jphil2003100102},
	number = {10},
	urldate = {2018-01-26},
	journal = {Journal of Philosophy},
	author = {Eliasmith, Chris},
	year = {2003},
	keywords = {readme, explanation, philosophy of science, computation, philosophy of mind, thesis},
	pages = {493--520},
	file = {Eliasmith - 2003 - Moving beyond metaphors understanding the mind fo.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Eliasmith - 2003 - Moving beyond metaphors understanding the mind fo.pdf:application/pdf},
}

@article{landauer_solution_1997,
	title = {A solution to {Plato}'s problem: {The} latent semantic analysis theory of acquisition, induction, and representation of knowledge.},
	volume = {104},
	issn = {1939-1471, 0033-295X},
	shorttitle = {A solution to {Plato}'s problem},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-295X.104.2.211},
	doi = {10.1037/0033-295X.104.2.211},
	language = {en},
	number = {2},
	urldate = {2018-01-26},
	journal = {Psychological Review},
	author = {Landauer, Thomas K. and Dumais, Susan T.},
	year = {1997},
	keywords = {readme, distributional semantics, project.vsmMEG, VSM, cognitive science, semantics, LSA},
	pages = {211--240},
	file = {Landauer and Dumais - 1997 - A solution to Plato's problem The latent semantic.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Landauer and Dumais - 1997 - A solution to Plato's problem The latent semantic.pdf:application/pdf},
}

@article{yarkoni_choosing_2017,
	title = {Choosing prediction over explanation in psychology: lessons from machine learning},
	volume = {12},
	issn = {1745-6916},
	shorttitle = {Choosing {Prediction} {Over} {Explanation} in {Psychology}},
	url = {https://doi.org/10.1177/1745691617693393},
	doi = {10.1177/1745691617693393},
	abstract = {Psychology has historically been concerned, first and foremost, with explaining the causal mechanisms that give rise to behavior. Randomized, tightly controlled experiments are enshrined as the gold standard of psychological research, and there are endless investigations of the various mediating and moderating variables that govern various behaviors. We argue that psychology’s near-total focus on explaining the causes of behavior has led much of the field to be populated by research programs that provide intricate theories of psychological mechanism but that have little (or unknown) ability to predict future behaviors with any appreciable accuracy. We propose that principles and techniques from the field of machine learning can help psychology become a more predictive science. We review some of the fundamental concepts and tools of machine learning and point out examples where these concepts have been used to conduct interesting and important psychological research that focuses on predictive research questions. We suggest that an increased focus on prediction, rather than explanation, can ultimately lead us to greater understanding of behavior.},
	language = {en},
	number = {6},
	urldate = {2018-01-25},
	journal = {Perspectives on Psychological Science},
	author = {Yarkoni, Tal and Westfall, Jacob},
	month = nov,
	year = {2017},
	keywords = {readme, statistics, ML, explanation, cognitive science},
	pages = {1100--1122},
	file = {Yarkoni and Westfall - 2017 - Choosing prediction over explanation in psychology.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Yarkoni and Westfall - 2017 - Choosing prediction over explanation in psychology.pdf:application/pdf},
}

@incollection{clark_vector_2015,
	edition = {2},
	title = {Vector space models of lexical meaning},
	isbn = {978-0-470-67073-6},
	language = {en},
	booktitle = {Handbook of contemporary semantic theory},
	publisher = {Wiley-Blackwell},
	author = {Clark, Stephen},
	year = {2015},
	keywords = {readme, distributional semantics, project.vsmMEG, VSM},
	pages = {493--552},
	file = {Clark - 2015 - Vector space models of lexical meaning.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Clark - 2015 - Vector space models of lexical meaning.pdf:application/pdf},
}

@article{broderick_electrophysiological_2017,
	title = {Electrophysiological correlates of semantic dissimilarity reflect the comprehension of natural, narrative speech.},
	copyright = {© 2017, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/early/2017/09/24/193201},
	doi = {10.1101/193201},
	abstract = {Understanding natural speech requires that the human brain convert complex spectrotemporal patterns of acoustic input into meaning in a rapid manner that is reasonably tightly time-locked to the incoming speech signal. However, neural evidence for such a time-locked process has been lacking. Here, we sought such evidence by using a computational model to quantify the meaning carried by each word based on how semantically dissimilar it was to its preceding context and then regressing this quantity against electroencephalographic (EEG) data recorded from subjects as they listened to narrative speech. This produced a prominent negativity at a time-lag of 200-600 ms on centro-parietal EEG electrodes. Subsequent EEG experiments involving time-reversed speech, cocktail party attention and audiovisual speech-in-noise demonstrated that this response was exquisitely sensitive to whether or not subjects were understanding the speech they heard. These findings demonstrate that, when successfully comprehending natural speech, the human brain encodes meaning as a function of the amount of new information carried by each word in a relatively time-locked fashion.},
	language = {en},
	urldate = {2018-01-12},
	journal = {bioRxiv},
	author = {Broderick, Michael P. and Anderson, Andrew J. and Liberto, Giovanni M. Di and Crosse, Michael J. and Lalor, Edmund C.},
	month = sep,
	year = {2017},
	keywords = {readme, project.vsmMEG, VSM, semantics, word embedding},
	pages = {193201},
	file = {Broderick et al. - 2017 - Electrophysiological correlates of semantic dissim.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Broderick et al. - 2017 - Electrophysiological correlates of semantic dissim.pdf:application/pdf},
}

@article{stephens_place_2013,
	title = {A place for time: the spatiotemporal structure of neural dynamics during natural audition},
	volume = {110},
	copyright = {Copyright © 2013 the American Physiological Society},
	issn = {0022-3077, 1522-1598},
	shorttitle = {A place for time},
	url = {http://jn.physiology.org/content/110/9/2019},
	doi = {10.1152/jn.00268.2013},
	abstract = {We use functional magnetic resonance imaging (fMRI) to analyze neural responses to natural auditory stimuli. We characterize the fMRI time series through the shape of the voxel power spectrum and find that the timescales of neural dynamics vary along a spatial gradient, with faster dynamics in early auditory cortex and slower dynamics in higher order brain regions. The timescale gradient is observed through the unsupervised clustering of the power spectra of individual brains, both in the presence and absence of a stimulus, and is enhanced in the stimulus-locked component that is shared across listeners. Moreover, intrinsically faster dynamics occur in areas that respond preferentially to momentary stimulus features, while the intrinsically slower dynamics occur in areas that integrate stimulus information over longer timescales. These observations connect the timescales of intrinsic neural dynamics to the timescales of information processing, suggesting a temporal organizing principle for neural computation across the cerebral cortex.},
	language = {en},
	number = {9},
	urldate = {2017-11-27},
	journal = {Journal of Neurophysiology},
	author = {Stephens, Greg J. and Honey, Christopher J. and Hasson, Uri},
	month = nov,
	year = {2013},
	pmid = {23926041},
	keywords = {fMRI, project.lstmMEG, read, TRW, project.lstmMRI},
	pages = {2019--2026},
	file = {Stephens et al. - 2013 - A place for time the spatiotemporal structure of .pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Stephens et al. - 2013 - A place for time the spatiotemporal structure of .pdf:application/pdf},
}

@book{fromkin_peace_2009,
	address = {New York},
	edition = {2nd Holt pbk. ed},
	title = {A peace to end all peace: the fall of the {Ottoman} {Empire} and the creation of the modern {Middle} {East}},
	isbn = {978-0-8050-8809-0},
	shorttitle = {A peace to end all peace},
	publisher = {H. Holt and Co},
	author = {Fromkin, David},
	year = {2009},
	keywords = {history, international relations, middle east, non-fiction, reading list},
}

@article{gjorgjieva_computational_2016,
	series = {Neurobiology of cognitive behavior},
	title = {Computational implications of biophysical diversity and multiple timescales in neurons and synapses for circuit performance},
	volume = {37},
	issn = {0959-4388},
	url = {http://www.sciencedirect.com/science/article/pii/S0959438815001865},
	doi = {10.1016/j.conb.2015.12.008},
	abstract = {Despite advances in experimental and theoretical neuroscience, we are still trying to identify key biophysical details that are important for characterizing the operation of brain circuits. Biological mechanisms at the level of single neurons and synapses can be combined as ‘building blocks’ to generate circuit function. We focus on the importance of capturing multiple timescales when describing these intrinsic and synaptic components. Whether inherent in the ionic currents, the neuron's complex morphology, or the neurotransmitter composition of synapses, these multiple timescales prove crucial for capturing the variability and richness of circuit output and enhancing the information-carrying capacity observed across nervous systems.},
	urldate = {2018-01-21},
	journal = {Current Opinion in Neurobiology},
	author = {Gjorgjieva, Julijana and Drion, Guillaume and Marder, Eve},
	month = apr,
	year = {2016},
	keywords = {project.lstmMEG, readme, TRW, computational neuroscience, theory, time},
	pages = {44--52},
}

@article{honey_slow_2012,
	title = {Slow cortical dynamics and the accumulation of information over long timescales},
	volume = {76},
	issn = {0896-6273},
	url = {http://www.sciencedirect.com/science/article/pii/S0896627312007179},
	doi = {10.1016/j.neuron.2012.08.011},
	abstract = {Summary
Making sense of the world requires us to process information over multiple timescales. We sought to identify brain regions that accumulate information over short and long timescales and to characterize the distinguishing features of their dynamics. We recorded electrocorticographic (ECoG) signals from individuals watching intact and scrambled movies. Within sensory regions, fluctuations of high-frequency (64–200 Hz) power reliably tracked instantaneous low-level properties of the intact and scrambled movies. Within higher order regions, the power fluctuations were more reliable for the intact movie than the scrambled movie, indicating that these regions accumulate information over relatively long time periods (several seconds or longer). Slow ({\textless}0.1 Hz) fluctuations of high-frequency power with time courses locked to the movies were observed throughout the cortex. Slow fluctuations were relatively larger in regions that accumulated information over longer time periods, suggesting a connection between slow neuronal population dynamics and temporally extended information processing.},
	number = {2},
	urldate = {2017-11-20},
	journal = {Neuron},
	author = {Honey, Christopher J. and Thesen, Thomas and Donner, Tobias H. and Silbert, Lauren J. and Carlson, Chad E. and Devinsky, Orrin and Doyle, Werner K. and Rubin, Nava and Heeger, David J. and Hasson, Uri},
	month = oct,
	year = {2012},
	keywords = {project.lstmMEG, read, TRW, ECoG},
	pages = {423--434},
	file = {Honey et al. - 2012 - Slow cortical dynamics and the accumulation of inf.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Honey et al. - 2012 - Slow cortical dynamics and the accumulation of inf.pdf:application/pdf},
}

@article{palangi_deep_2015,
	title = {Deep sentence embedding using long short-term memory networks: {Analysis} and application to information retrieval},
	doi = {10.1109/TASLP.2016.2520371},
	abstract = {This paper develops a model that addresses sentence embedding, a hot topic in current natural language processing research, using recurrent neural networks with Long Short-Term Memory (LSTM) cells. Due to its ability to capture long term memory, the LSTM-RNN accumulates increasingly richer information as it goes through the sentence, and when it reaches the last word, the hidden layer of the network provides a semantic representation of the whole sentence. In this paper, the LSTM-RNN is trained in a weakly supervised manner on user click-through data logged by a commercial web search engine. Visualization and analysis are performed to understand how the embedding process works. The model is found to automatically attenuate the unimportant words and detects the salient keywords in the sentence. Furthermore, these detected keywords are found to automatically activate different cells of the LSTM-RNN, where words belonging to a similar topic activate the same cell. As a semantic representation of the sentence, the embedding vector can be used in many different applications. These automatic keyword detection and topic allocation abilities enabled by the LSTM-RNN allow the network to perform document retrieval, a difficult language processing task, where the similarity between the query and documents can be measured by the distance between their corresponding sentence embedding vectors computed by the LSTM-RNN. On a web search task, the LSTM-RNN embedding is shown to significantly outperform several existing state of the art methods. We emphasize that the proposed model generates sentence embedding vectors that are specially useful for web document retrieval tasks. A comparison with a well known general sentence embedding method, the Paragraph Vector, is performed. The results show that the proposed method in this paper significantly outperforms it for web document retrieval task.},
	journal = {arXiv},
	author = {Palangi, Hamid and Deng, Li and Shen, Yelong and Gao, Jianfeng and He, Xiaodong and Chen, Jianshu and Song, Xinying and Ward, Rabab},
	month = feb,
	year = {2015},
	keywords = {project.lstmMEG, readme},
}

@misc{judge_way_2018,
	title = {The way music moves us shows the mind is more than a machine – {Jenny} {Judge} {\textbar} {Aeon} {Essays}},
	url = {https://aeon.co/essays/the-way-music-moves-us-shows-the-mind-is-more-than-a-machine},
	abstract = {Music reminds us that the mind is more than a calculator. We are resonant bodies as much as representing machines},
	language = {en},
	urldate = {2018-01-20},
	journal = {Aeon},
	author = {Judge, Jenny},
	month = jan,
	year = {2018},
	keywords = {fixme, read, philosophy of mind, cognitive science, time, entrainment, music, nyc, synchrony},
	file = {Snapshot:/Users/kriarm/Zotero/storage/Q2ZN4CW2/the-way-music-moves-us-shows-the-mind-is-more-than-a-machine.html:text/html},
}

@article{mars_connectivity_2018,
	series = {The {Evolution} of {Language}},
	title = {Connectivity and the search for specializations in the language-capable brain},
	volume = {21},
	issn = {2352-1546},
	url = {http://www.sciencedirect.com/science/article/pii/S235215461730181X},
	doi = {10.1016/j.cobeha.2017.11.001},
	abstract = {The search for the anatomical basis of language has traditionally been a search for specializations. More recently such research has focused both on aspects of brain organization that are unique to humans and aspects shared with other primates. This work has mostly concentrated on the architecture of connections between brain areas. However, as specializations can take many guises, comparison of anatomical organization across species is often complicated. We demonstrate how viewing different types of specializations within a common framework allows one to better appreciate both shared and unique aspects of brain organization. We illustrate this point by discussing recent insights into the anatomy of the dorsal language pathway to the frontal cortex and areas for laryngeal control in the motor cortex.},
	urldate = {2018-01-20},
	journal = {Current Opinion in Behavioral Sciences},
	author = {Mars, Rogier B and Eichert, Nicole and Jbabdi, Saad and Verhagen, Lennart and Rushworth, Matthew F. S.},
	month = jun,
	year = {2018},
	keywords = {read},
	pages = {19--26},
	file = {Mars et al. - 2018 - Connectivity and the search for specializations in.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Mars et al. - 2018 - Connectivity and the search for specializations in.pdf:application/pdf},
}

@article{duarte_synaptic_2017,
	series = {Neurobiology of {Learning} and {Plasticity}},
	title = {Synaptic patterning and the timescales of cortical dynamics},
	volume = {43},
	issn = {0959-4388},
	url = {http://www.sciencedirect.com/science/article/pii/S0959438817300545},
	doi = {10.1016/j.conb.2017.02.007},
	abstract = {Neocortical circuits, as large heterogeneous recurrent networks, can potentially operate and process signals at multiple timescales, but appear to be differentially tuned to operate within certain temporal receptive windows. The modular and hierarchical organization of this selectivity mirrors anatomical and physiological relations throughout the cortex and is likely determined by the regional electrochemical composition. Being consistently patterned and actively regulated, the expression of molecules involved in synaptic transmission constitutes the most significant source of laminar and regional variability. Due to their complex kinetics and adaptability, synapses form a natural primary candidate underlying this regional temporal selectivity. The ability of cortical networks to reflect the temporal structure of the sensory environment can thus be regulated by evolutionary and experience-dependent processes.},
	urldate = {2018-01-12},
	journal = {Current Opinion in Neurobiology},
	author = {Duarte, Renato and Seeholzer, Alexander and Zilles, Karl and Morrison, Abigail},
	month = apr,
	year = {2017},
	keywords = {project.lstmMEG, fixme, read, TRW, time, receptor, synapse, transmitter},
	pages = {156--165},
	file = {Duarte et al. - 2017 - Synaptic patterning and the timescales of cortical.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Duarte et al. - 2017 - Synaptic patterning and the timescales of cortical.pdf:application/pdf},
}

@article{de-wit_is_2016,
	title = {Is neuroimaging measuring information in the brain?},
	volume = {23},
	issn = {1069-9384, 1531-5320},
	url = {https://link.springer.com/article/10.3758/s13423-016-1002-0},
	doi = {10.3758/s13423-016-1002-0},
	abstract = {Psychology moved beyond the stimulus response mapping of behaviorism by adopting an information processing framework. This shift from behavioral to cognitive science was partly inspired by work demonstrating that the concept of information could be defined and quantified (Shannon, 1948). This transition developed further from cognitive science into cognitive neuroscience, in an attempt to measure information in the brain. In the cognitive neurosciences, however, the term information is often used without a clear definition. This paper will argue that, if the formulation proposed by Shannon is applied to modern neuroimaging, then numerous results would be interpreted differently. More specifically, we argue that much modern cognitive neuroscience implicitly focuses on the question of how we can interpret the activations we record in the brain (experimenter-as-receiver), rather than on the core question of how the rest of the brain can interpret those activations (cortex-as-receiver). A clearer focus on whether activations recorded via neuroimaging can actually act as information in the brain would not only change how findings are interpreted but should also change the direction of empirical research in cognitive neuroscience.},
	language = {en},
	number = {5},
	urldate = {2017-12-02},
	journal = {Psychonomic Bulletin \& Review},
	author = {de-Wit, Lee and Alexander, David and Ekroll, Vebjørn and Wagemans, Johan},
	month = oct,
	year = {2016},
	keywords = {readme, explanation, philosophy of neuroscience, philosophy of science, epistemology, connectivity},
	pages = {1415--1428},
	file = {de-Wit et al. - 2016 - Is neuroimaging measuring information in the brain.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/de-Wit et al. - 2016 - Is neuroimaging measuring information in the brain.pdf:application/pdf},
}

@article{burnston_computational_2016,
	title = {Computational neuroscience and localized neural function},
	volume = {193},
	issn = {0039-7857, 1573-0964},
	url = {https://link.springer.com/article/10.1007/s11229-016-1099-8},
	doi = {10.1007/s11229-016-1099-8},
	abstract = {In this paper I criticize a view of functional localization in neuroscience, which I call “computational absolutism” (CA). “Absolutism” in general is the view that each part of the brain should be given a single, univocal function ascription. Traditional varieties of absolutism posit that each part of the brain processes a particular type of information and/or performs a specific task. These function attributions are currently beset by physiological evidence which seems to suggest that brain areas are multifunctional—that they process distinct information and perform different tasks depending on context. Many theorists take this contextual variation as inimical to successful localization, and claim that we can avoid it by changing our functional descriptions to computational descriptions. The idea is that we can have highly generalizable and predictive functional theories if we can discover a single computation performed by each area regardless of the specific context in which it operates. I argue, drawing on computational models of perceptual area MT, that this computational version of absolutism fails to come through on its promises. In MT, the modeling field has not produced a univocal computational description, but instead a plurality of models analyzing different aspects of MT function. Moreover, CA cannot appeal to theoretical unification to solve this problem, since highly general models, on their own, neither explain nor predict what MT does in any particular context. I close by offering a perspective on neural modeling inspired by Nancy Cartwright’s and Margaret Morrison’s views of modeling in the physical sciences.},
	language = {en},
	number = {12},
	urldate = {2017-12-01},
	journal = {Synthese},
	author = {Burnston, Daniel C.},
	month = dec,
	year = {2016},
	keywords = {readme, explanation, philosophy of neuroscience, philosophy of science, epistemology, computational neuroscience},
	pages = {3741--3762},
	file = {Burnston - 2016 - Computational neuroscience and localized neural fu.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Burnston - 2016 - Computational neuroscience and localized neural fu.pdf:application/pdf},
}

@article{chirimuuta_minimal_2014,
	title = {Minimal models and canonical neural computations: the distinctness of computational explanation in neuroscience},
	volume = {191},
	issn = {0039-7857, 1573-0964},
	shorttitle = {Minimal models and canonical neural computations},
	url = {https://link.springer.com/article/10.1007/s11229-013-0369-y},
	doi = {10.1007/s11229-013-0369-y},
	abstract = {In a recent paper, Kaplan (Synthese 183:339–373, 2011) takes up the task of extending Craver’s (Explaining the brain, 2007) mechanistic account of explanation in neuroscience to the new territory of computational neuroscience. He presents the model to mechanism mapping (3M) criterion as a condition for a model’s explanatory adequacy. This mechanistic approach is intended to replace earlier accounts which posited a level of computational analysis conceived as distinct and autonomous from underlying mechanistic details. In this paper I discuss work in computational neuroscience that creates difficulties for the mechanist project. Carandini and Heeger (Nat Rev Neurosci 13:51–62, 2012) propose that many neural response properties can be understood in terms of canonical neural computations. These are “standard computational modules that apply the same fundamental operations in a variety of contexts.” Importantly, these computations can have numerous biophysical realisations, and so straightforward examination of the mechanisms underlying these computations carries little explanatory weight. Through a comparison between this modelling approach and minimal models in other branches of science, I argue that computational neuroscience frequently employs a distinct explanatory style, namely, efficient coding explanation. Such explanations cannot be assimilated into the mechanistic framework but do bear interesting similarities with evolutionary and optimality explanations elsewhere in biology.},
	language = {en},
	number = {2},
	urldate = {2017-11-24},
	journal = {Synthese},
	author = {Chirimuuta, M.},
	month = jan,
	year = {2014},
	keywords = {explanation, philosophy of neuroscience, philosophy of science, read, computation, efficient coding},
	pages = {127--153},
	file = {Chirimuuta - 2014 - Minimal models and canonical neural computations .pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Chirimuuta - 2014 - Minimal models and canonical neural computations .pdf:application/pdf},
}

@techreport{perkel_neural_1968,
	address = {Massachusetts, US},
	title = {Neural {Coding}. {A} {Report} based on {NRP} {Work} {Session}.},
	url = {https://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/19690022317.pdf},
	author = {Perkel, Donald H. and Holmes Bullock, Theodore},
	year = {1968},
	keywords = {readme, explanation, computation, epistemology, neural coding},
	file = {Perkel in Holmes Bullock - 1968 - Neural Coding. A Report based on NRP Work Session..pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Perkel in Holmes Bullock - 1968 - Neural Coding. A Report based on NRP Work Session..pdf:application/pdf},
}

@article{willems_prediction_2016,
	title = {Prediction during natural language comprehension},
	volume = {26},
	issn = {1047-3211},
	doi = {10.1093/cercor/bhv075},
	abstract = {The notion of prediction is studied in cognitive neuroscience with increasing intensity. We investigated the neural basis of 2 distinct aspects of word prediction, derived from information theory, during story comprehension. We assessed the effect of entropy of next-word probability distributions as well as surprisal. A computational model determined entropy and surprisal for each word in 3 literary stories. Twenty-four healthy participants listened to the same 3 stories while their brain activation was measured using fMRI. Reversed speech fragments were presented as a control condition. Brain areas sensitive to entropy were left ventral premotor cortex, left middle frontal gyrus, right inferior frontal gyrus, left inferior parietal lobule, and left supplementary motor area. Areas sensitive to surprisal were left inferior temporal sulcus (“visual word form area”), bilateral superior temporal gyrus, right amygdala, bilateral anterior temporal poles, and right inferior frontal sulcus. We conclude that prediction during language comprehension can occur at several levels of processing, including at the level of word form. Our study exemplifies the power of combining computational linguistics with cognitive neuroscience, and additionally underlines the feasibility of studying continuous spoken language materials with fMRI.},
	number = {6},
	urldate = {2017-12-06},
	journal = {Cerebral Cortex},
	author = {Willems, Roel M. and Frank, Stefan L. and Nijhof, Annabel D. and Hagoort, Peter and van den Bosch, Antal},
	month = jun,
	year = {2016},
	keywords = {fMRI, language model, LM, project.streams, read},
	pages = {2506--2516},
}

@book{gallistel_memory_2009,
	address = {Chichester, West Sussex, UK ; Malden, MA},
	title = {Memory and the computational brain: why cognitive science will transform neuroscience},
	isbn = {978-1-4051-2287-0 978-1-4051-2288-7},
	shorttitle = {Memory and the computational brain},
	publisher = {Wiley-Blackwell},
	author = {Gallistel, C. R. and King, Adam Philip},
	year = {2009},
	keywords = {cognitive neuroscience, hierarchy, explanation, entropy, information, information theory, probability theory, computation, mutual information, thesis.introduction, cognitive science, reading, thesis, efficient coding, neural coding},
	file = {Gallistel and King - 2009 - Memory and the computational brain.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Gallistel and King - 2009 - Memory and the computational brain.pdf:application/pdf},
}

@article{heeger_theory_2017,
	title = {Theory of cortical function},
	volume = {114},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/content/114/8/1773},
	doi = {10.1073/pnas.1619788114},
	abstract = {Most models of sensory processing in the brain have a feedforward architecture in which each stage comprises simple linear filtering operations and nonlinearities. Models of this form have been used to explain a wide range of neurophysiological and psychophysical data, and many recent successes in artificial intelligence (with deep convolutional neural nets) are based on this architecture. However, neocortex is not a feedforward architecture. This paper proposes a first step toward an alternative computational framework in which neural activity in each brain area depends on a combination of feedforward drive (bottom-up from the previous processing stage), feedback drive (top-down context from the next stage), and prior drive (expectation). The relative contributions of feedforward drive, feedback drive, and prior drive are controlled by a handful of state parameters, which I hypothesize correspond to neuromodulators and oscillatory activity. In some states, neural responses are dominated by the feedforward drive and the theory is identical to a conventional feedforward model, thereby preserving all of the desirable features of those models. In other states, the theory is a generative model that constructs a sensory representation from an abstract representation, like memory recall. In still other states, the theory combines prior expectation with sensory input, explores different possible perceptual interpretations of ambiguous sensory inputs, and predicts forward in time. The theory, therefore, offers an empirically testable framework for understanding how the cortex accomplishes inference, exploration, and prediction.},
	language = {en},
	number = {8},
	urldate = {2018-01-12},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Heeger, David J.},
	month = feb,
	year = {2017},
	pmid = {28167793},
	keywords = {hierarchy, project.lstmMEG, readme, computation, inference, thesis.introduction, theory, project.6, time},
	pages = {1773--1782},
}

@article{jackendoff_defense_2017,
	title = {In defense of theory},
	volume = {41},
	issn = {1551-6709},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/cogs.12324/abstract},
	doi = {10.1111/cogs.12324},
	abstract = {Formal theories of mental representation have receded from the importance they had in the early days of cognitive science. I argue that such theories are crucial in any mental domain, not just for their own sake, but to guide experimental inquiry, as well as to integrate the domain into the mind as a whole. To illustrate the criteria of adequacy for theories of mental representation, I compare two theoretical approaches to language: classical generative grammar (Chomsky, 1965, 1981, 1995) and the parallel architecture (Jackendoff, 1997, 2002). The grounds for comparison include (a) the internal coherence of the theory across phonology, syntax, and semantics; (b) the relation of language to other mental faculties; (c) the relationship between grammar and lexicon; (d) relevance to theories of language processing; and (e) the possibility of languages with little or no syntax.},
	language = {en},
	urldate = {2018-01-12},
	journal = {Cognitive Science},
	author = {Jackendoff, Ray},
	month = mar,
	year = {2017},
	keywords = {readme, grammar, thesis.introduction, theory, project.6, FCS},
	pages = {185--212},
	file = {Snapshot:/Users/kriarm/Zotero/storage/BEGD2YYK/abstract.html:text/html},
}

@book{jackendoff_foundations_2003,
	address = {Oxford, New York},
	title = {Foundations of {Language}: {Brain}, {Meaning}, {Grammar}, {Evolution}},
	isbn = {978-0-19-926437-7},
	shorttitle = {Foundations of {Language}},
	abstract = {How does human language work? How do we put ideas into words that others can understand? Can linguistics shed light on the way the brain operates?   Foundations of Language puts linguistics back at the centre of the search to understand human consciousness. Ray Jackendoff begins by surveying the developments in linguistics over the years since Noam Chomsky's Aspects of the Theory of Syntax. He goes on to propose a radical re-conception of how the brain processes language. This opens up vivid new perspectives on every major aspect of language and communication, including grammar, vocabulary, learning, the origins of human language, and how language relates to the real world. Foundations of Language makes important connections with other disciplines which have been isolated from linguistics for many years. It sets a new agenda for close cooperation between the study of language, mind, the brain, behaviour, and evolution.},
	publisher = {Oxford University Press},
	author = {Jackendoff, Ray},
	month = sep,
	year = {2003},
	keywords = {syntax, project.lstmMEG, project.streams, thesis.introduction, cognitive science, language, common, thesis},
}

@article{chaudhuri_computational_2016,
	title = {Computational principles of memory},
	volume = {19},
	copyright = {2016 Nature Publishing Group},
	issn = {1546-1726},
	url = {https://www.nature.com/articles/nn.4237},
	doi = {10.1038/nn.4237},
	abstract = {{\textless}p{\textgreater}What are the challenges associated with storing information over time in the brain? Here the authors explore the computational principles by which biological memory might be built. They develop a high-level view of shared problems and themes in short- and long-term memory and highlight questions for future research.{\textless}/p{\textgreater}},
	language = {En},
	number = {3},
	urldate = {2018-01-11},
	journal = {Nature Neuroscience},
	author = {Chaudhuri, Rishidev and Fiete, Ila},
	month = mar,
	year = {2016},
	keywords = {project.lstmMEG, readme, computation, computational neuroscience, memory, reading},
	pages = {394},
	file = {Chaudhuri in Fiete - 2016 - Computational principles of memory.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Chaudhuri in Fiete - 2016 - Computational principles of memory.pdf:application/pdf},
}

@article{hubert_receptive_1962,
	title = {Receptive fields, binocular interaction and functional architecture in the cat’s visual cortex.},
	volume = {160},
	journal = {Journal of Physiology},
	author = {Hubert, D. H. and Wiesel, T. N.},
	year = {1962},
	keywords = {hierarchy, readme, vision, visual cortex},
	pages = {106--154},
	file = {Hubert and Wiesel - 1962 - Receptive fields, binocular interaction and functi.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Hubert and Wiesel - 1962 - Receptive fields, binocular interaction and functi.pdf:application/pdf},
}

@article{maris_nonparametric_2007,
	title = {Nonparametric statistical testing of {EEG}- and {MEG}-data},
	volume = {164},
	issn = {0165-0270},
	doi = {10.1016/j.jneumeth.2007.03.024},
	abstract = {In this paper, we show how ElectroEncephaloGraphic (EEG) and MagnetoEncephaloGraphic (MEG) data can be analyzed statistically using nonparametric techniques. Nonparametric statistical tests offer complete freedom to the user with respect to the test statistic by means of which the experimental conditions are compared. This freedom provides a straightforward way to solve the multiple comparisons problem (MCP) and it allows to incorporate biophysically motivated constraints in the test statistic, which may drastically increase the sensitivity of the statistical test. The paper is written for two audiences: (1) empirical neuroscientists looking for the most appropriate data analysis method, and (2) methodologists interested in the theoretical concepts behind nonparametric statistical tests. For the empirical neuroscientist, a large part of the paper is written in a tutorial-like fashion, enabling neuroscientists to construct their own statistical test, maximizing the sensitivity to the expected effect. And for the methodologist, it is explained why the nonparametric test is formally correct. This means that we formulate a null hypothesis (identical probability distribution in the different experimental conditions) and show that the nonparametric test controls the false alarm rate under this null hypothesis.},
	number = {1},
	urldate = {2018-01-10},
	journal = {Journal of Neuroscience Methods},
	author = {Maris, Eric and Oostenveld, Robert},
	month = aug,
	year = {2007},
	keywords = {MEG, EEG, methods, project.streams, nonparametric statistics},
	pages = {177--190},
	file = {Maris and Oostenveld - 2007 - Nonparametric statistical testing of EEG- and MEG-.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Maris and Oostenveld - 2007 - Nonparametric statistical testing of EEG- and MEG-.pdf:application/pdf},
}

@article{keuleers_subtlex-nl:_2010,
	title = {{SUBTLEX}-{NL}: {A} new measure for {Dutch} word frequency based on film subtitles},
	volume = {42},
	issn = {1554-351X, 1554-3528},
	shorttitle = {{SUBTLEX}-{NL}},
	doi = {10.3758/BRM.42.3.643},
	language = {en},
	number = {3},
	urldate = {2018-01-10},
	journal = {Behavior Research Methods},
	author = {Keuleers, Emmanuel and Brysbaert, Marc and New, Boris},
	month = aug,
	year = {2010},
	keywords = {methods, project.streams, thesis, corpus, SUBTLEX},
	pages = {643--650},
}

@article{gross_dynamic_2001,
	title = {Dynamic imaging of coherent sources: {Studying} neural interactions in the human brain},
	volume = {98},
	doi = {10.1073/pnas.98.2.694},
	abstract = {Functional connectivity between cortical areas may appear as correlated time behavior of neural activity. It has been suggested that merging of separate features into a single percept ("binding") is associated with coherent gamma band activity across the cortical areas involved. Therefore, it would be of utmost interest to image cortico-cortical coherence in the working human brain. The frequency specificity and transient nature of these interactions requires time-sensitive tools such as magneto- or electroencephalography (MEG/EEG). Coherence between signals of sensors covering different scalp areas is commonly taken as a measure of functional coupling. However, this approach provides vague information on the actual cortical areas involved, owing to the complex relation between the active brain areas and the sensor recordings. We propose a solution to the crucial issue of proceeding beyond the MEG sensor level to estimate coherences between cortical areas. Dynamic imaging of coherent sources (DICS) uses a spatial filter to localize coherent brain regions and provides the time courses of their activity. Reference points for the computation of neural coupling may be based on brain areas of maximum power or other physiologically meaningful information, or they may be estimated starting from sensor coherences. The performance of DICS is evaluated with simulated data and illustrated with recordings of spontaneous activity in a healthy subject and a parkinsonian patient. Methods for estimating functional connectivities between brain areas will facilitate characterization of cortical networks involved in sensory, motor, or cognitive tasks and will allow investigation of pathological connectivities in neurological disorders.},
	number = {2},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Gross, J and Kujala, J and Hamalainen, M and Timmermann, L and Schnitzler, A and Salmelin, R},
	month = jan,
	year = {2001},
	pmid = {11209067},
	keywords = {MEG, methods, project.streams, neural oscillations, common, source reconstruction, DICS},
	pages = {694--9},
}

@article{dehaene_neural_2015,
	title = {The neural representation of sequences: from transition probabilities to algebraic patterns and linguistic trees},
	volume = {88},
	issn = {0896-6273},
	shorttitle = {The {Neural} {Representation} of {Sequences}},
	url = {http://www.sciencedirect.com/science/article/pii/S089662731500776X},
	doi = {10.1016/j.neuron.2015.09.019},
	abstract = {A sequence of images, sounds, or words can be stored at several levels of detail, from specific items and their timing to abstract structure. We propose a taxonomy of five distinct cerebral mechanisms for sequence coding: transitions and timing knowledge, chunking, ordinal knowledge, algebraic patterns, and nested tree structures. In each case, we review the available experimental paradigms and list the behavioral and neural signatures of the systems involved. Tree structures require a specific recursive neural code, as yet unidentified by electrophysiology, possibly unique to humans, and which may explain the singularity of human language and cognition.},
	number = {1},
	urldate = {2017-11-27},
	journal = {Neuron},
	author = {Dehaene, Stanislas and Meyniel, Florent and Wacongne, Catherine and Wang, Liping and Pallier, Christophe},
	month = oct,
	year = {2015},
	keywords = {syntax, hierarchy, project.lstmMEG, readme, language},
	pages = {2--19},
	file = {Dehaene idr. - 2015 - The neural representation of sequences from trans.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Dehaene et al. - 2015 - The neural representation of sequences from trans.pdf:application/pdf},
}

@article{fries_rhythms_2015,
	title = {Rhythms for cognition: communication through coherence},
	volume = {88},
	issn = {0896-6273},
	shorttitle = {Rhythms for {Cognition}},
	doi = {10.1016/j.neuron.2015.09.034},
	language = {English},
	number = {1},
	urldate = {2018-01-10},
	journal = {Neuron},
	author = {Fries, Pascal},
	month = oct,
	year = {2015},
	pmid = {26447583},
	keywords = {project.streams, read, neural oscillations, thesis.introduction},
	pages = {220--235},
}

@inproceedings{chien_hierarchical_2017,
	title = {A hierarchical model for sequential perception and  sequence learning},
	url = {http://pmg.dynamoadmin.org/documents/1031/5928ca0868ed3f824e8a2577.pdf},
	urldate = {2017-11-28},
	author = {Chien and Honey, Christopher J.},
	year = {2017},
	keywords = {hierarchy, learning, project.lstmMEG, readme, TRW, sequence},
	file = {Chien and Honey - 2017 - A hierarchical model for sequential perception and.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Chien and Honey - 2017 - A hierarchical model for sequential perception and.pdf:application/pdf},
}

@article{guclu_deep_2015,
	title = {Deep neural networks reveal a gradient in the complexity of neural representations across the ventral stream},
	volume = {35},
	copyright = {Copyright © 2015 the authors 0270-6474/15/3510005-10\$15.00/0},
	issn = {0270-6474, 1529-2401},
	doi = {10.1523/JNEUROSCI.5023-14.2015},
	abstract = {Converging evidence suggests that the primate ventral visual pathway encodes increasingly complex stimulus features in downstream areas. We quantitatively show that there indeed exists an explicit gradient for feature complexity in the ventral pathway of the human brain. This was achieved by mapping thousands of stimulus features of increasing complexity across the cortical sheet using a deep neural network. Our approach also revealed a fine-grained functional specialization of downstream areas of the ventral stream. Furthermore, it allowed decoding of representations from human brain activity at an unsurpassed degree of accuracy, confirming the quality of the developed approach. Stimulus features that successfully explained neural responses indicate that population receptive fields were explicitly tuned for object categorization. This provides strong support for the hypothesis that object categorization is a guiding principle in the functional organization of the primate ventral stream.},
	language = {en},
	number = {27},
	urldate = {2018-01-09},
	journal = {Journal of Neuroscience},
	author = {Güçlü, Umut and Gerven, Marcel A. J. van},
	month = jul,
	year = {2015},
	pmid = {26157000},
	keywords = {project.lstmMEG, readme, ANN, complexity, DNN},
	pages = {10005--10014},
	file = {Güçlü and Gerven - 2015 - Deep neural networks reveal a gradient in the comp.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Güçlü and Gerven - 2015 - Deep neural networks reveal a gradient in the comp.pdf:application/pdf},
}

@article{palmeri_model-based_2017,
	series = {Model-based {Cognitive} {Neuroscience}},
	title = {Model-based cognitive neuroscience},
	volume = {76},
	issn = {0022-2496},
	url = {http://www.sciencedirect.com/science/article/pii/S002224961630116X},
	doi = {10.1016/j.jmp.2016.10.010},
	abstract = {This special issue explores the growing intersection between mathematical psychology and cognitive neuroscience. Mathematical psychology, and cognitive modeling more generally, has a rich history of formalizing and testing hypotheses about cognitive mechanisms within a mathematical and computational language, making exquisite predictions of how people perceive, learn, remember, and decide. Cognitive neuroscience aims to identify neural mechanisms associated with key aspects of cognition using techniques like neurophysiology, electrophysiology, and structural and functional brain imaging. These two come together in a powerful new approach called model-based cognitive neuroscience, which can both inform cognitive modeling and help to interpret neural measures. Cognitive models decompose complex behavior into representations and processes and these latent model states can be used to explain the modulation of brain states under different experimental conditions. Reciprocally, neural measures provide data that help constrain cognitive models and adjudicate between competing cognitive models that make similar predictions about behavior. As examples, brain measures are related to cognitive model parameters fitted to individual participant data, measures of brain dynamics are related to measures of model dynamics, model parameters are constrained by neural measures, model parameters or model states are used in statistical analyses of neural data, or neural and behavioral data are analyzed jointly within a hierarchical modeling framework. We provide an introduction to the field of model-based cognitive neuroscience and to the articles contained within this special issue.},
	number = {Part B},
	urldate = {2018-01-04},
	journal = {Journal of Mathematical Psychology},
	author = {Palmeri, Thomas J. and Love, Bradley C. and Turner, Brandon M.},
	month = feb,
	year = {2017},
	keywords = {cognitive neuroscience, read, computation, model-based},
	pages = {59--64},
}

@article{wang_flexible_2018,
	title = {Flexible timing by temporal scaling of cortical responses},
	volume = {21},
	copyright = {2017 The Author(s)},
	issn = {1546-1726},
	doi = {10.1038/s41593-017-0028-6},
	abstract = {Humans can deliberately control the timing of their actions but the neural mechanisms underlying such control are largely unknown. In this article, Wang, Narain and their colleagues report that such flexibility emerges in rhesus monkeys from the ability of their brain to flexibly control the speed at which cortical responses unfold in time.},
	language = {En},
	number = {1},
	urldate = {2018-01-05},
	journal = {Nature Neuroscience},
	author = {Wang, Jing and Narain, Devika and Hosseini, Eghbal A. and Jazayeri, Mehrdad},
	month = jan,
	year = {2018},
	keywords = {reading},
	pages = {102},
}

@article{boone_cognitive_2016,
	title = {The cognitive neuroscience revolution},
	volume = {193},
	issn = {0039-7857, 1573-0964},
	url = {https://link.springer.com/article/10.1007/s11229-015-0783-4},
	doi = {10.1007/s11229-015-0783-4},
	abstract = {We outline a framework of multilevel neurocognitive mechanisms that incorporates representation and computation. We argue that paradigmatic explanations in cognitive neuroscience fit this framework and thus that cognitive neuroscience constitutes a revolutionary break from traditional cognitive science. Whereas traditional cognitive scientific explanations were supposed to be distinct and autonomous from mechanistic explanations, neurocognitive explanations aim to be mechanistic through and through. Neurocognitive explanations aim to integrate computational and representational functions and structures across multiple levels of organization in order to explain cognition. To a large extent, practicing cognitive neuroscientists have already accepted this shift, but philosophical theory has not fully acknowledged and appreciated its significance. As a result, the explanatory framework underlying cognitive neuroscience has remained largely implicit. We explicate this framework and demonstrate its contrast with previous approaches.},
	language = {en},
	number = {5},
	urldate = {2017-12-20},
	journal = {Synthese},
	author = {Boone, Worth and Piccinini, Gualtiero},
	month = may,
	year = {2016},
	keywords = {cognitive neuroscience, explanation, fixme, philosophy of neuroscience, philosophy of science, read, computation, cognitive science, project.6, thesis, Hodgkin-Huxley model, representation, representation.definition},
	pages = {1509--1534},
	file = {Boone and Piccinini - 2016 - The cognitive neuroscience revolution.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Boone and Piccinini - 2016 - The cognitive neuroscience revolution.pdf:application/pdf},
}

@article{buranyi_is_2017,
	chapter = {Science},
	title = {Is the staggeringly profitable business of scientific publishing bad for science?},
	issn = {0261-3077},
	url = {http://www.theguardian.com/science/2017/jun/27/profitable-business-scientific-publishing-bad-for-science},
	abstract = {The long read: It is an industry like no other, with profit margins to rival Google – and it was created by one of Britain’s most notorious tycoons: Robert Maxwell},
	language = {en-GB},
	urldate = {2018-01-01},
	journal = {The Guardian},
	author = {Buranyi, Stephen},
	month = jun,
	year = {2017},
	keywords = {read, metascience, peer-review, Elsevier, publishing},
}

@book{brandt_runaway_2017,
	title = {The runaway species: how human creativity remakes the world},
	isbn = {978-0-85786-206-8 978-0-85786-207-5 978-1-936787-52-4},
	shorttitle = {The runaway species},
	abstract = {The brain is typically portrayed as an organ with a map of regions dedicated to specific tasks. But, says acclaimed neuroscientist and bestselling author David Eagleman, that textbook model is wrong. The brain is a dynamic system; the connections between its cells are constantly blossoming, dying and reconfiguring. Drawing on up-to-the-minute research, Eagleman takes us on a fascinating journey into brain plasticity to discover how a child can function with one half of his brain removed and how a blind mountain climber can use an electrode grid on his tongue to 'see'. He proves how the brain optimizes its circuitry based on the tasks relevant to goals and survival and how this knowledge opens the door to dazzling new technologies. The magic of our brains lies in the way they unceasingly re-weave themselves to form an electric, living fabric. Eagleman gets to the heart of who we are and how the brain plasticity revolution is lighting up the path of the future by analysing how our most vital organ works in a way that has never been done before.},
	language = {English},
	author = {Brandt, Anthony K and Eagleman, David},
	year = {2017},
	note = {OCLC: 1006711354},
}

@incollection{daugman_brain_1993,
	address = {Cambridge, MA, USA},
	title = {Brain metaphor and brain theory},
	isbn = {0-262-69164-7},
	url = {http://dl.acm.org/citation.cfm?id=174471.174473},
	booktitle = {Computational neuroscience},
	publisher = {MIT Press},
	author = {Daugman, John G.},
	editor = {Schwartz, Eric L.},
	year = {1993},
	keywords = {readme, explanation, philosophy of neuroscience, philosophy of science, metascience, theory, computer metaphor, metaphor},
	pages = {9--18},
}

@article{piccinini_computational_2007,
	title = {Computational modelling vs. {Computational} explanation: {Is} everything a {Turing} {Machine}, and does it matter to the philosophy of mind?},
	volume = {85},
	issn = {0004-8402},
	shorttitle = {Computational modelling vs. {Computational} explanation},
	url = {http://www-tandfonline-com.ru.idm.oclc.org/doi/abs/10.1080/00048400601176494},
	doi = {10.1080/00048400601176494},
	abstract = {According to pancomputationalism, everything is a computing system. In this paper, I distinguish between different varieties of pancomputationalism. I find that although some varieties are more plausible than others, only the strongest variety is relevant to the philosophy of mind, but only the most trivial varieties are true. As a side effect of this exercise, I offer a clarified distinction between computational modelling and computational explanation.},
	number = {1},
	urldate = {2017-12-20},
	journal = {Australasian Journal of Philosophy},
	author = {Piccinini, Gualtiero},
	month = mar,
	year = {2007},
	keywords = {explanation, philosophy of neuroscience, philosophy of science, read, mechanistic explanation, computation, epistemology, project.6, representation.definition, computability theory, computation.definition, definition, digit.definition, explanatory force, functional explanation},
	pages = {93--115},
	file = {Piccinini - 2007 - Computational modelling vs. Computational explanat.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Piccinini - 2007 - Computational modelling vs. Computational explanat.pdf:application/pdf},
}

@article{friston_mathematics_2017,
	title = {The mathematics of mind-time},
	url = {https://aeon.co/essays/consciousness-is-not-a-thing-but-a-process-of-inference},
	abstract = {The special trick of consciousness is being able to project action and time into a range of possible futures},
	language = {English},
	urldate = {2017-12-24},
	journal = {Aeon},
	author = {Friston, Karl},
	month = may,
	year = {2017},
	keywords = {evidence, read, information, probability theory, inference, time, consciousness, levels, surprise},
	file = {Snapshot:/Users/kriarm/Zotero/storage/WMMM52IP/consciousness-is-not-a-thing-but-a-process-of-inference.html:text/html},
}

@article{van_den_bosch_memory-based_2009,
	title = {Memory-based machine translation and language modeling},
	volume = {91},
	issn = {1804-0462, 0032-6585},
	doi = {10.2478/v10108-009-0012-8},
	number = {-1},
	urldate = {2017-12-14},
	journal = {The Prague Bulletin of Mathematical Linguistics},
	author = {van den Bosch, Antal and Berck, Peter},
	month = jan,
	year = {2009},
	keywords = {language model, LM, methods, project.streams, computational linguistics},
}

@article{maass_searching_2016,
	series = {Computational modeling},
	title = {Searching for principles of brain computation},
	volume = {11},
	issn = {2352-1546},
	url = {http://www.sciencedirect.com/science/article/pii/S235215461630119X},
	doi = {10.1016/j.cobeha.2016.06.003},
	abstract = {Experimental methods in neuroscience, such as calcium-imaging and recordings with multi-electrode arrays, are advancing at a rapid pace. They produce insight into the simultaneous activity of large numbers of neurons, and into plasticity processes in the brains of awake and behaving animals. These new data constrain models for neural computation and network plasticity that underlie perception, cognition, behavior, and learning. I will discuss in this short article four such constraints: inherent recurrent network activity and heterogeneous dynamic properties of neurons and synapses, stereotypical spatio-temporal activity patterns in networks of neurons, high trial-to-trial variability of network responses, and functional stability in spite of permanently ongoing changes in the network. I am proposing that these constraints provide hints to underlying principles of brain computation and learning.},
	number = {Supplement C},
	urldate = {2017-12-21},
	journal = {Current Opinion in Behavioral Sciences},
	author = {Maass, Wolfgang},
	month = oct,
	year = {2016},
	keywords = {project.lstmMEG, readme, computation, thesis.introduction, computational neuroscience, model-based, project.6, model},
	pages = {81--92},
	file = {Maass - 2016 - Searching for principles of brain computation.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Maass - 2016 - Searching for principles of brain computation.pdf:application/pdf;ScienceDirect Snapshot:/Users/kriarm/Zotero/storage/U5XK6YMP/S235215461630119X.html:text/html},
}

@incollection{frank_linking_2015,
	title = {Linking across levels of computation in model-based cognitive neuroscience},
	isbn = {978-1-4939-2235-2 978-1-4939-2236-9},
	url = {https://link.springer.com/chapter/10.1007/978-1-4939-2236-9_8},
	abstract = {Computational approaches to cognitive neuroscience encompass multiple levels of analysis, from detailed biophysical models of neural activity to abstract algorithmic or normative models of cognition, with several levels in between. Despite often strong opinions on the ‘right’ level of modeling, there is no single panacea: attempts to link biological with higher level cognitive processes require a multitude of approaches. Here I argue that these disparate approaches should not be viewed as competitive, nor should they be accessible to only other researchers already endorsing the particular level of modeling. Rather, insights gained from one level of modeling should inform modeling endeavors at the level above and below it. One way to achieve this synergism is to link levels of modeling by quantitatively fitting the behavioral outputs of detailed mechanistic models with higher level descriptions. If the fits are reasonable (e.g., similar to those achieved when applying high level models to human behavior), one can then derive plausible links between mechanism and computation. Model-based cognitive neuroscience approaches can then be employed to manipulate or measure neural function motivated by the candidate mechanisms, and to test whether these are related to high level model parameters. I describe several examples of this approach in the domain of reward-based learning, cognitive control, and decision making and show how neural and algorithmic models have each informed or refined the other.},
	language = {en},
	urldate = {2017-12-21},
	booktitle = {An {Introduction} to {Model}-{Based} {Cognitive} {Neuroscience}},
	publisher = {Springer, New York, NY},
	author = {Frank, Michael J.},
	year = {2015},
	doi = {10.1007/978-1-4939-2236-9_8},
	keywords = {readme, computation, thesis.introduction, computational neuroscience, model-based, project.6, levels, decision-making},
	pages = {159--177},
}

@article{jonas_could_2017,
	title = {Could a neuroscientist understand a microprocessor?},
	volume = {13},
	issn = {1553-7358},
	url = {http://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1005268},
	doi = {10.1371/journal.pcbi.1005268},
	abstract = {Author Summary Neuroscience is held back by the fact that it is hard to evaluate if a conclusion is correct; the complexity of the systems under study and their experimental inaccessability make the assessment of algorithmic and data analytic technqiues challenging at best. We thus argue for testing approaches using known artifacts, where the correct interpretation is known. Here we present a microprocessor platform as one such test case. We find that many approaches in neuroscience, when used naïvely, fall short of producing a meaningful understanding.},
	number = {1},
	urldate = {2017-12-21},
	journal = {PLOS Computational Biology},
	author = {Jonas, Eric and Kording, Konrad Paul},
	month = dec,
	year = {2017},
	keywords = {cognitive neuroscience, explanation, philosophy of neuroscience, philosophy of science, read, project.6, understanding, CPU},
	pages = {e1005268},
	file = {Jonas and Kording - 2017 - Could a neuroscientist understand a microprocessor.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Jonas and Kording - 2017 - Could a neuroscientist understand a microprocessor.pdf:application/pdf},
}

@article{huntenburg_large-scale_2018,
	title = {Large-scale gradients in human cortical organization},
	volume = {22},
	issn = {1364-6613, 1879-307X},
	url = {http://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(17)30240-1},
	doi = {10.1016/j.tics.2017.11.002},
	language = {English},
	number = {1},
	urldate = {2017-12-20},
	journal = {Trends in Cognitive Sciences},
	author = {Huntenburg, Julia M. and Bazin, Pierre-Louis and Margulies, Daniel S.},
	month = jan,
	year = {2018},
	pmid = {29203085},
	keywords = {hierarchy, project.lstmMEG, read, TRW, thesis.introduction, thesis, cortex, gradient},
	pages = {21--31},
	file = {Huntenburg et al. - 2018 - Large-scale gradients in human cortical organizati.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Huntenburg et al. - 2018 - Large-scale gradients in human cortical organizati.pdf:application/pdf},
}

@article{mesulam_sensation_1998,
	title = {From sensation to cognition},
	volume = {121 ( Pt 6)},
	issn = {0006-8950},
	abstract = {Sensory information undergoes extensive associative elaboration and attentional modulation as it becomes incorporated into the texture of cognition. This process occurs along a core synaptic hierarchy which includes the primary sensory, upstream unimodal, downstream unimodal, heteromodal, paralimbic and limbic zones of the cerebral cortex. Connections from one zone to another are reciprocal and allow higher synaptic levels to exert a feedback (top-down) influence upon earlier levels of processing. Each cortical area provides a nexus for the convergence of afferents and divergence of efferents. The resultant synaptic organization supports parallel as well as serial processing, and allows each sensory event to initiate multiple cognitive and behavioural outcomes. Upstream sectors of unimodal association areas encode basic features of sensation such as colour, motion, form and pitch. More complex contents of sensory experience such as objects, faces, word-forms, spatial locations and sound sequences become encoded within downstream sectors of unimodal areas by groups of coarsely tuned neurons. The highest synaptic levels of sensory-fugal processing are occupied by heteromodal, paralimbic and limbic cortices, collectively known as transmodal areas. The unique role of these areas is to bind multiple unimodal and other transmodal areas into distributed but integrated multimodal representations. Transmodal areas in the midtemporal cortex, Wernicke's area, the hippocampal-entorhinal complex and the posterior parietal cortex provide critical gateways for transforming perception into recognition, word-forms into meaning, scenes and events into experiences, and spatial locations into targets for exploration. All cognitive processes arise from analogous associative transformations of similar sets of sensory inputs. The differences in the resultant cognitive operation are determined by the anatomical and physiological properties of the transmodal node that acts as the critical gateway for the dominant transformation. Interconnected sets of transmodal nodes provide anatomical and computational epicentres for large-scale neurocognitive networks. In keeping with the principles of selectively distributed processing, each epicentre of a large-scale network displays a relative specialization for a specific behavioural component of its principal neurospychological domain. The destruction of transmodal epicentres causes global impairments such as multimodal anomia, neglect and amnesia, whereas their selective disconnection from relevant unimodal areas elicits modality-specific impairments such as prosopagnosia, pure word blindness and category-specific anomias. The human brain contains at least five anatomically distinct networks. The network for spatial awareness is based on transmodal epicentres in the posterior parietal cortex and the frontal eye fields; the language network on epicentres in Wernicke's and Broca's areas; the explicit memory/emotion network on epicentres in the hippocampal-entorhinal complex and the amygdala; the face-object recognition network on epicentres in the midtemporal and temporopolar cortices; and the working memory-executive function network on epicentres in the lateral prefrontal cortex and perhaps the posterior parietal cortex. Individual sensory modalities give rise to streams of processing directed to transmodal nodes belonging to each of these networks. The fidelity of sensory channels is actively protected through approximately four synaptic levels of sensory-fugal processing. The modality-specific cortices at these four synaptic levels encode the most veridical representations of experience. Attentional, motivational and emotional modulations, including those related to working memory, novelty-seeking and mental imagery, become increasingly more pronounced within downstream components of unimodal areas, where they help to create a highly edited subjective version of the world. (ABSTRACT TRUNCATED)},
	language = {eng},
	journal = {Brain: A Journal of Neurology},
	author = {Mesulam, M. M.},
	month = jun,
	year = {1998},
	pmid = {9648540},
	keywords = {cognitive neuroscience, hierarchy, project.lstmMEG, readme, common, thesis, cognitive neuroscience.theory, cortical organization},
	pages = {1013--1052},
}

@article{taylor_global_2015,
	title = {The global landscape of cognition: hierarchical aggregation as an organizational principle of human cortical networks and functions},
	volume = {5},
	copyright = {2015 Nature Publishing Group},
	issn = {2045-2322},
	shorttitle = {The global landscape of cognition},
	url = {https://www.nature.com/articles/srep18112},
	doi = {10.1038/srep18112},
	abstract = {The global landscape of cognition: hierarchical aggregation as an organizational principle of human cortical networks and functions},
	language = {En},
	urldate = {2017-12-21},
	journal = {Scientific Reports},
	author = {Taylor, P. and Hobbs, J. N. and Burroni, J. and Siegelmann, H. T.},
	month = dec,
	year = {2015},
	keywords = {hierarchy, project.lstmMEG, readme, TRW, cerebral organization},
	pages = {18112},
	file = {Taylor et al. - 2015 - The global landscape of cognition hierarchical ag.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Taylor et al. - 2015 - The global landscape of cognition hierarchical ag.pdf:application/pdf},
}

@article{piccinini_computational_2006,
	title = {Computational explanation in neuroscience},
	volume = {153},
	issn = {0039-7857, 1573-0964},
	url = {https://link.springer.com/article/10.1007/s11229-006-9096-y},
	doi = {10.1007/s11229-006-9096-y},
	abstract = {According to some philosophers, computational explanation is proprietary to psychology—it does not belong in neuroscience. But neuroscientists routinely offer computational explanations of cognitive phenomena. In fact, computational explanation was initially imported from computability theory into the science of mind by neuroscientists, who justified this move on neurophysiological grounds. Establishing the legitimacy and importance of computational explanation in neuroscience is one thing; shedding light on it is another. I raise some philosophical questions pertaining to computational explanation and outline some promising answers that are being developed by a number of authors.},
	language = {en},
	number = {3},
	urldate = {2017-12-20},
	journal = {Synthese},
	author = {Piccinini, Gualtiero},
	month = dec,
	year = {2006},
	keywords = {readme, explanation, fixme, philosophy of neuroscience, philosophy of science, computation, project.6, Synthese},
	pages = {343--353},
	file = {Piccinini - 2006 - Computational explanation in neuroscience.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Piccinini - 2006 - Computational explanation in neuroscience.pdf:application/pdf},
}

@article{rusanen_computational_2016,
	title = {On computational explanations},
	volume = {193},
	issn = {0039-7857, 1573-0964},
	url = {https://link.springer.com/article/10.1007/s11229-016-1101-5},
	doi = {10.1007/s11229-016-1101-5},
	abstract = {Computational explanations focus on information processing required in specific cognitive capacities, such as perception, reasoning or decision-making. These explanations specify the nature of the information processing task, what information needs to be represented, and why it should be operated on in a particular manner. In this article, the focus is on three questions concerning the nature of computational explanations: (1) What type of explanations they are, (2) in what sense computational explanations are explanatory and (3) to what extent they involve a special, “independent” or “autonomous” level of explanation. In this paper, we defend the view computational explanations are genuine explanations, which track non-causal/formal dependencies. Specifically, we argue that they do not provide mere sketches for explanation, in contrast to what for example Piccinini and Craver (Synthese 183(3):283–311, 2011) suggest. This view of computational explanations implies some degree of “autonomy” for the computational level. However, as we will demonstrate that does not make this view “computationally chauvinistic” in a way that Piccinini (Synthese 153:343–353, 2006b) or Kaplan (Synthese 183(3):339–373, 2011) have charged it to be.},
	language = {en},
	number = {12},
	urldate = {2017-12-20},
	journal = {Synthese},
	author = {Rusanen, Anna-Mari and Lappi, Otto},
	month = dec,
	year = {2016},
	keywords = {readme, causation, explanation, philosophy of neuroscience, philosophy of science, computation, cognitive science, project.6},
	pages = {3931--3949},
}

@article{levy_unity_2016,
	title = {The unity of neuroscience: a flat view},
	volume = {193},
	issn = {0039-7857, 1573-0964},
	shorttitle = {The unity of neuroscience},
	url = {https://link.springer.com/article/10.1007/s11229-016-1256-0},
	doi = {10.1007/s11229-016-1256-0},
	abstract = {This paper offers a novel view of unity in neuroscience. I set out by discussing problems with the classical account of unity-by-reduction, due to Oppenheim and Putnam. That view relies on a strong notion of levels, which has substantial problems. A more recent alternative, the mechanistic “mosaic” view due to Craver, does not have such problems. But I argue that the mosaic ideal of unity is too minimal, and we should, if possible, aspire for more. Relying on a number of recent works in theoretical neuroscience—network motifs, canonical neural computations (CNCs) and design-principles—I then present my alternative: a “flat” view of unity, i.e. one that is not based on levels. Instead, it treats unity as attained via the identification of recurrent explanatory patterns, under which a range of neuroscientific phenomena are subsumed. I develop this view by recourse to a causal conception of explanation, and distinguish it from Kitcher’s view of explanatory unification and related ideas. Such a view of unity is suitably ambitious, I suggest, and has empirical plausibility. It is fit to serve as an appropriate working hypothesis for 21st century neuroscience.},
	language = {en},
	number = {12},
	urldate = {2017-12-20},
	journal = {Synthese},
	author = {Levy, Arnon},
	month = dec,
	year = {2016},
	keywords = {readme, explanation, philosophy of neuroscience, philosophy of science, epistemology, levels, Synthese, unity of science},
	pages = {3843--3863},
}

@article{edelman_minority_2016,
	title = {The minority report: some common assumptions to reconsider in the modelling of the brain and behaviour},
	volume = {28},
	issn = {0952-813X},
	shorttitle = {The minority report},
	url = {https://doi.org/10.1080/0952813X.2015.1042534},
	doi = {10.1080/0952813X.2015.1042534},
	abstract = {Reverse-engineering the brain involves adopting and testing a hierarchy of working hypotheses regarding the computational problems that it solves, the representations and algorithms that it employs and the manner in which these are implemented. Because problem-level assumptions set the course for the entire research programme, it is particularly important to be open to the possibility that we have them wrong, but tacit algorithm- and implementation-level hypotheses can also benefit from occasional scrutiny. This paper focuses on the extent to which our computational understanding of how the brain works is shaped by three such rarely discussed assumptions, which span the levels of Marr's hierarchy: (i) that animal behaviour amounts to a series of stimulus/response bouts, (ii) that learning can be adequately modelled as being driven by the optimisation of a fixed objective function and (iii) that massively parallel, uniformly connected layered or recurrent network architectures suffice to support learning and behaviour. In comparison, a more realistic approach acknowledges that animal behaviour in the wild is characterised by dynamically branching serial order and is often agentic rather than reactive. Arguably, such behaviour calls for open-ended learning of world structure and may require a neural architecture that includes precisely wired circuits reflecting the serial and branching structure of behavioural tasks.},
	number = {4},
	urldate = {2017-12-19},
	journal = {Journal of Experimental \& Theoretical Artificial Intelligence},
	author = {Edelman, Shimon},
	month = jul,
	year = {2016},
	keywords = {readme, explanation, ANN, computational neuroscience, model, DNN, reinforcment learning},
	pages = {751--776},
}

@article{liu_perceptual_2017,
	title = {Perceptual integration rapidly activates dorsal visual pathway to guide local processing in early visual areas},
	volume = {15},
	issn = {1545-7885},
	url = {http://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.2003646},
	doi = {10.1371/journal.pbio.2003646},
	abstract = {Author summary How the brain integrates local elements into a global object (i.e., perceptual integration) in noisy contexts constitutes a fundamental yet challenging question in cognitive neuroscience. Here, we recorded brain activity by using magnetoencephalography from human subjects watching glass-pattern stimuli to examine the fine spatiotemporal neuronal responses during perceptual integration. We demonstrate that high-level brain regions initially extract a coarse global form of the inputs, which is then relayed along the dorsal visual pathway in a reversed hierarchical manner to low-level areas to modulate local analysis. This global-to-local modulation mechanism is especially beneficial in noisy environments by rapidly making an “initial guess” to guide detail analysis so that the ambiguities in inputs can be efficiently resolved.},
	number = {11},
	urldate = {2017-12-15},
	journal = {PLOS Biology},
	author = {Liu, Ling and Wang, Fan and Zhou, Ke and Ding, Nai and Luo, Huan},
	month = nov,
	year = {2017},
	keywords = {readme},
	pages = {e2003646},
}

@article{miller_canonical_2016,
	series = {Neurobiology of cognitive behavior},
	title = {Canonical computations of cerebral cortex},
	volume = {37},
	issn = {0959-4388},
	url = {http://www.sciencedirect.com/science/article/pii/S095943881600009X},
	doi = {10.1016/j.conb.2016.01.008},
	abstract = {The idea that there is a fundamental cortical circuit that performs canonical computations remains compelling though far from proven. Here we review evidence for two canonical operations within sensory cortical areas: a feedforward computation of selectivity; and a recurrent computation of gain in which, given sufficiently strong external input, perhaps from multiple sources, intracortical input largely, but not completely, cancels this external input. This operation leads to many characteristic cortical nonlinearities in integrating multiple stimuli. The cortical computation must combine such local processing with hierarchical processing across areas. We point to important changes in moving from sensory cortex to motor and frontal cortex and the possibility of substantial differences between cortex in rodents vs. species with columnar organization of selectivity.},
	number = {Supplement C},
	urldate = {2017-12-15},
	journal = {Current Opinion in Neurobiology},
	author = {Miller, Kenneth D},
	month = apr,
	year = {2016},
	keywords = {hierarchy, readme, explanation, computation, ANN, computational neuroscience, project.6, reading, CNC},
	pages = {75--84},
	file = {Miller - 2016 - Canonical computations of cerebral cortex.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Miller - 2016 - Canonical computations of cerebral cortex.pdf:application/pdf},
}

@article{carandini_normalization_2012,
	title = {Normalization as a canonical neural computation},
	volume = {13},
	copyright = {2011 Nature Publishing Group},
	issn = {1471-0048},
	url = {https://www.nature.com/articles/nrn3136},
	doi = {10.1038/nrn3136},
	abstract = {{\textless}p{\textgreater}Normalization computes a ratio between the response of an individual neuron and the summed activity of a pool of neurons. Here, the authors review the evidence that it serves as a canonical computation — one that is applied to processing different types of information in multiple brain regions in multiple species.{\textless}/p{\textgreater}},
	language = {En},
	number = {1},
	urldate = {2017-12-15},
	journal = {Nature Reviews Neuroscience},
	author = {Carandini, Matteo and Heeger, David J.},
	month = jan,
	year = {2012},
	keywords = {readme},
	pages = {51},
}

@article{graves_hybrid_2016,
	title = {Hybrid computing using a neural network with dynamic external memory},
	volume = {538},
	copyright = {2016 Nature Publishing Group},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature20101},
	doi = {10.1038/nature20101},
	abstract = {{\textless}p{\textgreater}A ‘differentiable neural computer’ is introduced that combines the learning capabilities of a neural network with an external memory analogous to the random-access memory in a conventional computer.{\textless}/p{\textgreater}},
	language = {En},
	number = {7626},
	urldate = {2017-12-15},
	journal = {Nature},
	author = {Graves, Alex and Wayne, Greg and Reynolds, Malcolm and Harley, Tim and Danihelka, Ivo and Grabska-Barwińska, Agnieszka and Colmenarejo, Sergio Gómez and Grefenstette, Edward and Ramalho, Tiago and Agapiou, John and Badia, Adrià Puigdomènech and Hermann, Karl Moritz and Zwols, Yori and Ostrovski, Georg and Cain, Adam and King, Helen and Summerfield, Christopher and Blunsom, Phil and Kavukcuoglu, Koray and Hassabis, Demis},
	month = oct,
	year = {2016},
	keywords = {project.lstmMEG, computation, ANN, memory, reading},
	pages = {471},
}

@article{willems_other_2014,
	title = {On the other hand: including left-handers in cognitive neuroscience and neurogenetics},
	volume = {15},
	copyright = {2014 Nature Publishing Group},
	issn = {1471-0048},
	shorttitle = {On the other hand},
	url = {https://www.nature.com/articles/nrn3679},
	doi = {10.1038/nrn3679},
	abstract = {{\textless}p{\textgreater}Left-handers are often excluded from neuroscience and neurogenetics studies in order to reduce variance in the data. In this Perspective, Willems \textit{et al}. discuss the potential of studying this substantial but often-ignored portion of the population.{\textless}/p{\textgreater}},
	language = {En},
	number = {3},
	urldate = {2017-12-14},
	journal = {Nature Reviews Neuroscience},
	author = {Willems, Roel M. and Haegen, Lise Van der and Fisher, Simon E. and Francks, Clyde},
	month = mar,
	year = {2014},
	keywords = {cognitive neuroscience, project.streams, read, common, handedness},
	pages = {193},
}

@article{haufe_interpretation_2014,
	title = {On the interpretation of weight vectors of linear models in multivariate neuroimaging},
	volume = {87},
	issn = {1053-8119},
	url = {http://www.sciencedirect.com/science/article/pii/S1053811913010914},
	doi = {10.1016/j.neuroimage.2013.10.067},
	abstract = {The increase in spatiotemporal resolution of neuroimaging devices is accompanied by a trend towards more powerful multivariate analysis methods. Often it is desired to interpret the outcome of these methods with respect to the cognitive processes under study. Here we discuss which methods allow for such interpretations, and provide guidelines for choosing an appropriate analysis for a given experimental goal: For a surgeon who needs to decide where to remove brain tissue it is most important to determine the origin of cognitive functions and associated neural processes. In contrast, when communicating with paralyzed or comatose patients via brain–computer interfaces, it is most important to accurately extract the neural processes specific to a certain mental state. These equally important but complementary objectives require different analysis methods. Determining the origin of neural processes in time or space from the parameters of a data-driven model requires what we call a forward model of the data; such a model explains how the measured data was generated from the neural sources. Examples are general linear models (GLMs). Methods for the extraction of neural information from data can be considered as backward models, as they attempt to reverse the data generating process. Examples are multivariate classifiers. Here we demonstrate that the parameters of forward models are neurophysiologically interpretable in the sense that significant nonzero weights are only observed at channels the activity of which is related to the brain process under study. In contrast, the interpretation of backward model parameters can lead to wrong conclusions regarding the spatial or temporal origin of the neural signals of interest, since significant nonzero weights may also be observed at channels the activity of which is statistically independent of the brain process under study. As a remedy for the linear case, we propose a procedure for transforming backward models into forward models. This procedure enables the neurophysiological interpretation of the parameters of linear backward models. We hope that this work raises awareness for an often encountered problem and provides a theoretical basis for conducting better interpretable multivariate neuroimaging analyses.},
	number = {Supplement C},
	urldate = {2017-11-22},
	journal = {NeuroImage},
	author = {Haufe, Stefan and Meinecke, Frank and Görgen, Kai and Dähne, Sven and Haynes, John-Dylan and Blankertz, Benjamin and Bießmann, Felix},
	month = feb,
	year = {2014},
	keywords = {fMRI, EEG, project.lstmMEG, readme, encoding, multivariate},
	pages = {96--110},
}

@article{nicholas_peer_2015,
	title = {Peer review: still king in the digital age},
	volume = {28},
	issn = {1741-4857},
	shorttitle = {Peer review},
	url = {http://onlinelibrary.wiley.com/doi/10.1087/20150104/abstract},
	doi = {10.1087/20150104},
	abstract = {The article presents one of the main findings of an international study of 4,000 academic researchers that examined how trustworthiness is determined in the digital environment when it comes to scholarly reading, citing, and publishing. The study shows that peer review is still the most trustworthy characteristic of all. There is, though, a common perception that open access journals are not peer reviewed or do not have proper peer-review systems. Researchers appear to have moved inexorably from a print-based system to a digital system, but it has not significantly changed the way they decide what to trust. They do not trust social media. Only a minority – although significantly mostly young and early career researchers – thought that social media are anything other than more appropriate to personal interactions and peripheral to their professional/academic lives. There are other significant differences, according to the age of the researcher. Thus, in regard to choosing an outlet for publication of their work, young researchers are much less concerned with the fact that it is peer reviewed.},
	language = {en},
	number = {1},
	urldate = {2017-11-30},
	journal = {Learned Publishing},
	author = {Nicholas, David and Watkinson, Anthony and Jamali, Hamid R. and Herman, Eti and Tenopir, Carol and Volentine, Rachel and Allard, Suzie and Levine, Kenneth},
	month = jan,
	year = {2015},
	keywords = {read, metascience, peer-review},
	pages = {15--21},
}

@article{maris_nonparametric_2007-1,
	title = {Nonparametric statistical testing of coherence differences},
	volume = {163},
	issn = {0165-0270},
	url = {http://www.sciencedirect.com/science/article/pii/S016502700700074X},
	doi = {10.1016/j.jneumeth.2007.02.011},
	abstract = {Many important questions in neuroscience are about interactions between neurons or neuronal groups. These interactions are often quantified by coherence, which is a frequency-indexed measure that quantifies the extent to which two signals exhibit a consistent phase relation. In this paper, we consider the statistical testing of the difference between coherence values observed in two experimental conditions. We pay special attention to problems induced by (1) unequal sample sizes and (2) the fact that coherence is typically evaluated at a large number of frequency bins and between large numbers of pairs of neurons or neuronal groups (the multiple comparisons problem). We show that nonparametric statistical tests provide convincing and elegant solutions for both problems. We also show that these tests allow to incorporate biophysically motivated constraints in the test statistic, which may drastically increase the sensitivity of the test. Finally, we explain why the nonparametric test is formally correct. This means that we formulate a null hypothesis (identical probability distribution in the different experimental conditions) and show that the nonparametric test controls the false alarm rate under this null hypothesis. The proposed methodology is illustrated by analyses of data collected in a study on cortico-spinal coherence [Schoffelen JM, Oostenveld R, Fries P. Neuronal coherence as a mechanism of effective corticospinal interaction. Science 2005;308(5718):111-3].},
	number = {1},
	urldate = {2017-12-14},
	journal = {Journal of Neuroscience Methods},
	author = {Maris, Eric and Schoffelen, Jan-Mathijs and Fries, Pascal},
	month = jun,
	year = {2007},
	keywords = {methods, project.streams, statistics, read, common, nonparametric statistics, chapter1, coherence},
	pages = {161--175},
}

@article{christiansen_language_2008,
	title = {Language as shaped by the brain},
	volume = {31},
	issn = {1469-1825, 0140-525X},
	url = {https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/language-as-shaped-by-the-brain/EA4ABB50915417A1A10569707F574F5E},
	doi = {10.1017/S0140525X08004998},
	abstract = {AbstractIt is widely assumed that human learning and the structure of human languages are intimately related. This relationship is frequently suggested to derive from a language-specific biological endowment, which encodes universal, but communicatively arbitrary, principles of language structure (a Universal Grammar or UG). How might such a UG have evolved? We argue that UG could not have arisen either by biological adaptation or non-adaptationist genetic processes, resulting in a logical problem of language evolution. Specifically, as the processes of language change are much more rapid than processes of genetic change, language constitutes a “moving target” both over time and across different human populations, and, hence, cannot provide a stable environment to which language genes could have adapted. We conclude that a biologically determined UG is not evolutionarily viable. Instead, the original motivation for UG – the mesh between learners and languages – arises because language has been shaped to fit the human brain, rather than vice versa. Following Darwin, we view language itself as a complex and interdependent “organism,” which evolves under selectional pressures from human learning and processing mechanisms. That is, languages themselves are shaped by severe selectional pressure from each generation of language users and learners. This suggests that apparently arbitrary aspects of linguistic structure may result from general learning and processing biases deriving from the structure of thought processes, perceptuo-motor factors, cognitive limitations, and pragmatics.},
	number = {5},
	urldate = {2017-11-27},
	journal = {Behavioral and Brain Sciences},
	author = {Christiansen, Morten H. and Chater, Nick},
	month = oct,
	year = {2008},
	keywords = {readme, common, universal grammar},
	pages = {489--509},
}

@article{hasson_hierarchical_2015,
	title = {Hierarchical process memory: memory as an integral component of information processing},
	volume = {19},
	issn = {1364-6613},
	shorttitle = {Hierarchical process memory},
	url = {http://www.sciencedirect.com/science/article/pii/S1364661315000923},
	doi = {10.1016/j.tics.2015.04.006},
	abstract = {Models of working memory (WM) commonly focus on how information is encoded into and retrieved from storage at specific moments. However, in the majority of real-life processes, past information is used continuously to process incoming information across multiple timescales. Considering single-unit, electrocorticography, and functional imaging data, we argue that (i) virtually all cortical circuits can accumulate information over time, and (ii) the timescales of accumulation vary hierarchically, from early sensory areas with short processing timescales (10s to 100s of milliseconds) to higher-order areas with long processing timescales (many seconds to minutes). In this hierarchical systems perspective, memory is not restricted to a few localized stores, but is intrinsic to information processing that unfolds throughout the brain on multiple timescales.},
	number = {6},
	urldate = {2017-11-22},
	journal = {Trends in Cognitive Sciences},
	author = {Hasson, Uri and Chen, Janice and Honey, Christopher J.},
	month = jun,
	year = {2015},
	keywords = {project.lstmMEG, read},
	pages = {304--313},
	file = {Hasson et al. - 2015 - Hierarchical process memory memory as an integral.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Hasson et al. - 2015 - Hierarchical process memory memory as an integral.pdf:application/pdf},
}

@article{koskinen_uncovering_2014,
	title = {Uncovering cortical {MEG} responses to listened audiobook stories},
	volume = {100},
	issn = {1053-8119},
	url = {http://www.sciencedirect.com/science/article/pii/S1053811914004959},
	doi = {10.1016/j.neuroimage.2014.06.018},
	abstract = {Naturalistic stimuli, such as normal speech and narratives, are opening up intriguing prospects in neuroscience, especially when merging neuroimaging with machine learning methodology. Here we propose a task-optimized spatial filtering strategy for uncovering individual magnetoencephalographic (MEG) responses to audiobook stories. Ten subjects listened to 1-h-long recording once, as well as to 48 repetitions of a 1-min-long speech passage. Employing response replicability as statistical validity and utilizing unsupervised learning methods, we trained spatial filters that were able to generalize over datasets of an individual. For this blind-signal-separation (BSS) task, we derived a version of multi-set similarity-constrained canonical correlation analysis (SimCCA) that theoretically provides maximal signal-to-noise ratio (SNR) in this setting. Irrespective of significant noise in unaveraged MEG traces, the method successfully uncovered feasible time courses up to {\textasciitilde}120Hz, with the most prominent signals below 20Hz. Individual trial-to-trial correlations of such time courses reached the level of 0.55 (median 0.33 in the group) at {\textasciitilde}0.5Hz, with considerable variation between subjects. By this filtering, the SNR increased up to 20 times. In comparison, independent component analysis (ICA) or principal component analysis (PCA) did not improve SNR notably. The validity of the extracted brain signals was further assessed by inspecting their associations with the stimulus, as well as by mapping the contributing cortical signal sources. The results indicate that the proposed methodology effectively reduces noise in MEG recordings to that extent that brain responses can be seen to nonrecurring audiobook stories. The study paves the way for applications aiming at accurately modeling the stimulus–response-relationship by tackling the response variability, as well as for real-time monitoring of brain signals of individuals in naturalistic experimental conditions.},
	number = {Supplement C},
	urldate = {2017-11-22},
	journal = {NeuroImage},
	author = {Koskinen, M. and Seppä, M.},
	month = oct,
	year = {2014},
	keywords = {MEG, project.lstmMEG, readme},
	pages = {263--270},
}

@article{nelson_neurophysiological_2017,
	title = {Neurophysiological dynamics of phrase-structure building during sentence processing},
	volume = {114},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/content/114/18/E3669},
	doi = {10.1073/pnas.1701590114},
	abstract = {Although sentences unfold sequentially, one word at a time, most linguistic theories propose that their underlying syntactic structure involves a tree of nested phrases rather than a linear sequence of words. Whether and how the brain builds such structures, however, remains largely unknown. Here, we used human intracranial recordings and visual word-by-word presentation of sentences and word lists to investigate how left-hemispheric brain activity varies during the formation of phrase structures. In a broad set of language-related areas, comprising multiple superior temporal and inferior frontal sites, high-gamma power increased with each successive word in a sentence but decreased suddenly whenever words could be merged into a phrase. Regression analyses showed that each additional word or multiword phrase contributed a similar amount of additional brain activity, providing evidence for a merge operation that applies equally to linguistic objects of arbitrary complexity. More superficial models of language, based solely on sequential transition probability over lexical and syntactic categories, only captured activity in the posterior middle temporal gyrus. Formal model comparison indicated that the model of multiword phrase construction provided a better fit than probability-based models at most sites in superior temporal and inferior frontal cortices. Activity in those regions was consistent with a neural implementation of a bottom-up or left-corner parser of the incoming language stream. Our results provide initial intracranial evidence for the neurophysiological reality of the merge operation postulated by linguists and suggest that the brain compresses syntactically well-formed sequences of words into a hierarchy of nested phrases.},
	language = {en},
	number = {18},
	urldate = {2017-11-22},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Nelson, Matthew J. and Karoui, Imen El and Giber, Kristof and Yang, Xiaofang and Cohen, Laurent and Koopman, Hilda and Cash, Sydney S. and Naccache, Lionel and Hale, John T. and Pallier, Christophe and Dehaene, Stanislas},
	month = may,
	year = {2017},
	pmid = {28416691},
	keywords = {syntax, project.lstmMEG, project.streams, fixme, read, entropy, surprisal, model-based, n-gram, ECoG},
	pages = {E3669--E3678},
	file = {Nelson et al. - 2017 - Neurophysiological dynamics of phrase-structure bu.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Nelson et al. - 2017 - Neurophysiological dynamics of phrase-structure bu.pdf:application/pdf},
}

@article{seeliger_convolutional_2017,
	title = {Convolutional neural network-based encoding and decoding of visual object recognition in space and time},
	issn = {1053-8119},
	url = {http://www.sciencedirect.com/science/article/pii/S1053811917305864},
	doi = {10.1016/j.neuroimage.2017.07.018},
	abstract = {Representations learned by deep convolutional neural networks (CNNs) for object recognition are a widely investigated model of the processing hierarchy in the human visual system. Using functional magnetic resonance imaging, CNN representations of visual stimuli have previously been shown to correspond to processing stages in the ventral and dorsal streams of the visual system. Whether this correspondence between models and brain signals also holds for activity acquired at high temporal resolution has been explored less exhaustively. Here, we addressed this question by combining CNN-based encoding models with magnetoencephalography (MEG). Human participants passively viewed 1,000 images of objects while MEG signals were acquired. We modelled their high temporal resolution source-reconstructed cortical activity with CNNs, and observed a feed-forward sweep across the visual hierarchy between 75 and 200 ms after stimulus onset. This spatiotemporal cascade was captured by the network layer representations, where the increasingly abstract stimulus representation in the hierarchical network model was reflected in different parts of the visual cortex, following the visual ventral stream. We further validated the accuracy of our encoding model by decoding stimulus identity in a left-out validation set of viewed objects, achieving state-of-the-art decoding accuracy.},
	urldate = {2017-12-12},
	journal = {NeuroImage},
	author = {Seeliger, K. and Fritsche, M. and Güçlü, U. and Schoenmakers, S. and Schoffelen, J. -M. and Bosch, S. E. and van Gerven, M. A. J.},
	month = jul,
	year = {2017},
	keywords = {MEG, project.lstmMEG, readme, CNN, vision, ANN, model-based, decoding, encoding},
	file = {Seeliger et al. - 2017 - Convolutional neural network-based encoding and de.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Seeliger et al. - 2017 - Convolutional neural network-based encoding and de.pdf:application/pdf},
}

@article{hari_brain_2015,
	title = {The brain timewise: how timing shapes and supports brain function},
	volume = {370},
	copyright = {. © 2015 The Authors. Published by the Royal Society under the terms of the Creative Commons Attribution License http://creativecommons.org/licenses/by/4.0/, which permits unrestricted use, provided the original author and source are credited.},
	issn = {0962-8436, 1471-2970},
	shorttitle = {The brain timewise},
	url = {http://rstb.royalsocietypublishing.org/content/370/1668/20140170},
	doi = {10.1098/rstb.2014.0170},
	abstract = {We discuss the importance of timing in brain function: how temporal dynamics of the world has left its traces in the brain during evolution and how we can monitor the dynamics of the human brain with non-invasive measurements. Accurate timing is important for the interplay of neurons, neuronal circuitries, brain areas and human individuals. In the human brain, multiple temporal integration windows are hierarchically organized, with temporal scales ranging from microseconds to tens and hundreds of milliseconds for perceptual, motor and cognitive functions, and up to minutes, hours and even months for hormonal and mood changes. Accurate timing is impaired in several brain diseases. From the current repertoire of non-invasive brain imaging methods, only magnetoencephalography (MEG) and scalp electroencephalography (EEG) provide millisecond time-resolution; our focus in this paper is on MEG. Since the introduction of high-density whole-scalp MEG/EEG coverage in the 1990s, the instrumentation has not changed drastically; yet, novel data analyses are advancing the field rapidly by shifting the focus from the mere pinpointing of activity hotspots to seeking stimulus- or task-specific information and to characterizing functional networks. During the next decades, we can expect increased spatial resolution and accuracy of the time-resolved brain imaging and better understanding of brain function, especially its temporal constraints, with the development of novel instrumentation and finer-grained, physiologically inspired generative models of local and network activity. Merging both spatial and temporal information with increasing accuracy and carrying out recordings in naturalistic conditions, including social interaction, will bring much new information about human brain function.},
	language = {en},
	number = {1668},
	urldate = {2017-11-22},
	journal = {Phil. Trans. R. Soc. B},
	author = {Hari, Riitta and Parkkonen, Lauri},
	month = may,
	year = {2015},
	pmid = {25823867},
	keywords = {project.lstmMEG, readme, TRW, time, project},
	pages = {20140170},
}

@article{cheveigne_decoding_2017,
	title = {Decoding the auditory brain with canonical component analysis},
	copyright = {© 2017, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/early/2017/11/10/217281},
	doi = {10.1101/217281},
	abstract = {The relation between a stimulus and the brain response that it evokes can shed light on perceptual processes within the brain. It can also be harnessed to control external devices for Brain Computer Interface (BCI) applications. While the classic event-related potential (ERP) is appropriate for isolated stimuli, more sophisticated "decoding" strategies are needed to address ongoing stimuli such as speech, music or environmental sounds. Here we describe an approach based on Canonical Correlation Analysis (CCA) that finds the optimal transform to apply to both stimulus and response to reveal correlations between the two. Compared to prior methods based on forward or backward models for stimulus-response mapping, CCA finds significantly higher correlation scores, thus providing increased sensitivity to relatively small effects, and supports classifier schemes that yield higher classification scores. CCA strips the brain response of variance unrelated to the stimulus, and the stimulus representation of variance that does not affect the response, and thus improves our observation of the relation between stimulus and response.},
	language = {en},
	urldate = {2017-11-20},
	journal = {bioRxiv},
	author = {Cheveigne, Alain de and Wong, Daniel and Liberto, Giovanni Di and Hjortkjaer, Jens and Slaney, Malcolm and Lalor, Edmund},
	month = nov,
	year = {2017},
	keywords = {project.lstmMEG, readme},
	pages = {217281},
}

@article{lake_still_2017,
	title = {Still not systematic after all these years: {On} the compositional skills of sequence-to-sequence recurrent networks},
	shorttitle = {Still not systematic after all these years},
	url = {http://arxiv.org/abs/1711.00350},
	abstract = {Humans can understand and produce new utterances effortlessly, thanks to their systematic compositional skills. Once a person learns the meaning of a new verb "dax," he or she can immediately understand the meaning of "dax twice" or "sing and dax." In this paper, we introduce the SCAN domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences. We then test the zero-shot generalization capabilities of a variety of recurrent neural networks (RNNs) trained on SCAN with sequence-to-sequence methods. We find that RNNs can generalize well when the differences between training and test commands are small, so that they can apply "mix-and-match" strategies to solve the task. However, when generalization requires systematic compositional skills (as in the "dax" example above), RNNs fail spectacularly. We conclude with a proof-of-concept experiment in neural machine translation, supporting the conjecture that lack of systematicity is an important factor explaining why neural networks need very large training sets.},
	urldate = {2017-11-20},
	journal = {arXiv:1711.00350 [cs]},
	author = {Lake, Brenden M. and Baroni, Marco},
	month = oct,
	year = {2017},
	note = {arXiv: 1711.00350},
	keywords = {project.lstmMEG, readme},
}

@article{brody_basic_2003,
	title = {Basic mechanisms for graded persistent activity: discrete attractors, continuous attractors, and dynamic representations},
	volume = {13},
	issn = {0959-4388},
	shorttitle = {Basic mechanisms for graded persistent activity},
	url = {http://www.sciencedirect.com/science/article/pii/S0959438803000503},
	doi = {10.1016/S0959-4388(03)00050-3},
	abstract = {Persistent neural activity is observed in many systems, and is thought to be a neural substrate for holding memories over time delays of a few seconds. Recent work has addressed two issues. First, how can networks of neurons robustly hold such an active memory? Computer systems obtain significant robustness to noise by approximating analogue quantities with discrete digital representations. In a similar manner, theoretical models of persistent activity in spiking neurons have shown that the most robust and stable way to store the short-term memory of a continuous parameter is to approximate it with a discrete representation. This general idea applies very broadly to mechanisms that range from biochemical networks to single cells and to large circuits of neurons. Second, why is it commonly observed that persistent activity in the cortex can be strongly time-varying? This observation is almost ubiquitous, and therefore must be taken into account in our models and our understanding of how short-term memories are held in the cortex.},
	number = {2},
	urldate = {2017-11-20},
	journal = {Current Opinion in Neurobiology},
	author = {Brody, Carlos D and Romo, Ranulfo and Kepecs, Adam},
	month = apr,
	year = {2003},
	keywords = {project.lstmMEG, readme},
	pages = {204--211},
}

@book{palahniuk_fight_2015,
	title = {Fight club: a {Novel}.},
	isbn = {978-0-393-06639-5},
	shorttitle = {Fight club},
	url = {http://NCDigital.lib.overdrive.com/ContentDetails.htm?ID=8E144041-18C8-4726-A503-A9B2BD8FCF33},
	abstract = {The first rule about fight club is you don't talk about fight club. In his debut novel, Chuck Palahniuk showed himself to be his generation's most visionary satirist. Fight Club's estranged narrator leaves his lackluster job when he comes under the thrall of Tyler Durden, an enigmatic young man who holds secret boxing matches in the basement of bars. There two men fight "as long as they have to." A gloriously original work that exposes what is at the core of our modern world.},
	language = {English},
	urldate = {2017-12-09},
	author = {Palahniuk, Chuck},
	year = {2015},
	note = {OCLC: 928759800},
	keywords = {read, fiction},
}

@article{rousselet_beyond_2017,
	title = {Beyond differences in means: robust graphical methods to compare two groups in neuroscience},
	volume = {46},
	issn = {1460-9568},
	shorttitle = {Beyond differences in means},
	url = {http://onlinelibrary.wiley.com/doi/10.1111/ejn.13610/abstract},
	doi = {10.1111/ejn.13610},
	abstract = {If many changes are necessary to improve the quality of neuroscience research, one relatively simple step could have great pay-offs: to promote the adoption of detailed graphical methods, combined with robust inferential statistics. Here, we illustrate how such methods can lead to a much more detailed understanding of group differences than bar graphs and t-tests on means. To complement the neuroscientist's toolbox, we present two powerful tools that can help us understand how groups of observations differ: the shift function and the difference asymmetry function. These tools can be combined with detailed visualisations to provide complementary perspectives about the data. We provide implementations in R and MATLAB of the graphical tools, and all the examples in the article can be reproduced using R scripts.},
	language = {en},
	number = {2},
	urldate = {2017-12-08},
	journal = {European Journal of Neuroscience},
	author = {Rousselet, Guillaume A. and Pernet, Cyril R. and Wilcox, Rand R.},
	month = jul,
	year = {2017},
	keywords = {statistics, read, dataviz, robust statistics},
	pages = {1738--1748},
}

@article{wang_brain_2016,
	series = {Neurobiology of cognitive behavior},
	title = {Brain structure and dynamics across scales: in search of rules},
	volume = {37},
	issn = {0959-4388},
	shorttitle = {Brain structure and dynamics across scales},
	url = {http://www.sciencedirect.com/science/article/pii/S0959438815001889},
	doi = {10.1016/j.conb.2015.12.010},
	abstract = {Louis Henry Sullivan, the father of skyscrapers, famously stated ‘Form ever follows function’. In this short review, we will focus on the relationship between form (structure) and function (dynamics) in the brain. We summarize recent advances on the quantification of directed- and weighted-mesoscopic connectivity of mammalian cortex, the exponential distance rule for mesoscopic and microscopic circuit wiring, a spatially embedded random model of inter-areal cortical networks, and a large-scale dynamical circuit model of money's cortex that gives rise to a hierarchy of timescales. These findings demonstrate that inter-areal cortical networks are dense (hence such concepts as ‘small-world’ need to be refined when applied to the brain), spatially dependent (therefore purely topological approach of graph theory has limited applicability) and heterogeneous (consequently cortical areas cannot be treated as identical ‘nodes’).},
	number = {Supplement C},
	urldate = {2017-12-05},
	journal = {Current Opinion in Neurobiology},
	author = {Wang, Xiao-Jing and Kennedy, Henry},
	month = apr,
	year = {2016},
	keywords = {project.lstmMEG, readme, TRW, spikes, time, primate},
	pages = {92--98},
}

@incollection{piccinini_computation_2017,
	edition = {Summer 2017},
	title = {Computation in physical systems},
	url = {https://plato.stanford.edu/archives/sum2017/entries/computation-physicalsystems/},
	booktitle = {The {Stanford} {Encyclopedia} of {Philosophy}},
	publisher = {Metaphysics Research Lab, Stanford University},
	author = {Piccinini, Gualtiero},
	editor = {Zalta, Edward N.},
	year = {2017},
	keywords = {readme, explanation, philosophy of neuroscience, philosophy of science, computation, philosophy of mind, project.6, philosophy, physical computation},
}

@article{fresco_explanatory_2012,
	title = {The explanatory role of computation in cognitive science},
	volume = {22},
	issn = {0924-6495, 1572-8641},
	doi = {10.1007/s11023-012-9286-y},
	language = {en},
	number = {4},
	urldate = {2018-04-08},
	journal = {Minds and Machines},
	author = {Fresco, Nir},
	month = nov,
	year = {2012},
	keywords = {readme, explanation, computation, philosophy of mind, cognitive science, project.6, computational explanation},
	pages = {353--380},
}

@article{godfrey-smith_triviality_2009,
	title = {Triviality arguments against functionalism},
	volume = {145},
	issn = {0031-8116, 1573-0883},
	url = {http://link.springer.com/10.1007/s11098-008-9231-3},
	doi = {10.1007/s11098-008-9231-3},
	language = {en},
	number = {2},
	urldate = {2018-04-08},
	journal = {Philosophical Studies},
	author = {Godfrey-Smith, Peter},
	month = aug,
	year = {2009},
	keywords = {readme, explanation, philosophy of science, computation, computational implementation, project.6, computational explanation, philsophy of mind, philosophy},
	pages = {273--295},
}

@book{craver_levels_2014,
	title = {Levels},
	isbn = {978-3-95857-049-8},
	url = {https://open-mind.net/papers/levels/getAbstract},
	abstract = {The levels metaphor is commonly used to describe science, its theories, and the world. Yet the metaphor means different things in different contexts, inviting equivocation. These distinct applications of the metaphor can be distinguished by the relata they relate, the relation between levels that they assert, and the rule by which they locate items at a level. I discuss these many applications of the levels metaphor with an eye to developing a descriptively adequate account of one particular application: levels of mechanisms. I argue that this application of the metaphor is central to the explanatory practices of the special sciences and defensible as a metaphysical picture of how phenomena studied in the special sciences are constituted.},
	language = {en},
	urldate = {2018-02-23},
	publisher = {Open MIND. Frankfurt am Main: MIND Group},
	author = {Craver, Carl F.},
	month = jan,
	year = {2014},
	doi = {10.15502/9783958570498},
	keywords = {explanation, philosophy of neuroscience, read, levels},
	file = {Craver - 2014 - Levels.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Craver - 2014 - Levels.pdf:application/pdf},
}

@article{grice_meaning_1957,
	title = {Meaning},
	volume = {66},
	issn = {00318108},
	url = {http://www.jstor.org/stable/2182440?origin=crossref},
	doi = {10.2307/2182440},
	number = {3},
	urldate = {2018-04-08},
	journal = {The Philosophical Review},
	author = {Grice, H. P.},
	month = jul,
	year = {1957},
	keywords = {readme, explanation, philosophy of science, computation, philosophy of mind, semantics, philosophy, meaning},
	pages = {377},
}

@book{ashby_design_1978,
	address = {London: Chapman \& Hall},
	edition = {Reprinted},
	series = {Science paperbacks},
	title = {Design for a brain: the origin of adaptive behaviour},
	isbn = {978-0-412-20090-8},
	shorttitle = {Design for a brain},
	number = {10},
	author = {Ashby, William Ross},
	year = {1978},
	note = {OCLC: 252250636},
	keywords = {readme, core, adaptive, cybernetics},
}

@article{bach_knowing_2012,
	title = {Knowing how much you don't know: a neural organization of uncertainty estimates},
	volume = {13},
	issn = {1471-003X, 1471-0048},
	shorttitle = {Knowing how much you don't know},
	doi = {10.1038/nrn3289},
	language = {en},
	number = {8},
	urldate = {2018-04-06},
	journal = {Nature Reviews Neuroscience},
	author = {Bach, Dominik R. and Dolan, Raymond J.},
	month = aug,
	year = {2012},
	keywords = {project.streams, readme, information theory, uncertainty, decision-making},
	pages = {572--586},
}

@article{bar_proactive_2009,
	title = {The proactive brain: memory for predictions},
	volume = {364},
	issn = {0962-8436, 1471-2970},
	shorttitle = {The proactive brain},
	doi = {10.1098/rstb.2008.0310},
	language = {en},
	number = {1521},
	urldate = {2018-04-06},
	journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	author = {Bar, M.},
	month = may,
	year = {2009},
	keywords = {project.streams, memory, perception, predictive coding},
	pages = {1235--1243},
}

@article{pouget_probabilistic_2013,
	title = {Probabilistic brains: knowns and unknowns},
	volume = {16},
	issn = {1097-6256, 1546-1726},
	shorttitle = {Probabilistic brains},
	doi = {10.1038/nn.3495},
	language = {en},
	number = {9},
	urldate = {2018-04-06},
	journal = {Nature Neuroscience},
	author = {Pouget, Alexandre and Beck, Jeffrey M and Ma, Wei Ji and Latham, Peter E},
	month = sep,
	year = {2013},
	keywords = {project.streams, read, probability theory, perception, decision-making, Bayesian brain},
	pages = {1170--1178},
}

@article{den_ouden_how_2012,
	title = {How prediction errors shape perception, attention, and motivation},
	volume = {3},
	issn = {1664-1078},
	doi = {10.3389/fpsyg.2012.00548},
	urldate = {2018-04-06},
	journal = {Frontiers in Psychology},
	author = {den Ouden, Hanneke E. M. and Kok, Peter and de Lange, Floris P.},
	year = {2012},
	keywords = {attention, project.streams, perception, predictive coding, cognitive control, motivation},
}

@article{friston_theory_2005,
	title = {A theory of cortical responses},
	volume = {360},
	issn = {0962-8436, 1471-2970},
	doi = {10.1098/rstb.2005.1622},
	language = {en},
	number = {1456},
	urldate = {2018-04-06},
	journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	author = {Friston, K.},
	month = apr,
	year = {2005},
	keywords = {project.streams, readme, information theory, predictive coding, Bayesian brain},
	pages = {815--836},
}

@article{bar_predictions:_2009,
	title = {Predictions: a universal principle in the operation of the human brain},
	volume = {364},
	issn = {0962-8436, 1471-2970},
	shorttitle = {Predictions},
	doi = {10.1098/rstb.2008.0321},
	language = {en},
	number = {1521},
	urldate = {2018-04-06},
	journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	author = {Bar, M.},
	month = may,
	year = {2009},
	keywords = {project.streams, memory, predictive coding},
	pages = {1181--1182},
}

@article{huettig_is_2016,
	title = {Is prediction necessary to understand language? {Probably} not},
	volume = {31},
	issn = {2327-3798, 2327-3801},
	shorttitle = {Is prediction necessary to understand language?},
	doi = {10.1080/23273798.2015.1072223},
	language = {en},
	number = {1},
	urldate = {2018-04-06},
	journal = {Language, Cognition and Neuroscience},
	author = {Huettig, Falk and Mani, Nivedita},
	month = jan,
	year = {2016},
	keywords = {project.streams, prediction, language comprehension, cognitive science, predictive coding},
	pages = {19--31},
}

@article{levy_expectation-based_2008,
	title = {Expectation-based syntactic comprehension},
	volume = {106},
	issn = {00100277},
	doi = {10.1016/j.cognition.2007.05.006},
	language = {en},
	number = {3},
	urldate = {2018-04-06},
	journal = {Cognition},
	author = {Levy, Roger},
	month = mar,
	year = {2008},
	keywords = {syntax, project.streams, entropy, sentence comprehension, cognitive science, suprisal},
	pages = {1126--1177},
}

@article{bastos_canonical_2012,
	title = {Canonical microcircuits for predictive coding},
	volume = {76},
	issn = {0896-6273},
	doi = {10.1016/j.neuron.2012.10.038},
	abstract = {This Perspective considers the influential notion of a canonical (cortical) microcircuit in light of recent theories about neuronal processing. Specifically, we conciliate quantitative studies of microcircuitry and the functional logic of neuronal computations. We revisit the established idea that message passing among hierarchical cortical areas implements a form of Bayesian inference—paying careful attention to the implications for intrinsic connections among neuronal populations. By deriving canonical forms for these computations, one can associate specific neuronal populations with specific computational roles. This analysis discloses a remarkable correspondence between the microcircuitry of the cortical column and the connectivity implied by predictive coding. Furthermore, it provides some intuitive insights into the functional asymmetries between feedforward and feedback connections and the characteristic frequencies over which they operate.},
	number = {4},
	urldate = {2018-04-06},
	journal = {Neuron},
	author = {Bastos, Andre M. and Usrey, W. Martin and Adams, Rick A. and Mangun, George R. and Fries, Pascal and Friston, Karl J.},
	month = nov,
	year = {2012},
	keywords = {project.streams, readme, predictive coding, canonical microcircuit},
	pages = {695--711},
	file = {Bastos et al. - 2012 - Canonical microcircuits for predictive coding.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Bastos et al. - 2012 - Canonical microcircuits for predictive coding.pdf:application/pdf},
}

@article{kuperberg_what_2016,
	title = {What do we mean by prediction in language comprehension?},
	volume = {31},
	issn = {2327-3798, 2327-3801},
	doi = {10.1080/23273798.2015.1102299},
	language = {en},
	number = {1},
	urldate = {2018-04-05},
	journal = {Language, Cognition and Neuroscience},
	author = {Kuperberg, Gina R. and Jaeger, T. Florian},
	month = jan,
	year = {2016},
	keywords = {project.streams, prediction, language comprehension, predictive coding, bayesian models, generative models},
	pages = {32--59},
}

@article{clark_whatever_2013,
	title = {Whatever next? {Predictive} brains, situated agents, and the future of cognitive science},
	volume = {36},
	issn = {0140-525X, 1469-1825},
	shorttitle = {Whatever next?},
	doi = {10.1017/S0140525X12000477},
	language = {en},
	number = {03},
	urldate = {2018-04-05},
	journal = {Behavioral and Brain Sciences},
	author = {Clark, Andy},
	month = jun,
	year = {2013},
	keywords = {project.streams, embodied cognition, common, predictive coding},
	pages = {181--204},
}

@article{martin_mechanism_2017,
	title = {A mechanism for the cortical computation of hierarchical linguistic structure},
	volume = {15},
	issn = {1545-7885},
	url = {http://journals.plos.org/plosbiology/article?id=10.1371/journal.pbio.2000663},
	doi = {10.1371/journal.pbio.2000663},
	abstract = {Biological systems often detect species-specific signals in the environment. In humans, speech and language are species-specific signals of fundamental biological importance. To detect the linguistic signal, human brains must form hierarchical representations from a sequence of perceptual inputs distributed in time. What mechanism underlies this ability? One hypothesis is that the brain repurposed an available neurobiological mechanism when hierarchical linguistic representation became an efficient solution to a computational problem posed to the organism. Under such an account, a single mechanism must have the capacity to perform multiple, functionally related computations, e.g., detect the linguistic signal and perform other cognitive functions, while, ideally, oscillating like the human brain. We show that a computational model of analogy, built for an entirely different purpose—learning relational reasoning—processes sentences, represents their meaning, and, crucially, exhibits oscillatory activation patterns resembling cortical signals elicited by the same stimuli. Such redundancy in the cortical and machine signals is indicative of formal and mechanistic alignment between representational structure building and “cortical” oscillations. By inductive inference, this synergy suggests that the cortical signal reflects structure generation, just as the machine signal does. A single mechanism—using time to encode information across a layered network—generates the kind of (de)compositional representational hierarchy that is crucial for human language and offers a mechanistic linking hypothesis between linguistic representation and cortical computation.},
	language = {en},
	number = {3},
	urldate = {2018-04-04},
	journal = {PLOS Biology},
	author = {Martin, Andrea E. and Doumas, Leonidas A. A.},
	month = feb,
	year = {2017},
	keywords = {syntax, hierarchy, readme, explanation, computation, neural networks, ANN, language, time, oscillations, neural coding},
	pages = {e2000663},
	file = {Martin and Doumas - 2017 - A mechanism for the cortical computation of hierar.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Martin and Doumas - 2017 - A mechanism for the cortical computation of hierar.pdf:application/pdf},
}

@article{kaplan_explanation_2011,
	title = {Explanation and description in computational neuroscience},
	volume = {183},
	issn = {0039-7857, 1573-0964},
	doi = {10.1007/s11229-011-9970-0},
	language = {en},
	number = {3},
	urldate = {2018-04-03},
	journal = {Synthese},
	author = {Kaplan, David Michael},
	month = dec,
	year = {2011},
	keywords = {explanation, fixme, philosophy of neuroscience, read, computation, philosophy of mind, computational neuroscience, project.6, computational explanation},
	pages = {339--373},
	file = {Kaplan - 2011 - Explanation and description in computational neuro.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Kaplan - 2011 - Explanation and description in computational neuro.pdf:application/pdf},
}

@article{chalmers_varieties_2012,
	title = {The varieties of computation: a reply},
	volume = {13},
	issn = {1598-2327},
	shorttitle = {The {Varieties} of {Computation}},
	url = {2},
	doi = {10.17791/jcs.2012.13.3.211},
	language = {en},
	number = {3},
	urldate = {2018-04-02},
	journal = {Journal of Cognitive Science},
	author = {Chalmers, David J.},
	month = sep,
	year = {2012},
	keywords = {readme, explanation, philosophy of science, computation, cognitive science, project.6, cognitive modeling},
	pages = {211--248},
	file = {Chalmers - 2012 - The varieties of computation a reply.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Chalmers - 2012 - The varieties of computation a reply.pdf:application/pdf},
}

@article{chalmers_computational_2011,
	title = {A computational foundation for the study of cognition},
	volume = {12},
	issn = {1598-2327},
	doi = {10.17791/jcs.2011.12.4.325},
	language = {en},
	number = {4},
	urldate = {2018-04-02},
	journal = {Journal of Cognitive Science},
	author = {Chalmers, David J.},
	month = dec,
	year = {2011},
	keywords = {readme},
	pages = {325--359},
	file = {Chalmers - 2011 - A computational foundation for the study of cognit.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Chalmers - 2011 - A computational foundation for the study of cognit.pdf:application/pdf},
}

@book{dennett_intentional_1998,
	address = {Cambridge, Mass.},
	edition = {7. printing},
	series = {A {Bradford} book},
	title = {The intentional stance},
	isbn = {978-0-262-54053-7},
	publisher = {MIT Press},
	author = {Dennett, Daniel Clement},
	year = {1998},
	note = {OCLC: 257058868},
	keywords = {evolution, explanation, computation, reading, folk psychology, intentional systems theory, subpersonal psychology},
}

@article{chalmers_implementing_1994,
	title = {On implementing a computation},
	volume = {4},
	issn = {0924-6495, 1572-8641},
	doi = {10.1007/BF00974166},
	language = {en},
	number = {4},
	urldate = {2018-03-31},
	journal = {Minds and Machines},
	author = {Chalmers, David J.},
	month = nov,
	year = {1994},
	keywords = {readme, explanation, philosophy of science, computation, philosophy of mind, computational implementation, philosophy, cognition},
	pages = {391--402},
}

@book{pylyshyn_computation_1989,
	address = {Cambridge, Mass.},
	edition = {5. printing},
	series = {A {Bradford} book},
	title = {Computation and cognition: toward a foundation for cognitive science},
	isbn = {978-0-262-66058-7 978-0-262-16098-8},
	shorttitle = {Computation and cognition},
	language = {eng},
	publisher = {MIT Press},
	author = {Pylyshyn, Zenon W.},
	year = {1989},
	note = {OCLC: 256210813},
	keywords = {readme, explanation, computation, thesis.introduction, cognitive science, project.6, representation},
}

@article{engel_beta-band_2010,
	title = {Beta-band oscillations—signalling the status quo?},
	volume = {20},
	issn = {09594388},
	doi = {10.1016/j.conb.2010.02.015},
	language = {en},
	number = {2},
	urldate = {2018-03-15},
	journal = {Current Opinion in Neurobiology},
	author = {Engel, Andreas K and Fries, Pascal},
	month = apr,
	year = {2010},
	keywords = {cognitive neuroscience, project.streams, read, beta, oscillations},
	pages = {156--165},
}

@article{nieuwland_when_2006,
	title = {When peanuts fall in love: {N400} evidence for the power of discourse},
	volume = {18},
	issn = {0898-929X, 1530-8898},
	shorttitle = {When peanuts fall in love},
	doi = {10.1162/jocn.2006.18.7.1098},
	language = {en},
	number = {7},
	urldate = {2018-03-28},
	journal = {Journal of Cognitive Neuroscience},
	author = {Nieuwland, Mante S. and Van Berkum, Jos J. A.},
	month = jul,
	year = {2006},
	keywords = {ERP, project.streams, sentence comprehension, discourse},
	pages = {1098--1111},
}

@article{cashdollar_role_2016,
	title = {The role of working memory in the probabilistic inference of future sensory events},
	issn = {1047-3211, 1460-2199},
	url = {http://cercor.oxfordjournals.org/cgi/doi/10.1093/cercor/bhw138},
	doi = {10.1093/cercor/bhw138},
	language = {en},
	urldate = {2018-03-27},
	journal = {Cerebral Cortex},
	author = {Cashdollar, Nathan and Ruhnau, Philipp and Weisz, Nathan and Hasson, Uri},
	month = may,
	year = {2016},
	pages = {bhw138},
}

@article{donahue_long-term_2014,
	title = {Long-term recurrent convolutional networks for visual recognition and description},
	url = {http://arxiv.org/abs/1411.4389},
	abstract = {Models based on deep convolutional networks have dominated recent image interpretation tasks; we investigate whether models which are also recurrent, or "temporally deep", are effective for tasks involving sequences, visual and otherwise. We develop a novel recurrent convolutional architecture suitable for large-scale visual learning which is end-to-end trainable, and demonstrate the value of these models on benchmark video recognition tasks, image description and retrieval problems, and video narration challenges. In contrast to current models which assume a fixed spatio-temporal receptive field or simple temporal averaging for sequential processing, recurrent convolutional models are "doubly deep"' in that they can be compositional in spatial and temporal "layers". Such models may have advantages when target concepts are complex and/or training data are limited. Learning long-term dependencies is possible when nonlinearities are incorporated into the network state updates. Long-term RNN models are appealing in that they directly can map variable-length inputs (e.g., video frames) to variable length outputs (e.g., natural language text) and can model complex temporal dynamics; yet they can be optimized with backpropagation. Our recurrent long-term models are directly connected to modern visual convnet models and can be jointly trained to simultaneously learn temporal dynamics and convolutional perceptual representations. Our results show such models have distinct advantages over state-of-the-art models for recognition or generation which are separately defined and/or optimized.},
	urldate = {2017-11-23},
	journal = {arXiv:1411.4389 [cs]},
	author = {Donahue, Jeff and Hendricks, Lisa Anne and Rohrbach, Marcus and Venugopalan, Subhashini and Guadarrama, Sergio and Saenko, Kate and Darrell, Trevor},
	month = nov,
	year = {2014},
	note = {arXiv: 1411.4389},
	keywords = {project.lstmMEG, read},
	file = {arXiv.org Snapshot:/Users/kriarm/Zotero/storage/9LNL9ACE/1411.html:text/html},
}

@article{overath_information_2007,
	title = {An information theoretic characterisation of auditory encoding},
	volume = {5},
	issn = {1545-7885},
	doi = {10.1371/journal.pbio.0050288},
	language = {en},
	number = {11},
	urldate = {2018-03-26},
	journal = {PLoS Biology},
	author = {Overath, Tobias and Cusack, Rhodri and Kumar, Sukhbinder and von Kriegstein, Katharina and Warren, Jason D and Grube, Manon and Carlyon, Robert P and Griffiths, Timothy D},
	editor = {Zatorre, Robert},
	month = oct,
	year = {2007},
	keywords = {fMRI, project.streams, readme, entropy, audio envelope, auditory perception},
	pages = {e288},
}

@article{strange_information_2005,
	title = {Information theory, novelty and hippocampal responses: unpredicted or unpredictable?},
	volume = {18},
	issn = {08936080},
	shorttitle = {Information theory, novelty and hippocampal responses},
	doi = {10.1016/j.neunet.2004.12.004},
	language = {en},
	number = {3},
	urldate = {2018-03-26},
	journal = {Neural Networks},
	author = {Strange, Bryan A. and Duggins, Andrew and Penny, William and Dolan, Raymond J. and Friston, Karl J.},
	month = apr,
	year = {2005},
	keywords = {project.streams, readme, entropy, information theory, probability theory, uncertainty, hippocampus},
	pages = {225--230},
}

@article{fruchter_lexical_2015,
	title = {Lexical preactivation in basic linguistic phrases},
	volume = {27},
	issn = {0898-929X, 1530-8898},
	doi = {10.1162/jocn_a_00822},
	language = {en},
	number = {10},
	urldate = {2018-03-26},
	journal = {Journal of Cognitive Neuroscience},
	author = {Fruchter, Joseph and Linzen, Tal and Westerlund, Masha and Marantz, Alec},
	month = oct,
	year = {2015},
	keywords = {MEG, project.streams, predictability, spoken word comprehension, transition probability},
	pages = {1912--1935},
}

@article{nastase_uncertainty_2014,
	title = {Uncertainty in visual and auditory series is coded by modality-general and modality-specific neural systems: {Uncertainty} in {Visual} and {Auditory} {Series}},
	volume = {35},
	issn = {10659471},
	shorttitle = {Uncertainty in visual and auditory series},
	doi = {10.1002/hbm.22238},
	language = {en},
	number = {4},
	urldate = {2018-03-26},
	journal = {Human Brain Mapping},
	author = {Nastase, Samuel and Iacovella, Vittorio and Hasson, Uri},
	month = apr,
	year = {2014},
	keywords = {fMRI, project.streams, entropy, information theory, perception, uncertainty, auditory perception, statistical learning},
	pages = {1111--1128},
}

@book{holt_why_2012,
	address = {New York},
	title = {Why does the world exist? an existential detective story},
	isbn = {978-0-87140-359-9 978-0-87140-409-1},
	shorttitle = {Why does the world exist?},
	language = {eng},
	publisher = {Liveright},
	author = {Holt, Jim},
	year = {2012},
	note = {OCLC: 809827362},
	keywords = {philosophy of science, read, philosophy, existence},
}

@article{gan_learning_2016,
	title = {Learning generic sentence representations using convolutional neural networks},
	url = {http://arxiv.org/abs/1611.07897},
	abstract = {We propose a new encoder-decoder approach to learn distributed sentence representations that are applicable to multiple purposes. The model is learned by using a convolutional neural network as an encoder to map an input sentence into a continuous vector, and using a long short-term memory recurrent neural network as a decoder. Several tasks are considered, including sentence reconstruction and future sentence prediction. Further, a hierarchical encoder-decoder model is proposed to encode a sentence to predict multiple future sentences. By training our models on a large collection of novels, we obtain a highly generic convolutional sentence encoder that performs well in practice. Experimental results on several benchmark datasets, and across a broad range of applications, demonstrate the superiority of the proposed model over competing methods.},
	urldate = {2018-03-21},
	journal = {arXiv:1611.07897 [cs]},
	author = {Gan, Zhe and Pu, Yunchen and Henao, Ricardo and Li, Chunyuan and He, Xiaodong and Carin, Lawrence},
	month = nov,
	year = {2016},
	note = {arXiv: 1611.07897},
	keywords = {project.lstmMEG, reading},
}

@article{naselaris_cognitive_2018,
	title = {Cognitive computational neuroscience: {A} new conference for an emerging discipline},
	issn = {13646613},
	shorttitle = {Cognitive {Computational} {Neuroscience}},
	doi = {10.1016/j.tics.2018.02.008},
	language = {en},
	urldate = {2018-03-24},
	journal = {Trends in Cognitive Sciences},
	author = {Naselaris, Thomas and Bassett, Danielle S. and Fletcher, Alyson K. and Kording, Konrad and Kriegeskorte, Nikolaus and Nienborg, Hendrikje and Poldrack, Russell A. and Shohamy, Daphna and Kay, Kendrick},
	month = feb,
	year = {2018},
	keywords = {read},
}

@article{shagrir_brain_2018,
	title = {The brain as an input–output model of the world},
	volume = {28},
	issn = {0924-6495, 1572-8641},
	doi = {10.1007/s11023-017-9443-4},
	language = {en},
	number = {1},
	urldate = {2018-03-24},
	journal = {Minds and Machines},
	author = {Shagrir, Oron},
	month = mar,
	year = {2018},
	keywords = {readme, explanation, philosophy of neuroscience, philosophy of science, computation},
	pages = {53--75},
}

@article{piccinini_computation_2018,
	title = {Computation and representation in cognitive neuroscience},
	volume = {28},
	issn = {0924-6495, 1572-8641},
	doi = {10.1007/s11023-018-9461-x},
	language = {en},
	number = {1},
	urldate = {2018-03-24},
	journal = {Minds and Machines},
	author = {Piccinini, Gualtiero},
	month = mar,
	year = {2018},
	keywords = {readme, explanation, philosophy of neuroscience, philosophy of science, computation, project.6},
	pages = {1--6},
}

@article{pereira_toward_2018,
	title = {Toward a universal decoder of linguistic meaning from brain activation},
	volume = {9},
	copyright = {2018 The Author(s)},
	issn = {2041-1723},
	url = {https://www.nature.com/articles/s41467-018-03068-4},
	doi = {10.1038/s41467-018-03068-4},
	abstract = {Previous work decoding linguistic meaning from imaging data has generally been limited to a small number of semantic categories. Here, authors show that a decoder trained on neuroimaging data of single concepts sampling the semantic space can robustly decode meanings of semantically diverse new sentences with topics not encountered during training.},
	language = {en},
	number = {1},
	urldate = {2018-03-16},
	journal = {Nature Communications},
	author = {Pereira, Francisco and Lou, Bin and Pritchett, Brianna and Ritter, Samuel and Gershman, Samuel J. and Kanwisher, Nancy and Botvinick, Matthew and Fedorenko, Evelina},
	month = mar,
	year = {2018},
	keywords = {read, nbl.jc},
	pages = {963},
}

@article{wang_language_2018,
	title = {Language prediction is reflected by coupling between frontal gamma and posterior alpha oscillations},
	volume = {30},
	issn = {0898-929X, 1530-8898},
	url = {https://www.mitpressjournals.org/doi/abs/10.1162/jocn_a_01190},
	doi = {10.1162/jocn_a_01190},
	language = {en},
	number = {3},
	urldate = {2018-03-22},
	journal = {Journal of Cognitive Neuroscience},
	author = {Wang, Lin and Hagoort, Peter and Jensen, Ole},
	month = mar,
	year = {2018},
	pages = {432--447},
}

@article{leonard_dynamic_2015,
	title = {Dynamic encoding of speech sequence probability in human temporal cortex},
	volume = {35},
	issn = {0270-6474, 1529-2401},
	doi = {10.1523/JNEUROSCI.4100-14.2015},
	language = {en},
	number = {18},
	urldate = {2018-03-22},
	journal = {Journal of Neuroscience},
	author = {Leonard, M. K. and Bouchard, K. E. and Tang, C. and Chang, E. F.},
	month = may,
	year = {2015},
	keywords = {project.streams, speech comprehension, ECoG, transition probability, broadband gamma, phonotactics},
	pages = {7203--7214},
}

@article{armeni_probabilistic_2017,
	title = {Probabilistic language models in cognitive neuroscience: {Promises} and pitfalls},
	volume = {83},
	issn = {0149-7634},
	shorttitle = {Probabilistic language models in cognitive neuroscience},
	doi = {10.1016/j.neubiorev.2017.09.001},
	abstract = {Cognitive neuroscientists of language comprehension study how neural computations relate to cognitive computations during comprehension. On the cognitive part of the equation, it is important that the computations and processing complexity are explicitly defined. Probabilistic language models can be used to give a computationally explicit account of language complexity during comprehension. Whereas such models have so far predominantly been evaluated against behavioral data, only recently have the models been used to explain neurobiological signals. Measures obtained from these models emphasize the probabilistic, information-processing view of language understanding and provide a set of tools that can be used for testing neural hypotheses about language comprehension. Here, we provide a cursory review of the theoretical foundations and example neuroimaging studies employing probabilistic language models. We highlight the advantages and potential pitfalls of this approach and indicate avenues for future research.},
	number = {Supplement C},
	urldate = {2017-12-15},
	journal = {Neuroscience \& Biobehavioral Reviews},
	author = {Armeni, Kristijan and Willems, Roel M. and Frank, Stefan L.},
	month = dec,
	year = {2017},
	keywords = {MEG, language model, LM, project.lstmMEG, project.streams, read, entropy, narrative, surprisal},
	pages = {579--588},
	file = {Armeni et al. - 2017 - Probabilistic language models in cognitive neurosc.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Armeni et al. - 2017 - Probabilistic language models in cognitive neurosc.pdf:application/pdf},
}

@article{bottou_machine_2014,
	title = {From machine learning to machine reasoning},
	volume = {94},
	issn = {0885-6125, 1573-0565},
	url = {https://link.springer.com/article/10.1007/s10994-013-5335-x},
	doi = {10.1007/s10994-013-5335-x},
	abstract = {A plausible definition of “reasoning” could be “algebraically manipulating previously acquired knowledge in order to answer a new question”. This definition covers first-order logical inference or probabilistic inference. It also includes much simpler manipulations commonly used to build large learning systems. For instance, we can build an optical character recognition system by first training a character segmenter, an isolated character recognizer, and a language model, using appropriate labelled training sets. Adequately concatenating these modules and fine tuning the resulting system can be viewed as an algebraic operation in a space of models. The resulting model answers a new question, that is, converting the image of a text page into a computer readable text.This observation suggests a conceptual continuity between algebraically rich inference systems, such as logical or probabilistic inference, and simple manipulations, such as the mere concatenation of trainable learning systems. Therefore, instead of trying to bridge the gap between machine learning systems and sophisticated “all-purpose” inference mechanisms, we can instead algebraically enrich the set of manipulations applicable to training systems, and build reasoning capabilities from the ground up.},
	language = {en},
	number = {2},
	urldate = {2018-03-21},
	journal = {Machine Learning},
	author = {Bottou, Léon},
	month = feb,
	year = {2014},
	pages = {133--149},
	file = {Snapshot:/Users/kriarm/Zotero/storage/N3CARLD6/s10994-013-5335-x.html:text/html},
}

@article{bastiaansen_i_2008,
	title = {I see what you mean: {Theta} power increases are involved in the retrieval of lexical semantic information},
	volume = {106},
	issn = {0093934X},
	shorttitle = {I see what you mean},
	doi = {10.1016/j.bandl.2007.10.006},
	language = {en},
	number = {1},
	urldate = {2018-03-21},
	journal = {Brain and Language},
	author = {Bastiaansen, Marcel C.M. and Oostenveld, Robert and Jensen, Ole and Hagoort, Peter},
	month = jul,
	year = {2008},
	keywords = {EEG, project.streams, readme, oscillations},
	pages = {15--28},
}

@article{wang_integration_2012,
	title = {Integration or predictability? {A} further specification of the functional role of gamma oscillations in language comprehension},
	volume = {3},
	issn = {1664-1078},
	shorttitle = {Integration or {Predictability}?},
	doi = {10.3389/fpsyg.2012.00187},
	urldate = {2018-03-19},
	journal = {Frontiers in Psychology},
	author = {Wang, Lin and Zhu, Zude and Bastiaansen, Marcel},
	year = {2012},
	keywords = {project.streams, language, oscillations},
}

@article{pena_brain_2012,
	title = {Brain oscillations during spoken sentence processing},
	volume = {24},
	issn = {0898-929X, 1530-8898},
	doi = {10.1162/jocn_a_00144},
	language = {en},
	number = {5},
	urldate = {2018-03-19},
	journal = {Journal of Cognitive Neuroscience},
	author = {Peña, Marcela and Melloni, Lucia},
	month = may,
	year = {2012},
	keywords = {project.streams, read, speech comprehension, oscillations},
	pages = {1149--1164},
}

@article{rommers_context-dependent_2013,
	title = {Context-dependent semantic processing in the human brain: evidence from idiom comprehension},
	volume = {25},
	issn = {0898-929X, 1530-8898},
	shorttitle = {Context-dependent {Semantic} {Processing} in the {Human} {Brain}},
	doi = {10.1162/jocn_a_00337},
	language = {en},
	number = {5},
	urldate = {2018-03-14},
	journal = {Journal of Cognitive Neuroscience},
	author = {Rommers, Joost and Dijkstra, Ton and Bastiaansen, Marcel},
	month = may,
	year = {2013},
	keywords = {EEG, project.streams, sentence comprehension, language comprehension, oscillations},
	pages = {762--776},
}

@article{guerguiev_towards_2017,
	title = {Towards deep learning with segregated dendrites},
	volume = {6},
	issn = {2050-084X},
	url = {https://elifesciences.org/articles/22901},
	doi = {10.7554/eLife.22901},
	language = {en},
	urldate = {2018-03-19},
	journal = {eLife},
	author = {Guerguiev, Jordan and Lillicrap, Timothy P and Richards, Blake A},
	month = dec,
	year = {2017},
	keywords = {biological plausibility, AI, backprop, ANN, computational neuroscience, statistical learning, reading, biophysical computation, dendrites, problem development, project problem},
}

@article{rumelhart_learning_1986,
	title = {Learning representations by back-propagating errors},
	volume = {323},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/doifinder/10.1038/323533a0},
	doi = {10.1038/323533a0},
	language = {en},
	number = {6088},
	urldate = {2018-03-19},
	journal = {Nature},
	author = {Rumelhart, David E. and Hinton, Geoffrey E. and Williams, Ronald J.},
	month = oct,
	year = {1986},
	keywords = {RNN, AI, backprop, readme, machine learning, ANN},
	pages = {533--536},
	file = {Rumelhart et al. - 1986 - Learning representations by back-propagating error.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Rumelhart et al. - 1986 - Learning representations by back-propagating error.pdf:application/pdf},
}

@article{smaldino_natural_2016,
	title = {The natural selection of bad science},
	volume = {3},
	issn = {2054-5703},
	doi = {10.1098/rsos.160384},
	language = {en},
	number = {9},
	urldate = {2018-03-17},
	journal = {Royal Society Open Science},
	author = {Smaldino, Paul E. and McElreath, Richard},
	month = sep,
	year = {2016},
	keywords = {science practice, science policy, publishing, academic publishing, natural selection},
	pages = {160384},
}

@article{keitel_auditory_2017,
	title = {Auditory cortical delta-entrainment interacts with oscillatory power in multiple fronto-parietal networks},
	volume = {147},
	issn = {10538119},
	doi = {10.1016/j.neuroimage.2016.11.062},
	language = {en},
	urldate = {2018-03-16},
	journal = {NeuroImage},
	author = {Keitel, Anne and Ince, Robin A.A. and Gross, Joachim and Kayser, Christoph},
	month = feb,
	year = {2017},
	keywords = {MEG, project.streams, read, speech comprehension, oscillations},
	pages = {32--42},
}

@article{vignali_oscillatory_2016,
	title = {Oscillatory brain dynamics during sentence reading: a fixation-related spectral perturbation analysis},
	volume = {10},
	issn = {1662-5161},
	shorttitle = {Oscillatory brain dynamics during sentence reading},
	doi = {10.3389/fnhum.2016.00191},
	urldate = {2018-03-16},
	journal = {Frontiers in Human Neuroscience},
	author = {Vignali, Lorenzo and Himmelstoss, Nicole A. and Hawelka, Stefan and Richlan, Fabio and Hutzler, Florian},
	month = apr,
	year = {2016},
	keywords = {MEG, project.streams, sentence comprehension, language, oscillations},
}

@article{mai_delta_2016,
	title = {Delta, theta, beta, and gamma brain oscillations index levels of auditory sentence processing},
	volume = {133},
	issn = {10538119},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S1053811916001737},
	doi = {10.1016/j.neuroimage.2016.02.064},
	language = {en},
	urldate = {2018-03-16},
	journal = {NeuroImage},
	author = {Mai, Guangting and Minett, James W. and Wang, William S.-Y.},
	month = jun,
	year = {2016},
	keywords = {EEG, project.streams, speech comprehension, oscillations},
	pages = {516--528},
}

@article{davidson_inverse_2007,
	title = {An inverse relation between event-related and time–frequency violation responses in sentence processing},
	volume = {1158},
	issn = {00068993},
	doi = {10.1016/j.brainres.2007.04.082},
	language = {en},
	urldate = {2018-03-15},
	journal = {Brain Research},
	author = {Davidson, D.J. and Indefrey, P.},
	month = jul,
	year = {2007},
	keywords = {project.streams, sentence comprehension, beta, language comprehension, oscillations},
	pages = {81--92},
}

@book{jurafsky_speech_2009,
	address = {Upper Saddle River, N.J},
	edition = {2nd ed},
	series = {Prentice {Hall} series in artificial intelligence},
	title = {Speech and language processing: an introduction to natural language processing, computational linguistics, and speech recognition},
	isbn = {978-0-13-187321-6},
	shorttitle = {Speech and language processing},
	publisher = {Pearson Prentice Hall},
	author = {Jurafsky, Dan and Martin, James H.},
	year = {2009},
	note = {OCLC: 213375806},
	keywords = {project.streams, computational linguistics, language modeling, n-gram, speech},
}

@article{frank_erp_2015,
	title = {The {ERP} response to the amount of information conveyed by words in sentences},
	volume = {140},
	issn = {0093-934X},
	doi = {10.1016/j.bandl.2014.10.006},
	abstract = {Reading times on words in a sentence depend on the amount of information the words convey, which can be estimated by probabilistic language models. We investigate whether event-related potentials (ERPs), too, are predicted by information measures. Three types of language models estimated four different information measures on each word of a sample of English sentences. Six different ERP deflections were extracted from the EEG signal of participants reading the same sentences. A comparison between the information measures and ERPs revealed a reliable correlation between N400 amplitude and word surprisal. Language models that make no use of syntactic structure fitted the data better than did a phrase-structure grammar, which did not account for unique variance in N400 amplitude. These findings suggest that different information measures quantify cognitively different processes and that readers do not make use of a sentence’s hierarchical structure for generating expectations about the upcoming word.},
	number = {Supplement C},
	urldate = {2017-12-06},
	journal = {Brain and Language},
	author = {Frank, Stefan L. and Otten, Leun J. and Galli, Giulia and Vigliocco, Gabriella},
	month = jan,
	year = {2015},
	keywords = {RNN, EEG, ERP, project.streams, read, surprisal, ANN, model-based, n-gram},
	pages = {1--11},
	file = {Frank et al. - 2015 - The ERP response to the amount of information conv.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Frank et al. - 2015 - The ERP response to the amount of information conv.pdf:application/pdf},
}

@article{giordano_contributions_2017,
	title = {Contributions of local speech encoding and functional connectivity to audio-visual speech perception},
	volume = {6},
	issn = {2050-084X},
	doi = {10.7554/eLife.24763},
	language = {en},
	urldate = {2018-03-07},
	journal = {eLife},
	author = {Giordano, Bruno L and Ince, Robin A A and Gross, Joachim and Schyns, Philippe G and Panzeri, Stefano and Kayser, Christoph},
	month = jun,
	year = {2017},
	keywords = {MEG, cognitive neuroscience, project.streams, connectivity, entrainment},
}

@article{gross_speech_2013,
	title = {Speech rhythms and multiplexed oscillatory sensory coding in the human brain},
	volume = {11},
	issn = {1545-7885},
	doi = {10.1371/journal.pbio.1001752},
	language = {en},
	number = {12},
	urldate = {2018-02-02},
	journal = {PLoS Biology},
	author = {Gross, Joachim and Hoogenboom, Nienke and Thut, Gregor and Schyns, Philippe and Panzeri, Stefano and Belin, Pascal and Garrod, Simon},
	editor = {Poeppel, David},
	month = dec,
	year = {2013},
	keywords = {MEG, cognitive neuroscience, methods, project.streams, neural oscillations, speech},
	pages = {e1001752},
}

@article{hagoort_nodes_2014,
	title = {Nodes and networks in the neural architecture for language: {Broca}'s region and beyond},
	volume = {28},
	issn = {09594388},
	shorttitle = {Nodes and networks in the neural architecture for language},
	doi = {10.1016/j.conb.2014.07.013},
	language = {en},
	urldate = {2017-12-06},
	journal = {Current Opinion in Neurobiology},
	author = {Hagoort, Peter},
	month = oct,
	year = {2014},
	keywords = {project.streams, read, theory, common, review},
	pages = {136--141},
}

@article{hald_eeg_2006,
	title = {{EEG} theta and gamma responses to semantic violations in online sentence processing},
	volume = {96},
	issn = {0093934X},
	doi = {10.1016/j.bandl.2005.06.007},
	language = {en},
	number = {1},
	urldate = {2018-03-11},
	journal = {Brain and Language},
	author = {Hald, Lea A. and Bastiaansen, Marcel C.M. and Hagoort, Peter},
	month = jan,
	year = {2006},
	keywords = {EEG, project.streams, sentence comprehension, oscillations},
	pages = {90--105},
}

@article{hale_information-theoretical_2016,
	title = {Information-theoretical complexity metrics},
	volume = {10},
	issn = {1749-818X},
	doi = {10.1111/lnc3.12196},
	abstract = {Information-theoretical complexity metrics are auxiliary hypotheses that link theories of parsing and grammar to potentially observable measurements such as reading times and neural signals. This review article considers two such metrics, Surprisal and Entropy Reduction, which are respectively built upon the two most natural notions of ‘information value’ for an observed event (Blachman ). This review sketches their conceptual background and touches on their relationship to other theories in cognitive science. It characterizes them as ‘lenses’ through which theorists ‘see’ the information-processing consequences of linguistic grammars. While these metrics are not themselves parsing algorithms, the review identifies candidate mechanisms that have been proposed for both of them.},
	language = {en},
	number = {9},
	urldate = {2017-12-06},
	journal = {Language and Linguistics Compass},
	author = {Hale, John},
	month = sep,
	year = {2016},
	keywords = {project.streams, read, entropy, surprisal, computational linguistics, n-gram},
	pages = {397--412},
}

@article{bechtel_non-redundant_2015,
	title = {The non-redundant contributions of marr's three levels of analysis for explaining information-processing mechanisms},
	volume = {7},
	issn = {17568757},
	doi = {10.1111/tops.12141},
	language = {en},
	number = {2},
	urldate = {2018-03-10},
	journal = {Topics in Cognitive Science},
	author = {Bechtel, William and Shagrir, Oron},
	month = apr,
	year = {2015},
	keywords = {readme},
	pages = {312--322},
}

@article{barsalou_what_2017,
	series = {Special {Issue}: {Concepts}, {Actions} and {Objects}: {Functional} and {Neural} {Perspectives}},
	title = {What does semantic tiling of the cortex tell us about semantics?},
	volume = {105},
	issn = {0028-3932},
	doi = {10.1016/j.neuropsychologia.2017.04.011},
	abstract = {Recent use of voxel-wise modeling in cognitive neuroscience suggests that semantic maps tile the cortex. Although this impressive research establishes distributed cortical areas active during the conceptual processing that underlies semantics, it tells us little about the nature of this processing. While mapping concepts between Marr's computational and implementation levels to support neural encoding and decoding, this approach ignores Marr's algorithmic level, central for understanding the mechanisms that implement cognition, in general, and conceptual processing, in particular. Following decades of research in cognitive science and neuroscience, what do we know so far about the representation and processing mechanisms that implement conceptual abilities? Most basically, much is known about the mechanisms associated with: (1) feature and frame representations, (2) grounded, abstract, and linguistic representations, (3) knowledge-based inference, (4) concept composition, and (5) conceptual flexibility. Rather than explaining these fundamental representation and processing mechanisms, semantic tiles simply provide a trace of their activity over a relatively short time period within a specific learning context. Establishing the mechanisms that implement conceptual processing in the brain will require more than mapping it to cortical (and sub-cortical) activity, with process models from cognitive science likely to play central roles in specifying the intervening mechanisms. More generally, neuroscience will not achieve its basic goals until it establishes algorithmic-level mechanisms that contribute essential explanations to how the brain works, going beyond simply establishing the brain areas that respond to various task conditions.},
	urldate = {2018-02-16},
	journal = {Neuropsychologia},
	author = {Barsalou, Lawrence W.},
	month = oct,
	year = {2017},
	keywords = {project.vsmMEG, VSM, explanation, read, project.6, semantics},
	pages = {18--38},
	file = {Barsalou - 2017 - What does semantic tiling of the cortex tell us ab.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Barsalou - 2017 - What does semantic tiling of the cortex tell us ab.pdf:application/pdf},
}

@article{anderson_predicting_2017,
	title = {Predicting neural activity patterns associated with sentences using a neurobiologically motivated model of semantic representation},
	volume = {27},
	issn = {1047-3211},
	doi = {10.1093/cercor/bhw240},
	abstract = {We introduce an approach that predicts neural representations of word meanings contained in sentences then superposes these to predict neural representations of new sentences. A neurobiological semantic model based on sensory, motor, social, emotional, and cognitive attributes was used as a foundation to define semantic content. Previous studies have predominantly predicted neural patterns for isolated words, using models that lack neurobiological interpretation. Fourteen participants read 240 sentences describing everyday situations while undergoing fMRI. To connect sentence-level fMRI activation patterns to the word-level semantic model, we devised methods to decompose the fMRI data into individual words. Activation patterns associated with each attribute in the model were then estimated using multiple-regression. This enabled synthesis of activation patterns for trained and new words, which were subsequently averaged to predict new sentences. Region-of-interest analyses revealed that prediction accuracy was highest using voxels in the left temporal and inferior parietal cortex, although a broad range of regions returned statistically significant results, showing that semantic information is widely distributed across the brain. The results show how a neurobiologically motivated semantic model can decompose sentence-level fMRI data into activation features for component words, which can be recombined to predict activation patterns for new sentences.},
	language = {en},
	number = {9},
	urldate = {2018-02-16},
	journal = {Cerebral Cortex},
	author = {Anderson, Andrew James and Binder, Jeffrey R. and Fernandino, Leonardo and Humphries, Colin J. and Conant, Lisa L. and Aguilar, Mario and Wang, Xixi and Doko, Donias and Raizada, Rajeev D. S.},
	month = sep,
	year = {2017},
	keywords = {readme, project.vsmMEG},
	pages = {4379--4395},
}

@article{ahissar_speech_2001,
	title = {Speech comprehension is correlated with temporal response patterns recorded from auditory cortex},
	volume = {98},
	issn = {0027-8424, 1091-6490},
	doi = {10.1073/pnas.201400998},
	language = {en},
	number = {23},
	urldate = {2018-03-07},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Ahissar, E. and Nagarajan, S. and Ahissar, M. and Protopapas, A. and Mahncke, H. and Merzenich, M. M.},
	month = nov,
	year = {2001},
	keywords = {MEG, cognitive neuroscience, project.streams, readme, speech, entrainment},
	pages = {13367--13372},
}

@article{weiss_too_2012,
	title = {“{Too} many betas do not spoil the broth”: the role of beta brain oscillations in language processing},
	volume = {3},
	issn = {1664-1078},
	shorttitle = {“{Too} {Many} betas do not {Spoil} the {Broth}”},
	doi = {10.3389/fpsyg.2012.00201},
	urldate = {2018-03-15},
	journal = {Frontiers in Psychology},
	author = {Weiss, Sabine and Mueller, Horst M.},
	year = {2012},
	keywords = {MEG, project.streams, language, oscillations},
}

@article{ding_cortical_2014,
	title = {Cortical entrainment to continuous speech: functional roles and interpretations},
	volume = {8},
	issn = {1662-5161},
	shorttitle = {Cortical entrainment to continuous speech},
	doi = {10.3389/fnhum.2014.00311},
	urldate = {2018-03-14},
	journal = {Frontiers in Human Neuroscience},
	author = {Ding, Nai and Simon, Jonathan Z.},
	month = may,
	year = {2014},
}

@article{fontolan_contribution_2014,
	title = {The contribution of frequency-specific activity to hierarchical information processing in the human auditory cortex},
	volume = {5},
	issn = {2041-1723},
	doi = {10.1038/ncomms5694},
	urldate = {2018-03-14},
	journal = {Nature Communications},
	author = {Fontolan, L. and Morillon, B. and Liegeois-Chauvel, C. and Giraud, Anne-Lise},
	month = sep,
	year = {2014},
	keywords = {read},
	pages = {4694},
}

@article{craver_are_2018,
	title = {Are more details better? {On} the norms of completeness for mechanistic explanations},
	shorttitle = {Are {More} {Details} {Better}?},
	url = {https://academic.oup.com/bjps/advance-article/doi/10.1093/bjps/axy015/4816342},
	doi = {10.1093/bjps/axy015},
	abstract = {Completeness is an important but misunderstood norm of explanation. It has recently been argued that mechanistic accounts of scientific explanation are committed to the thesis that models are complete only if they describe everything about a mechanism and, as a corollary, that incomplete models are always improved by adding more details. If so, mechanistic accounts are at odds with the obvious and important role of abstraction in scientific modelling. We respond to this characterization of the mechanist’s views about abstraction and articulate norms of completeness for mechanistic explanations that have no such unwanted implications.},
	language = {en},
	urldate = {2018-01-20},
	journal = {The British Journal for the Philosophy of Science},
	author = {Craver, Carl and Kaplan, David M.},
	year = {2018},
	keywords = {cognitive neuroscience, readme, explanation, philosophy of neuroscience, philosophy of science, mechanistic explanation, neuroscience, mechanism},
	file = {Craver and Kaplan - 2018 - Are More Details Better On the Norms of Completen.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Craver and Kaplan - 2018 - Are More Details Better On the Norms of Completen.pdf:application/pdf},
}

@article{berezutskaya_neural_2017,
	title = {Neural tuning to low-level features of speech throughout the perisylvian cortex},
	volume = {37},
	copyright = {Copyright © 2017 the authors 0270-6474/17/377906-15\$15.00/0},
	issn = {0270-6474, 1529-2401},
	doi = {10.1523/JNEUROSCI.0238-17.2017},
	abstract = {Despite a large body of research, we continue to lack a detailed account of how auditory processing of continuous speech unfolds in the human brain. Previous research showed the propagation of low-level acoustic features of speech from posterior superior temporal gyrus toward anterior superior temporal gyrus in the human brain (Hullett et al., 2016). In this study, we investigate what happens to these neural representations past the superior temporal gyrus and how they engage higher-level language processing areas such as inferior frontal gyrus. We used low-level sound features to model neural responses to speech outside of the primary auditory cortex. Two complementary imaging techniques were used with human participants (both males and females): electrocorticography (ECoG) and fMRI. Both imaging techniques showed tuning of the perisylvian cortex to low-level speech features. With ECoG, we found evidence of propagation of the temporal features of speech sounds along the ventral pathway of language processing in the brain toward inferior frontal gyrus. Increasingly coarse temporal features of speech spreading from posterior superior temporal cortex toward inferior frontal gyrus were associated with linguistic features such as voice onset time, duration of the formant transitions, and phoneme, syllable, and word boundaries. The present findings provide the groundwork for a comprehensive bottom-up account of speech comprehension in the human brain.{\textless}/p{\textgreater}{\textless}p{\textgreater}\textbf{SIGNIFICANCE STATEMENT} We know that, during natural speech comprehension, a broad network of perisylvian cortical regions is involved in sound and language processing. Here, we investigated the tuning to low-level sound features within these regions using neural responses to a short feature film. We also looked at whether the tuning organization along these brain regions showed any parallel to the hierarchy of language structures in continuous speech. Our results show that low-level speech features propagate throughout the perisylvian cortex and potentially contribute to the emergence of “coarse” speech representations in inferior frontal gyrus typically associated with high-level language processing. These findings add to the previous work on auditory processing and underline a distinctive role of inferior frontal gyrus in natural speech comprehension.},
	language = {en},
	number = {33},
	urldate = {2017-12-08},
	journal = {Journal of Neuroscience},
	author = {Berezutskaya, Julia and Freudenburg, Zachary V. and Güçlü, Umut and Gerven, Marcel A. J. van and Ramsey, Nick F.},
	month = aug,
	year = {2017},
	pmid = {28716965},
	keywords = {project.lstmMEG, read, speech, ECoG},
	pages = {7906--7920},
	file = {Berezutskaya et al. - 2017 - Neural Tuning to Low-Level Features of Speech thro.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Berezutskaya et al. - 2017 - Neural Tuning to Low-Level Features of Speech thro.pdf:application/pdf},
}

@article{rommers_alpha_2017,
	title = {Alpha and theta band dynamics related to sentential constraint and word expectancy},
	volume = {32},
	issn = {2327-3798, 2327-3801},
	doi = {10.1080/23273798.2016.1183799},
	language = {en},
	number = {5},
	urldate = {2018-03-14},
	journal = {Language, Cognition and Neuroscience},
	author = {Rommers, Joost and Dickson, Danielle S. and Norton, James J. S. and Wlotko, Edward W. and Federmeier, Kara D.},
	month = may,
	year = {2017},
	keywords = {cognitive neuroscience, project.streams, language, oscillations, cloze probability},
	pages = {576--589},
}

@inproceedings{nelson_entropy_2017,
	address = {Valencia, Spain},
	title = {Entropy reduction correlates with temporal lobe activity},
	booktitle = {Proceedings of the 7th {Workshop} on {Cognitive} {Modeling} and {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Nelson, Matthew J. and Dehaene, Stanislas and Pallier, Christophe and Hale, John T.},
	month = apr,
	year = {2017},
	keywords = {project.streams, read},
	pages = {1--10},
	file = {Nelson idr. - 2017 - Entropy reduction correlates with temporal lobe ac.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Nelson idr. - 2017 - Entropy reduction correlates with temporal lobe ac.pdf:application/pdf},
}

@article{hagoort_neurobiology_2014,
	title = {The neurobiology of language beyond single words},
	volume = {37},
	issn = {0147-006X, 1545-4126},
	doi = {10.1146/annurev-neuro-071013-013847},
	language = {en},
	number = {1},
	urldate = {2018-03-13},
	journal = {Annual Review of Neuroscience},
	author = {Hagoort, Peter and Indefrey, Peter},
	month = jul,
	year = {2014},
	keywords = {MEG, cognitive neuroscience, fMRI, project.streams, sentence comprehension, common, meta-analysis},
	pages = {347--362},
}

@article{lam_neural_2016,
	title = {Neural activity during sentence processing as reflected in theta, alpha, beta, and gamma oscillations},
	volume = {142},
	issn = {1053-8119},
	url = {http://www.sciencedirect.com/science/article/pii/S1053811916002032},
	doi = {10.1016/j.neuroimage.2016.03.007},
	abstract = {We used magnetoencephalography (MEG) to explore the spatiotemporal dynamics of neural oscillations associated with sentence processing in 102 participants. We quantified changes in oscillatory power as the sentence unfolded, and in response to individual words in the sentence. For words early in a sentence compared to those late in the same sentence, we observed differences in left temporal and frontal areas, and bilateral frontal and right parietal regions for the theta, alpha, and beta frequency bands. The neural response to words in a sentence differed from the response to words in scrambled sentences in left-lateralized theta, alpha, beta, and gamma. The theta band effects suggest that a sentential context facilitates lexical retrieval, and that this facilitation is stronger for words late in the sentence. Effects in the alpha and beta bands may reflect the unification of semantic and syntactic information, and are suggestive of easier unification late in a sentence. The gamma oscillations are indicative of predicting the upcoming word during sentence processing. In conclusion, changes in oscillatory neuronal activity capture aspects of sentence processing. Our results support earlier claims that language (sentence) processing recruits areas distributed across both hemispheres, and extends beyond the classical language regions.},
	urldate = {2018-03-13},
	journal = {NeuroImage},
	author = {Lam, Nietzsche H. L. and Schoffelen, Jan-Mathijs and Uddén, Julia and Hultén, Annika and Hagoort, Peter},
	month = nov,
	year = {2016},
	keywords = {MEG, cognitive neuroscience, project.streams, oscillations},
	pages = {43--54},
}

@inproceedings{pham_convolutional_2016,
	address = {Austin, Texas},
	title = {Convolutional neural network language models},
	url = {https://aclweb.org/anthology/D16-1123},
	abstract = {Convolutional Neural Networks (CNNs) have shown to yield very strong results in several Computer Vision tasks. Their application to language has received much less attention, and it has mainly focused on static classification tasks, such as sentence classification for Sentiment Analysis or relation extraction. In this work, we study the application of CNNs to language modeling, a dynamic, sequential prediction task that needs models to capture local as well as long-range dependency information. Our contribution is twofold. First, we show that CNNs achieve 11-26\% better absolute performance than feed-forward neural language models, demonstrating their potential for language representation even in sequential tasks. As for recurrent models, our model outperforms RNNs but is below state of the art LSTM models. Second, we gain some understanding of the behavior of the model, showing that CNNs in language act as feature detectors at a high level of abstraction, like in Computer Vision, and that the model can profitably use information from as far as 16 words before the target.},
	booktitle = {Proceedings of the 2016 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Pham, Ngoc-Quan and Kruszewski, German and Boleda, Gemma},
	year = {2016},
	keywords = {LM, project.lstmMEG, read, computational linguistics},
	pages = {1153--1162},
	file = {Attachment:/Users/kriarm/Zotero/storage/8TRMWBEX/Pham, Kruszewski, Boleda - 2016 - Convolutional Neural Network Language Models.pdf:application/pdf},
}

@article{kalchbrenner_convolutional_2014,
	title = {A convolutional neural network for modelling sentences},
	url = {http://arxiv.org/abs/1404.2188},
	abstract = {The ability to accurately represent sentences is central to language understanding. We describe a convolutional architecture dubbed the Dynamic Convolutional Neural Network (DCNN) that we adopt for the semantic modelling of sentences. The network uses Dynamic k-Max Pooling, a global pooling operation over linear sequences. The network handles input sentences of varying length and induces a feature graph over the sentence that is capable of explicitly capturing short and long-range relations. The network does not rely on a parse tree and is easily applicable to any language. We test the DCNN in four experiments: small scale binary and multi-class sentiment prediction, six-way question classification and Twitter sentiment prediction by distant supervision. The network achieves excellent performance in the first three tasks and a greater than 25\% error reduction in the last task with respect to the strongest baseline.},
	urldate = {2017-11-22},
	journal = {arXiv:1404.2188 [cs]},
	author = {Kalchbrenner, Nal and Grefenstette, Edward and Blunsom, Phil},
	month = apr,
	year = {2014},
	note = {arXiv: 1404.2188},
	keywords = {project.lstmMEG, read},
}

@article{chaudhuri_large-scale_2015,
	title = {A large-scale circuit mechanism for hierarchical dynamical processing in the primate cortex},
	volume = {88},
	issn = {0896-6273},
	url = {http://www.sciencedirect.com/science/article/pii/S0896627315007655},
	doi = {10.1016/j.neuron.2015.09.008},
	abstract = {Summary
We developed a large-scale dynamical model of the macaque neocortex, which is based on recently acquired directed- and weighted-connectivity data from tract-tracing experiments, and which incorporates heterogeneity across areas. A hierarchy of timescales naturally emerges from this system: sensory areas show brief, transient responses to input (appropriate for sensory processing), whereas association areas integrate inputs over time and exhibit persistent activity (suitable for decision-making and working memory). The model displays multiple temporal hierarchies, as evidenced by contrasting responses to visual versus somatosensory stimulation. Moreover, slower prefrontal and temporal areas have a disproportionate impact on global brain dynamics. These findings establish a circuit mechanism for “temporal receptive windows” that are progressively enlarged along the cortical hierarchy, suggest an extension of time integration in decision making from local to large circuits, and should prompt a re-evaluation of the analysis of functional connectivity (measured by fMRI or electroencephalography/magnetoencephalography) by taking into account inter-areal heterogeneity.},
	number = {2},
	urldate = {2017-12-05},
	journal = {Neuron},
	author = {Chaudhuri, Rishidev and Knoblauch, Kenneth and Gariel, Marie-Alice and Kennedy, Henry and Wang, Xiao-Jing},
	month = oct,
	year = {2015},
	keywords = {project.lstmMEG, readme, fixme, TRW},
	pages = {419--431},
	file = {Chaudhuri et al. - 2015 - A Large-Scale Circuit Mechanism for Hierarchical D.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Chaudhuri et al. - 2015 - A Large-Scale Circuit Mechanism for Hierarchical D.pdf:application/pdf},
}

@article{jozefowicz_exploring_2016,
	title = {Exploring the limits of language modeling},
	url = {http://arxiv.org/abs/1602.02410},
	abstract = {In this work we explore recent advances in Recurrent Neural Networks for large scale Language Modeling, a task central to language understanding. We extend current models to deal with two key challenges present in this task: corpora and vocabulary sizes, and complex, long term structure of language. We perform an exhaustive study on techniques such as character Convolutional Neural Networks or Long-Short Term Memory, on the One Billion Word Benchmark. Our best single model significantly improves state-of-the-art perplexity from 51.3 down to 30.0 (whilst reducing the number of parameters by a factor of 20), while an ensemble of models sets a new record by improving perplexity from 41.0 down to 23.7. We also release these models for the NLP and ML community to study and improve upon.},
	urldate = {2017-11-20},
	journal = {arXiv:1602.02410 [cs]},
	author = {Jozefowicz, Rafal and Vinyals, Oriol and Schuster, Mike and Shazeer, Noam and Wu, Yonghui},
	month = feb,
	year = {2016},
	note = {arXiv: 1602.02410},
	keywords = {project.lstmMEG, readme},
}

@article{rescorla_theory_2014,
	title = {A theory of computational implementation},
	volume = {191},
	issn = {0039-7857, 1573-0964},
	url = {https://link.springer.com/article/10.1007/s11229-013-0324-y},
	doi = {10.1007/s11229-013-0324-y},
	abstract = {I articulate and defend a new theory of what it is for a physical system to implement an abstract computational model. According to my descriptivist theory, a physical system implements a computational model just in case the model accurately describes the system. Specifically, the system must reliably transit between computational states in accord with mechanical instructions encoded by the model. I contrast my theory with an influential approach to computational implementation espoused by Chalmers, Putnam, and others. I deploy my theory to illuminate the relation between computation and representation. I also rebut arguments, propounded by Putnam and Searle, that computational implementation is trivial.},
	language = {en},
	number = {6},
	urldate = {2018-03-13},
	journal = {Synthese},
	author = {Rescorla, Michael},
	month = apr,
	year = {2014},
	keywords = {explanation, philosophy of neuroscience, philosophy of science, read, computation, philosophy of mind, computational implementation, project.6},
	pages = {1277--1307},
	file = {Rescorla - 2014 - A theory of computational implementation.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Rescorla - 2014 - A theory of computational implementation.pdf:application/pdf},
}

@article{duffy_facebook_2017,
	title = {“{Facebook} for academics”: the convergence of self-branding and social media logic on academia.edu},
	volume = {3},
	issn = {2056-3051, 2056-3051},
	shorttitle = {“{Facebook} for {Academics}”},
	doi = {10.1177/2056305117696523},
	language = {en},
	number = {1},
	urldate = {2018-03-11},
	journal = {Social Media + Society},
	author = {Duffy, Brooke Erin and Pooley, Jefferson D.},
	month = mar,
	year = {2017},
	keywords = {read, science policy, publishing, academic publishing, social media},
	pages = {205630511769652},
	file = {Duffy and Pooley - 2017 - “Facebook for Academics” The Convergence of Self-.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Duffy and Pooley - 2017 - “Facebook for Academics” The Convergence of Self-.pdf:application/pdf},
}

@article{lariviere_oligopoly_2015,
	title = {The oligopoly of academic publishers in the digital era},
	volume = {10},
	issn = {1932-6203},
	doi = {10.1371/journal.pone.0127502},
	language = {en},
	number = {6},
	urldate = {2018-03-12},
	journal = {PLOS ONE},
	author = {Larivière, Vincent and Haustein, Stefanie and Mongeon, Philippe},
	editor = {Glanzel, Wolfgang},
	month = jun,
	year = {2015},
	keywords = {readme, science policy, publishing, academic publishing, policy},
	pages = {e0127502},
}

@article{michalareas_alpha-beta_2016,
	title = {Alpha-beta and gamma rhythms subserve feedback and feedforward influences among human visual cortical areas},
	volume = {89},
	issn = {08966273},
	doi = {10.1016/j.neuron.2015.12.018},
	language = {en},
	number = {2},
	urldate = {2018-03-12},
	journal = {Neuron},
	author = {Michalareas, Georgios and Vezoli, Julien and van Pelt, Stan and Schoffelen, Jan-Mathijs and Kennedy, Henry and Fries, Pascal},
	month = jan,
	year = {2016},
	keywords = {MEG, project.streams, readme, vision, oscillations, predictive coding},
	pages = {384--397},
}

@article{lau_cortical_2008,
	title = {A cortical network for semantics: (de)constructing the {N400}},
	volume = {9},
	issn = {1471-003X, 1471-0048},
	shorttitle = {A cortical network for semantics},
	url = {http://www.nature.com/articles/nrn2532},
	doi = {10.1038/nrn2532},
	language = {en},
	number = {12},
	urldate = {2018-03-12},
	journal = {Nature Reviews Neuroscience},
	author = {Lau, Ellen F. and Phillips, Colin and Poeppel, David},
	month = dec,
	year = {2008},
	keywords = {cognitive neuroscience, N400, ERP, project.streams, readme, semantics},
	pages = {920--933},
}

@article{bastiaansen_theta_2005,
	title = {Theta responses are involved in lexical—semantic retrieval during language processing},
	volume = {17},
	issn = {0898-929X, 1530-8898},
	doi = {10.1162/0898929053279469},
	language = {en},
	number = {3},
	urldate = {2018-03-11},
	journal = {Journal of Cognitive Neuroscience},
	author = {Bastiaansen, Marcel C.M. and Linden, Marieke van der and Keurs, Mariken ter and Dijkstra, Ton and Hagoort, Peter},
	month = mar,
	year = {2005},
	keywords = {EEG, project.streams, theta, oscillations},
	pages = {530--541},
}

@article{de_hollander_different_2016,
	title = {Different ways of linking behavioral and neural data via computational cognitive models},
	volume = {1},
	issn = {24519022},
	doi = {10.1016/j.bpsc.2015.11.004},
	language = {en},
	number = {2},
	urldate = {2018-03-11},
	journal = {Biological Psychiatry: Cognitive Neuroscience and Neuroimaging},
	author = {de Hollander, Gilles and Forstmann, Birte U. and Brown, Scott D.},
	month = mar,
	year = {2016},
	keywords = {read},
	pages = {101--109},
}

@article{turner_approaches_2017,
	series = {Model-based {Cognitive} {Neuroscience}},
	title = {Approaches to analysis in model-based cognitive neuroscience},
	volume = {76},
	issn = {0022-2496},
	url = {http://www.sciencedirect.com/science/article/pii/S0022249616000031},
	doi = {10.1016/j.jmp.2016.01.001},
	abstract = {Our understanding of cognition has been advanced by two traditionally non-overlapping and non-interacting groups. Mathematical psychologists rely on behavioral data to evaluate formal models of cognition, whereas cognitive neuroscientists rely on statistical models to understand patterns of neural activity, often without any attempt to make a connection to the mechanism supporting the computation. Both approaches suffer from critical limitations as a direct result of their focus on data at one level of analysis (cf. Marr, 1982), and these limitations have inspired researchers to attempt to combine both neural and behavioral measures in a cross-level integrative fashion. The importance of solving this problem has spawned several entirely new theoretical and statistical frameworks developed by both mathematical psychologists and cognitive neuroscientists. However, with each new approach comes a particular set of limitations and benefits. In this article, we survey and characterize several approaches for linking brain and behavioral data. We organize these approaches on the basis of particular cognitive modeling goals: (1) using the neural data to constrain a behavioral model, (2) using the behavioral model to predict neural data, and (3) fitting both neural and behavioral data simultaneously. Within each goal, we highlight a few particularly successful approaches for accomplishing that goal, and discuss some applications. Finally, we provide a conceptual guide to choosing among various analytic approaches in performing model-based cognitive neuroscience.},
	urldate = {2018-03-11},
	journal = {Journal of Mathematical Psychology},
	author = {Turner, Brandon M. and Forstmann, Birte U. and Love, Bradley C. and Palmeri, Thomas J. and Van Maanen, Leendert},
	month = feb,
	year = {2017},
	keywords = {read, neuroscience, thesis.introduction, computational cognitive neuroscience, model-based, cognitive modeling},
	pages = {65--79},
	file = {Turner et al. - 2017 - Approaches to analysis in model-based cognitive ne.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Turner et al. - 2017 - Approaches to analysis in model-based cognitive ne.pdf:application/pdf},
}

@article{wang_beta_2012,
	title = {Beta oscillations relate to the {N400m} during language comprehension},
	volume = {33},
	issn = {10659471},
	url = {http://doi.wiley.com/10.1002/hbm.21410},
	doi = {10.1002/hbm.21410},
	language = {en},
	number = {12},
	urldate = {2018-03-11},
	journal = {Human Brain Mapping},
	author = {Wang, Lin and Jensen, Ole and van den Brink, Danielle and Weder, Nienke and Schoffelen, Jan-Mathijs and Magyari, Lilla and Hagoort, Peter and Bastiaansen, Marcel},
	month = dec,
	year = {2012},
	keywords = {MEG, project.streams, readme, sentence comprehension, beta, oscillations},
	pages = {2898--2912},
}

@article{lewis_predictive_2015,
	series = {Special issue: {Prediction} in speech and language processing},
	title = {A predictive coding framework for rapid neural dynamics during sentence-level language comprehension},
	volume = {68},
	issn = {0010-9452},
	doi = {10.1016/j.cortex.2015.02.014},
	abstract = {There is a growing literature investigating the relationship between oscillatory neural dynamics measured using electroencephalography (EEG) and/or magnetoencephalography (MEG), and sentence-level language comprehension. Recent proposals have suggested a strong link between predictive coding accounts of the hierarchical flow of information in the brain, and oscillatory neural dynamics in the beta and gamma frequency ranges. We propose that findings relating beta and gamma oscillations to sentence-level language comprehension might be unified under such a predictive coding account. Our suggestion is that oscillatory activity in the beta frequency range may reflect both the active maintenance of the current network configuration responsible for representing the sentence-level meaning under construction, and the top-down propagation of predictions to hierarchically lower processing levels based on that representation. In addition, we suggest that oscillatory activity in the low and middle gamma range reflect the matching of top-down predictions with bottom-up linguistic input, while evoked high gamma might reflect the propagation of bottom-up prediction errors to higher levels of the processing hierarchy. We also discuss some of the implications of this predictive coding framework, and we outline ideas for how these might be tested experimentally.},
	urldate = {2018-03-11},
	journal = {Cortex},
	author = {Lewis, Ashley G. and Bastiaansen, Marcel},
	month = jul,
	year = {2015},
	keywords = {MEG, cognitive neuroscience, EEG, project.streams, oscillations},
	pages = {155--168},
}

@article{love_cognitive_2016,
	title = {Cognitive models as bridge between brain and behavior},
	volume = {20},
	issn = {13646613},
	doi = {10.1016/j.tics.2016.02.006},
	language = {en},
	number = {4},
	urldate = {2018-03-10},
	journal = {Trends in Cognitive Sciences},
	author = {Love, Bradley C.},
	month = apr,
	year = {2016},
	keywords = {cognitive neuroscience, explanation, philosophy of neuroscience, read, computation, project.6, cognitive modeling},
	pages = {247--248},
}

@article{tripp_deeper_2017,
	title = {A deeper understanding of the brain},
	issn = {1053-8119},
	url = {http://www.sciencedirect.com/science/article/pii/S1053811917311035},
	doi = {10.1016/j.neuroimage.2017.12.079},
	urldate = {2018-03-09},
	journal = {NeuroImage},
	author = {Tripp, Bryan},
	month = dec,
	year = {2017},
	keywords = {explanation, read, computation, ANN, theory, thesis},
	file = {Tripp - 2017 - A deeper understanding of the brain.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Tripp - 2017 - A deeper understanding of the brain.pdf:application/pdf},
}

@article{irsoy_deep_2014,
	title = {Deep recursive neural networks for compositionality in language},
	url = {http://papers.nips.cc/paper/5551-deep-recursive-neural-networks-for-compositionality-in-language.pdf},
	urldate = {2017-11-20},
	journal = {Advances in Neural Information Processing Systems 27},
	author = {Irsoy, Ozan and Cardie, Claire},
	collaborator = {Ghahramani, Z. and Welling, M. and Cortes, C. and Lawrence, N. D. and Weinberger, K. Q.},
	year = {2014},
	keywords = {project.lstmMEG, readme},
	pages = {2096--2104},
	file = {NIPS Snapshort:/Users/kriarm/Zotero/storage/5UHQU4WM/5551-deep-recursive-neural-networks-for-compositionality-in-language.html:text/html},
}

@article{lam_robust_2018,
	title = {Robust neuronal oscillatory entrainment to speech displays individual variation in lateralisation},
	volume = {0},
	issn = {2327-3798},
	url = {https://doi.org/10.1080/23273798.2018.1437456},
	doi = {10.1080/23273798.2018.1437456},
	abstract = {Neural oscillations may be instrumental for the tracking and segmentation of continuous speech. Earlier work has suggested that delta, theta and gamma oscillations entrain to the speech rhythm. We used magnetoencephalography and a large sample of 102 participants to investigate oscillatory entrainment to speech, and observed robust entrainment of delta and theta activity, and weak group-level gamma entrainment. We show that the peak frequency and the hemispheric lateralisation of the entrainment are subject to considerable individual variability. The first finding may support the involvement of intrinsic oscillations in entrainment, and the second finding suggests that there is no systematic default right-hemispheric bias for processing acoustic signals on a slow time scale. Although low frequency entrainment to speech is a robust phenomenon, the characteristics of entrainment vary across individuals, and this variation is important for understanding the underlying neural mechanisms of entrainment, as well as its functional significance.},
	number = {0},
	urldate = {2018-03-09},
	journal = {Language, Cognition and Neuroscience},
	author = {Lam, Nietzsche H. L. and Hultén, Annika and Hagoort, Peter and Schoffelen, Jan-Mathijs},
	month = feb,
	year = {2018},
	keywords = {MEG, project.streams, readme, speech, entrainment},
	pages = {1--12},
}

@article{ding_neural_2012,
	title = {Neural coding of continuous speech in auditory cortex during monaural and dichotic listening},
	volume = {107},
	issn = {0022-3077, 1522-1598},
	doi = {10.1152/jn.00297.2011},
	language = {en},
	number = {1},
	urldate = {2018-03-07},
	journal = {Journal of Neurophysiology},
	author = {Ding, Nai and Simon, Jonathan Z.},
	month = jan,
	year = {2012},
	keywords = {MEG, project.streams, speech, entrainment},
	pages = {78--89},
}

@article{luo_phase_2007,
	title = {Phase patterns of neuronal responses reliably discriminate speech in human auditory cortex},
	volume = {54},
	issn = {08966273},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0896627307004138},
	doi = {10.1016/j.neuron.2007.06.004},
	language = {en},
	number = {6},
	urldate = {2018-03-07},
	journal = {Neuron},
	author = {Luo, Huan and Poeppel, David},
	month = jun,
	year = {2007},
	keywords = {MEG, project.streams, readme, speech, entrainment},
	pages = {1001--1010},
}

@article{kayser_irregular_2015,
	title = {Irregular speech rate dissociates auditory cortical entrainment, evoked responses, and frontal {Alpha}},
	volume = {35},
	issn = {0270-6474, 1529-2401},
	url = {http://www.jneurosci.org/cgi/doi/10.1523/JNEUROSCI.2243-15.2015},
	doi = {10.1523/JNEUROSCI.2243-15.2015},
	language = {en},
	number = {44},
	urldate = {2018-03-07},
	journal = {Journal of Neuroscience},
	author = {Kayser, S. J. and Ince, R. A. A. and Gross, J. and Kayser, C.},
	month = nov,
	year = {2015},
	keywords = {MEG, project.streams, read, speech, entrainment},
	pages = {14691--14701},
}

@article{park_frontal_2015,
	title = {Frontal top-down signals increase coupling of auditory low-frequency oscillations to continuous speech in human listeners},
	volume = {25},
	issn = {09609822},
	doi = {10.1016/j.cub.2015.04.049},
	language = {en},
	number = {12},
	urldate = {2018-03-07},
	journal = {Current Biology},
	author = {Park, Hyojin and Ince, Robin A.A. and Schyns, Philippe G. and Thut, Gregor and Gross, Joachim},
	month = jun,
	year = {2015},
	keywords = {MEG, project.streams, read, mutual information, entrainment},
	pages = {1649--1653},
}

@article{forstmann_reciprocal_2011,
	title = {Reciprocal relations between cognitive neuroscience and formal cognitive models: opposites attract?},
	volume = {15},
	issn = {1364-6613},
	shorttitle = {Reciprocal relations between cognitive neuroscience and formal cognitive models},
	url = {http://www.sciencedirect.com/science/article/pii/S1364661311000726},
	doi = {10.1016/j.tics.2011.04.002},
	abstract = {Cognitive neuroscientists study how the brain implements particular cognitive processes such as perception, learning, and decision-making. Traditional approaches in which experiments are designed to target a specific cognitive process have been supplemented by two recent innovations. First, formal cognitive models can decompose observed behavioral data into multiple latent cognitive processes, allowing brain measurements to be associated with a particular cognitive process more precisely and more confidently. Second, cognitive neuroscience can provide additional data to inform the development of formal cognitive models, providing greater constraint than behavioral data alone. We argue that these fields are mutually dependent; not only can models guide neuroscientific endeavors, but understanding neural mechanisms can provide key insights into formal models of cognition.},
	number = {6},
	urldate = {2018-03-06},
	journal = {Trends in Cognitive Sciences},
	author = {Forstmann, Birte U. and Wagenmakers, Eric-Jan and Eichele, Tom and Brown, Scott and Serences, John T.},
	month = jun,
	year = {2011},
	keywords = {cognitive neuroscience, explanation, read, computation, thesis.introduction, computational cognitive neuroscience, model-based, theory},
	pages = {272--279},
	file = {Forstmann et al. - 2011 - Reciprocal relations between cognitive neuroscienc.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Forstmann et al. - 2011 - Reciprocal relations between cognitive neuroscienc.pdf:application/pdf},
}

@article{turner_outlook_2017,
	title = {Outlook on deep neural networks in computational cognitive neuroscience},
	issn = {1053-8119},
	url = {http://www.sciencedirect.com/science/article/pii/S1053811917311023},
	doi = {10.1016/j.neuroimage.2017.12.078},
	urldate = {2018-03-06},
	journal = {NeuroImage},
	author = {Turner, Brandon M. and Miletić, Steven and Forstmann, Birte U.},
	month = dec,
	year = {2017},
	keywords = {readme, explanation, computation, ANN, thesis.introduction, computational cognitive neuroscience, theory, DNN},
	file = {Turner et al. - 2017 - Outlook on deep neural networks in computational c.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Turner et al. - 2017 - Outlook on deep neural networks in computational c.pdf:application/pdf},
}

@misc{noauthor_against_nodate,
	title = {Against guilty pleasures: {Adorno} on the crimes of pop culture – {Owen} {Hulatt} {\textbar} {Aeon} {Essays}},
	shorttitle = {Against guilty pleasures},
	url = {https://aeon.co/essays/against-guilty-pleasures-adorno-on-the-crimes-of-pop-culture},
	abstract = {For Adorno, popular culture is not just bad art – it enslaves us to repetition and robs us of our aesthetic freedom},
	language = {en},
	urldate = {2018-03-04},
	journal = {Aeon},
}

@article{bzdok_classical_2017,
	title = {Classical statistics and statistical learning in imaging neuroscience},
	volume = {11},
	issn = {1662-453X},
	url = {http://journal.frontiersin.org/article/10.3389/fnins.2017.00543/full},
	doi = {10.3389/fnins.2017.00543},
	urldate = {2018-03-04},
	journal = {Frontiers in Neuroscience},
	author = {Bzdok, Danilo},
	month = oct,
	year = {2017},
	keywords = {cognitive neuroscience, methods, statistics, statistical learning, reading, classical statistics},
}

@article{baroni_linguistic_2020,
	title = {Linguistic generalization and compositionality in modern artificial neural networks},
	volume = {375},
	issn = {0962-8436, 1471-2970},
	url = {https://royalsocietypublishing.org/doi/10.1098/rstb.2019.0307},
	doi = {10.1098/rstb.2019.0307},
	language = {en},
	number = {1791},
	urldate = {2020-03-26},
	journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	author = {Baroni, Marco},
	month = feb,
	year = {2020},
	keywords = {LSTM, RNN, compositionality},
	pages = {20190307},
}

@incollection{jain_incorporating_2018,
	title = {Incorporating context into language encoding models for {fMRI}},
	url = {http://papers.nips.cc/paper/7897-incorporating-context-into-language-encoding-models-for-fmri.pdf},
	urldate = {2019-01-25},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31},
	publisher = {Curran Associates, Inc.},
	author = {Jain, Shailee and Huth, Alexander},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
	keywords = {fMRI, language model, LSTM, project.lstmMEG, encoding models},
	pages = {6629--6638},
	file = {Jain and Huth - 2018 - Incorporating context into language encoding model.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Jain and Huth - 2018 - Incorporating context into language encoding model.pdf:application/pdf},
}

@article{marom_neural_2010,
	title = {Neural timescales or lack thereof},
	volume = {90},
	issn = {03010082},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0301008209001543},
	doi = {10.1016/j.pneurobio.2009.10.003},
	language = {en},
	number = {1},
	urldate = {2020-03-19},
	journal = {Progress in Neurobiology},
	author = {Marom, Shimon},
	month = jan,
	year = {2010},
	keywords = {project.lstmMEG, project.lsnn, timescales},
	pages = {16--28},
}

@article{de_cheveigne_filters_2019,
	title = {Filters: {When}, why, and how (not) to use them},
	volume = {102},
	issn = {08966273},
	shorttitle = {Filters},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627319301746},
	doi = {10.1016/j.neuron.2019.02.039},
	language = {en},
	number = {2},
	urldate = {2020-03-02},
	journal = {Neuron},
	author = {de Cheveigné, Alain and Nelken, Israel},
	month = apr,
	year = {2019},
	keywords = {MEG, methods, filters, Fourier, Fourier theorem, Fourier transform, signal processing},
	pages = {280--293},
}

@article{gross_magnetoencephalography_2019,
	title = {Magnetoencephalography in cognitive neuroscience: {A} primer},
	volume = {104},
	issn = {08966273},
	shorttitle = {Magnetoencephalography in {Cognitive} {Neuroscience}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627319305999},
	doi = {10.1016/j.neuron.2019.07.001},
	language = {en},
	number = {2},
	urldate = {2020-03-02},
	journal = {Neuron},
	author = {Gross, Joachim},
	month = oct,
	year = {2019},
	keywords = {MEG, oscillations, source reconstruction, review},
	pages = {189--204},
}

@inproceedings{giulianelli_under_2018,
	address = {Brussels, Belgium},
	title = {Under the hood: {Using} diagnostic classifiers to investigate and improve how language models track agreement information},
	shorttitle = {Under the {Hood}},
	url = {http://aclweb.org/anthology/W18-5426},
	doi = {10.18653/v1/W18-5426},
	language = {en},
	urldate = {2020-02-20},
	booktitle = {Proceedings of the 2018 {EMNLP} {Workshop} {BlackboxNLP}: {Analyzing} and {Interpreting} {Neural} {Networks} for {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Giulianelli, Mario and Harding, Jack and Mohnert, Florian and Hupkes, Dieuwke and Zuidema, Willem},
	year = {2018},
	keywords = {language model, LM, LSTM, interpretability, agreement, subject-verb agreement},
	pages = {240--248},
}

@article{belinkov_analysis_2019,
	title = {Analysis methods in neural language processing: {A} survey},
	volume = {7},
	issn = {2307-387X},
	shorttitle = {Analysis {Methods} in {Neural} {Language} {Processing}},
	url = {https://www.mitpressjournals.org/doi/abs/10.1162/tacl_a_00254},
	doi = {10.1162/tacl_a_00254},
	language = {en},
	urldate = {2020-02-18},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Belinkov, Yonatan and Glass, James},
	month = mar,
	year = {2019},
	keywords = {deep learning, language model, NLP, RNN, project.lstmMEG, interpretability},
	pages = {49--72},
}

@article{ogawa_differential_2010,
	title = {Differential temporal storage capacity in the baseline activity of neurons in macaque frontal eye field and area {V4}},
	volume = {103},
	issn = {0022-3077, 1522-1598},
	url = {https://www.physiology.org/doi/10.1152/jn.01066.2009},
	doi = {10.1152/jn.01066.2009},
	abstract = {Previous studies have suggested that spontaneous fluctuations in neuronal activity reflect intrinsic functional brain architecture. Inspired by these findings, we analyzed baseline neuronal activity in the monkey frontal eye field (FEF; a visuomotor area) and area V4 (a visual area) during the fixation period of a cognitive behavioral task in the absence of any task-specific stimuli or behaviors. Specifically, we examined the temporal storage capacity of the instantaneous discharge rate in FEF and V4 neurons by calculating the correlation of the spike count in a bin with that in another bin during the baseline activity of a trial. We found that most FEF neurons fired significantly more (or less) in one bin if they fired more (or less) in another bin within a trial, even when these two time bins were separated by hundreds of milliseconds. By contrast, similar long time-lag correlations were observed in only a small fraction of V4 neurons, indicating that temporal correlations were considerably stronger in FEF compared with those in V4 neurons. Additional analyses revealed that the findings were not attributable to other task-related variables or ongoing behavioral performance, suggesting that the differences in temporal correlation strength reflect differences in intrinsic structural and functional architecture between visual and visuomotor areas. Thus FEF neurons probably play a greater role than V4 neurons in neural circuits responsible for temporal storage in activity.},
	language = {en},
	number = {5},
	urldate = {2020-02-12},
	journal = {Journal of Neurophysiology},
	author = {Ogawa, Tadashi and Komatsu, Hidehiko},
	month = may,
	year = {2010},
	keywords = {project.lstmMEG, timescales, deepmeg, FEF, V4},
	pages = {2433--2445},
}

@article{sejnowski_unreasonable_2020,
	title = {The unreasonable effectiveness of deep learning in artificial intelligence},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1907373117},
	doi = {10.1073/pnas.1907373117},
	abstract = {Deep learning networks have been trained to recognize speech, caption photographs, and translate text between languages at high levels of performance. Although applications of deep learning networks to real-world problems have become ubiquitous, our understanding of why they are so effective is lacking. These empirical results should not be possible according to sample complexity in statistics and nonconvex optimization theory. However, paradoxes in the training and effectiveness of deep learning networks are being investigated and insights are being found in the geometry of high-dimensional spaces. A mathematical theory of deep learning would illuminate how they function, allow us to assess the strengths and weaknesses of different network architectures, and lead to major improvements. Deep learning has provided natural ways for humans to communicate with digital devices and is foundational for building artificial general intelligence. Deep learning was inspired by the architecture of the cerebral cortex and insights into autonomy and general intelligence may be found in other brain regions that are essential for planning and survival, but major breakthroughs will be needed to achieve these goals.},
	language = {en},
	urldate = {2020-02-12},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Sejnowski, Terrence J.},
	month = jan,
	year = {2020},
	keywords = {deep learning, neural networks, ANN, review},
	pages = {201907373},
}

@article{hasson_direct_2020,
	title = {Direct fit to nature: {An} evolutionary perspective on biological and artificial neural networks},
	volume = {105},
	issn = {08966273},
	shorttitle = {Direct {Fit} to {Nature}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S089662731931044X},
	doi = {10.1016/j.neuron.2019.12.002},
	language = {en},
	number = {3},
	urldate = {2020-02-12},
	journal = {Neuron},
	author = {Hasson, Uri and Nastase, Samuel A. and Goldstein, Ariel},
	month = feb,
	year = {2020},
	keywords = {evolution, neural networks, ANN, thesis.introduction, thesis, RNNs},
	pages = {416--434},
}

@article{cowan_many_2017,
	title = {The many faces of working memory and short-term storage},
	volume = {24},
	issn = {1069-9384, 1531-5320},
	url = {http://link.springer.com/10.3758/s13423-016-1191-6},
	doi = {10.3758/s13423-016-1191-6},
	language = {en},
	number = {4},
	urldate = {2020-02-08},
	journal = {Psychonomic Bulletin \& Review},
	author = {Cowan, Nelson},
	month = aug,
	year = {2017},
	keywords = {working memory, project.lstmMEG, project.lsnn, processing memory, review},
	pages = {1158--1170},
}

@article{schonbrodt_training_2019,
	title = {Training students for the {Open} {Science} future},
	volume = {3},
	issn = {2397-3374},
	url = {http://www.nature.com/articles/s41562-019-0726-z},
	doi = {10.1038/s41562-019-0726-z},
	language = {en},
	number = {10},
	urldate = {2020-02-07},
	journal = {Nature Human Behaviour},
	author = {Schönbrodt, Felix},
	month = oct,
	year = {2019},
	pages = {1031--1031},
}

@incollection{kety_general_1957,
	title = {The general metabolism of the brain in vivo},
	isbn = {978-0-08-009062-7},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B9780080090627500266},
	language = {en},
	urldate = {2020-01-30},
	booktitle = {Metabolism of the {Nervous} {System}},
	publisher = {Elsevier},
	author = {Kety, Seymour S.},
	year = {1957},
	doi = {10.1016/B978-0-08-009062-7.50026-6},
	pages = {221--237},
}

@article{young_recent_2018,
	title = {Recent trends in deep learning based natural language processing},
	url = {http://arxiv.org/abs/1708.02709},
	abstract = {Deep learning methods employ multiple processing layers to learn hierarchical representations of data and have produced state-of-the-art results in many domains. Recently, a variety of model designs and methods have blossomed in the context of natural language processing (NLP). In this paper, we review significant deep learning related models and methods that have been employed for numerous NLP tasks and provide a walk-through of their evolution. We also summarize, compare and contrast the various models and put forward a detailed understanding of the past, present and future of deep learning in NLP.},
	urldate = {2020-01-30},
	journal = {arXiv:1708.02709 [cs]},
	author = {Young, Tom and Hazarika, Devamanyu and Poria, Soujanya and Cambria, Erik},
	month = nov,
	year = {2018},
	note = {arXiv: 1708.02709},
	keywords = {deep learning, NLP, natural language processing},
}

@article{wang_flexible_2018,
	title = {Flexible timing by temporal scaling of cortical responses},
	volume = {21},
	issn = {1546-1726},
	doi = {10.1038/s41593-017-0028-6},
	abstract = {Humans can deliberately control the timing of their actions but the neural mechanisms underlying such control are largely unknown. In this article, Wang, Narain and their colleagues report that such flexibility emerges in rhesus monkeys from the ability of their brain to flexibly control the speed at which cortical responses unfold in time.},
	language = {English},
	number = {1},
	journal = {Nature Neuroscience},
	author = {Wang, Jing and Narain, Devika and Hosseini, Eghbal A. and Jazayeri, Mehrdad},
	month = jan,
	year = {2018},
	note = {tex.copyright: 2017 The Author(s)},
	keywords = {reading},
	pages = {102},
	file = {s41593-017-0028-6:/Users/kriarm/Zotero/storage/PJ37JGU6/s41593-017-0028-6.html:text/html;Wang idr. - 2018 - Flexible timing by temporal scaling of cortical re:/Users/kriarm/Zotero/storage/HRE3RWB9/Wang idr. - 2018 - Flexible timing by temporal scaling of cortical re.pdf:application/pdf},
}

@inproceedings{chien_hierarchical_2017,
	title = {A hierarchical model for sequential perception and sequence learning},
	author = {{Chien} and Honey, Christopher J.},
	year = {2017},
	keywords = {hierarchy, learning, project.lstmMEG, readme, TRW, sequence},
	file = {Chien and Honey - 2017 - A hierarchical model for sequential perception and:/Users/kriarm/Zotero/storage/DD6CY8DK/Chien and Honey - 2017 - A hierarchical model for sequential perception and.pdf:application/pdf},
}

@article{dehaene_neural_2015,
	title = {The neural representation of sequences: {From} transition probabilities to algebraic patterns and linguistic trees},
	volume = {88},
	issn = {0896-6273},
	shorttitle = {The {Neural} {Representation} of {Sequences}},
	doi = {10.1016/j.neuron.2015.09.019},
	abstract = {A sequence of images, sounds, or words can be stored at several levels of detail, from specific items and their timing to abstract structure. We propose a taxonomy of five distinct cerebral mechanisms for sequence coding: transitions and timing knowledge, chunking, ordinal knowledge, algebraic patterns, and nested tree structures. In each case, we review the available experimental paradigms and list the behavioral and neural signatures of the systems involved. Tree structures require a specific recursive neural code, as yet unidentified by electrophysiology, possibly unique to humans, and which may explain the singularity of human language and cognition.},
	number = {1},
	journal = {Neuron},
	author = {Dehaene, Stanislas and Meyniel, Florent and Wacongne, Catherine and Wang, Liping and Pallier, Christophe},
	month = oct,
	year = {2015},
	keywords = {syntax, hierarchy, project.lstmMEG, readme, language},
	pages = {2--19},
	file = {Dehaene et al. - 2015 - The neural representation of sequences from trans:/Users/kriarm/Zotero/storage/QGA77C5H/Dehaene et al. - 2015 - The neural representation of sequences from trans.pdf:application/pdf},
}

@article{gross_dynamic_2001,
	title = {Dynamic imaging of coherent sources: {Studying} neural interactions in the human brain},
	volume = {98},
	doi = {10.1073/pnas.98.2.694},
	abstract = {Functional connectivity between cortical areas may appear as correlated time behavior of neural activity. It has been suggested that merging of separate features into a single percept ("binding") is associated with coherent gamma band activity across the cortical areas involved. Therefore, it would be of utmost interest to image cortico-cortical coherence in the working human brain. The frequency specificity and transient nature of these interactions requires time-sensitive tools such as magneto- or electroencephalography (MEG/EEG). Coherence between signals of sensors covering different scalp areas is commonly taken as a measure of functional coupling. However, this approach provides vague information on the actual cortical areas involved, owing to the complex relation between the active brain areas and the sensor recordings. We propose a solution to the crucial issue of proceeding beyond the MEG sensor level to estimate coherences between cortical areas. Dynamic imaging of coherent sources (DICS) uses a spatial filter to localize coherent brain regions and provides the time courses of their activity. Reference points for the computation of neural coupling may be based on brain areas of maximum power or other physiologically meaningful information, or they may be estimated starting from sensor coherences. The performance of DICS is evaluated with simulated data and illustrated with recordings of spontaneous activity in a healthy subject and a parkinsonian patient. Methods for estimating functional connectivities between brain areas will facilitate characterization of cortical networks involved in sensory, motor, or cognitive tasks and will allow investigation of pathological connectivities in neurological disorders.},
	number = {2},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Gross, J and Kujala, J and Hamalainen, M and Timmermann, L and Schnitzler, A and Salmelin, R},
	month = jan,
	year = {2001},
	note = {tex.pmid: 11209067},
	keywords = {MEG, methods, project.streams, neural oscillations, common, source reconstruction, DICS},
	pages = {694--9},
}

@article{keuleers_subtlex-nl_2010,
	title = {{SUBTLEX}-{NL}: {A} new measure for {Dutch} word frequency based on film subtitles},
	volume = {42},
	issn = {1554-351X, 1554-3528},
	shorttitle = {{SUBTLEX}-{NL}},
	doi = {10.3758/BRM.42.3.643},
	language = {English},
	number = {3},
	journal = {Behavior Research Methods},
	author = {Keuleers, Emmanuel and Brysbaert, Marc and New, Boris},
	month = aug,
	year = {2010},
	keywords = {methods, project.streams, thesis, corpus, SUBTLEX},
	pages = {643--650},
}

@article{hubert_receptive_1962,
	title = {Receptive fields, binocular interaction and functional architecture in the cat's visual cortex.},
	volume = {160},
	journal = {Journal of Physiology},
	author = {Hubert, D. H. and Wiesel, T. N.},
	year = {1962},
	keywords = {hierarchy, readme, vision, visual cortex},
	pages = {106--154},
	file = {Hubert and Wiesel - 1962 - Receptive fields, binocular interaction and functi:/Users/kriarm/Zotero/storage/PAIZY3HM/Hubert and Wiesel - 1962 - Receptive fields, binocular interaction and functi.pdf:application/pdf},
}

@article{heeger_theory_2017,
	title = {Theory of cortical function},
	volume = {114},
	issn = {0027-8424, 1091-6490},
	doi = {10.1073/pnas.1619788114},
	abstract = {Most models of sensory processing in the brain have a feedforward architecture in which each stage comprises simple linear filtering operations and nonlinearities. Models of this form have been used to explain a wide range of neurophysiological and psychophysical data, and many recent successes in artificial intelligence (with deep convolutional neural nets) are based on this architecture. However, neocortex is not a feedforward architecture. This paper proposes a first step toward an alternative computational framework in which neural activity in each brain area depends on a combination of feedforward drive (bottom-up from the previous processing stage), feedback drive (top-down context from the next stage), and prior drive (expectation). The relative contributions of feedforward drive, feedback drive, and prior drive are controlled by a handful of state parameters, which I hypothesize correspond to neuromodulators and oscillatory activity. In some states, neural responses are dominated by the feedforward drive and the theory is identical to a conventional feedforward model, thereby preserving all of the desirable features of those models. In other states, the theory is a generative model that constructs a sensory representation from an abstract representation, like memory recall. In still other states, the theory combines prior expectation with sensory input, explores different possible perceptual interpretations of ambiguous sensory inputs, and predicts forward in time. The theory, therefore, offers an empirically testable framework for understanding how the cortex accomplishes inference, exploration, and prediction.},
	language = {English},
	number = {8},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Heeger, David J.},
	month = feb,
	year = {2017},
	note = {tex.pmid: 28167793},
	keywords = {hierarchy, project.lstmMEG, readme, computation, inference, thesis.introduction, theory, project.6, time},
	pages = {1773--1782},
}

@book{dumas_les_1995,
	address = {Paris},
	title = {Les trois mousquetaires},
	isbn = {978-2-253-00888-0},
	publisher = {Librairie générale Française},
	author = {pêre Dumas, Alexandre},
	year = {1995},
	keywords = {fiction, fr, history, routine, writing},
}

@article{chirimuuta_explanation_nodate,
	title = {Explanation in computational neuroscience: {Causal} and non-causal},
	shorttitle = {Explanation in {Computational} {Neuroscience}},
	doi = {10.1093/bjps/axw034},
	abstract = {This article examines three candidate cases of non-causal explanation in computational neuroscience. I argue that there are instances of efficient coding explanation that are strongly analogous to examples of non-causal explanation in physics and biology, as presented by Batterman ([2002]), Woodward ([2003]), and Lange ([2013]). By integrating Lange's and Woodward's accounts, I offer a new way to elucidate the distinction between causal and non-causal explanation, and to address concerns about the explanatory sufficiency of non-mechanistic models in neuroscience. I also use this framework to shed light on the dispute over the interpretation of dynamical models of the brain. 1 Introduction1.1 Efficient coding explanation in computational neuroscience1.2 Defining non-causal explanation2 Case I: Hybrid Computation3 Case II: The Gabor Model Revisited4 Case III: A Dynamical Model of Prefrontal Cortex4.1 A new explanation of context-dependent computation4.2 Causal or non-causal?5 Causal and Non-causal: Does the Difference Matter?},
	journal = {The British Journal for the Philosophy of Science},
	author = {Chirimuuta, M.},
	keywords = {readme, explanation, philosophy of neuroscience, philosophy of science, computation, epistemology},
	file = {3069966:/Users/kriarm/Zotero/storage/BAIE775H/3069966.html:text/html},
}

@article{chirimuuta_minimal_2014,
	title = {Minimal models and canonical neural computations: {The} distinctness of computational explanation in neuroscience},
	volume = {191},
	issn = {0039-7857, 1573-0964},
	shorttitle = {Minimal models and canonical neural computations},
	doi = {10.1007/s11229-013-0369-y},
	abstract = {In a recent paper, Kaplan (Synthese 183:339–373, 2011) takes up the task of extending Craver's (Explaining the brain, 2007) mechanistic account of explanation in neuroscience to the new territory of computational neuroscience. He presents the model to mechanism mapping (3M) criterion as a condition for a model's explanatory adequacy. This mechanistic approach is intended to replace earlier accounts which posited a level of computational analysis conceived as distinct and autonomous from underlying mechanistic details. In this paper I discuss work in computational neuroscience that creates difficulties for the mechanist project. Carandini and Heeger (Nat Rev Neurosci 13:51–62, 2012) propose that many neural response properties can be understood in terms of canonical neural computations. These are “standard computational modules that apply the same fundamental operations in a variety of contexts.” Importantly, these computations can have numerous biophysical realisations, and so straightforward examination of the mechanisms underlying these computations carries little explanatory weight. Through a comparison between this modelling approach and minimal models in other branches of science, I argue that computational neuroscience frequently employs a distinct explanatory style, namely, efficient coding explanation. Such explanations cannot be assimilated into the mechanistic framework but do bear interesting similarities with evolutionary and optimality explanations elsewhere in biology.},
	language = {English},
	number = {2},
	journal = {Synthese},
	author = {Chirimuuta, M.},
	month = jan,
	year = {2014},
	keywords = {explanation, philosophy of neuroscience, philosophy of science, read, computation, efficient coding},
	pages = {127--153},
	file = {Chirimuuta - 2014 - Minimal models and canonical neural computations:/Users/kriarm/Zotero/storage/KDJUCZ33/Chirimuuta - 2014 - Minimal models and canonical neural computations .pdf:application/pdf},
}

@article{burnston_computational_2016,
	title = {Computational neuroscience and localized neural function},
	volume = {193},
	issn = {0039-7857, 1573-0964},
	doi = {10.1007/s11229-016-1099-8},
	abstract = {In this paper I criticize a view of functional localization in neuroscience, which I call “computational absolutism” (CA). “Absolutism” in general is the view that each part of the brain should be given a single, univocal function ascription. Traditional varieties of absolutism posit that each part of the brain processes a particular type of information and/or performs a specific task. These function attributions are currently beset by physiological evidence which seems to suggest that brain areas are multifunctional—that they process distinct information and perform different tasks depending on context. Many theorists take this contextual variation as inimical to successful localization, and claim that we can avoid it by changing our functional descriptions to computational descriptions. The idea is that we can have highly generalizable and predictive functional theories if we can discover a single computation performed by each area regardless of the specific context in which it operates. I argue, drawing on computational models of perceptual area MT, that this computational version of absolutism fails to come through on its promises. In MT, the modeling field has not produced a univocal computational description, but instead a plurality of models analyzing different aspects of MT function. Moreover, CA cannot appeal to theoretical unification to solve this problem, since highly general models, on their own, neither explain nor predict what MT does in any particular context. I close by offering a perspective on neural modeling inspired by Nancy Cartwright's and Margaret Morrison's views of modeling in the physical sciences.},
	language = {English},
	number = {12},
	journal = {Synthese},
	author = {Burnston, Daniel C.},
	month = dec,
	year = {2016},
	keywords = {readme, explanation, philosophy of neuroscience, philosophy of science, epistemology, computational neuroscience},
	pages = {3741--3762},
	file = {Burnston - 2016 - Computational neuroscience and localized neural fu:/Users/kriarm/Zotero/storage/CBEFUVC9/Burnston - 2016 - Computational neuroscience and localized neural fu.pdf:application/pdf},
}

@article{de-wit_is_2016,
	title = {Is neuroimaging measuring information in the brain?},
	volume = {23},
	issn = {1069-9384, 1531-5320},
	doi = {10.3758/s13423-016-1002-0},
	abstract = {Psychology moved beyond the stimulus response mapping of behaviorism by adopting an information processing framework. This shift from behavioral to cognitive science was partly inspired by work demonstrating that the concept of information could be defined and quantified (Shannon, 1948). This transition developed further from cognitive science into cognitive neuroscience, in an attempt to measure information in the brain. In the cognitive neurosciences, however, the term information is often used without a clear definition. This paper will argue that, if the formulation proposed by Shannon is applied to modern neuroimaging, then numerous results would be interpreted differently. More specifically, we argue that much modern cognitive neuroscience implicitly focuses on the question of how we can interpret the activations we record in the brain (experimenter-as-receiver), rather than on the core question of how the rest of the brain can interpret those activations (cortex-as-receiver). A clearer focus on whether activations recorded via neuroimaging can actually act as information in the brain would not only change how findings are interpreted but should also change the direction of empirical research in cognitive neuroscience.},
	language = {English},
	number = {5},
	journal = {Psychonomic Bulletin \& Review},
	author = {de-Wit, Lee and Alexander, David and Ekroll, Vebjørn and Wagemans, Johan},
	month = oct,
	year = {2016},
	keywords = {readme, explanation, philosophy of neuroscience, philosophy of science, epistemology, connectivity},
	pages = {1415--1428},
	file = {de-Wit et al. - 2016 - Is neuroimaging measuring information in the brain:/Users/kriarm/Zotero/storage/PQKQKL8T/de-Wit et al. - 2016 - Is neuroimaging measuring information in the brain.pdf:application/pdf},
}

@article{mars_connectivity_2018,
	series = {The {Evolution} of {Language}},
	title = {Connectivity and the search for specializations in the language-capable brain},
	volume = {21},
	issn = {2352-1546},
	doi = {10.1016/j.cobeha.2017.11.001},
	abstract = {The search for the anatomical basis of language has traditionally been a search for specializations. More recently such research has focused both on aspects of brain organization that are unique to humans and aspects shared with other primates. This work has mostly concentrated on the architecture of connections between brain areas. However, as specializations can take many guises, comparison of anatomical organization across species is often complicated. We demonstrate how viewing different types of specializations within a common framework allows one to better appreciate both shared and unique aspects of brain organization. We illustrate this point by discussing recent insights into the anatomy of the dorsal language pathway to the frontal cortex and areas for laryngeal control in the motor cortex.},
	journal = {Current Opinion in Behavioral Sciences},
	author = {Mars, Rogier B and Eichert, Nicole and Jbabdi, Saad and Verhagen, Lennart and Rushworth, Matthew F. S.},
	month = jun,
	year = {2018},
	keywords = {read},
	pages = {19--26},
	file = {Mars et al. - 2018 - Connectivity and the search for specializations in:/Users/kriarm/Zotero/storage/K74SMZA4/Mars et al. - 2018 - Connectivity and the search for specializations in.pdf:application/pdf},
}

@incollection{guclu_brains_2016,
	title = {Brains on beats},
	url = {http://papers.nips.cc/paper/6222-brains-on-beats.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 29},
	publisher = {Curran Associates, Inc.},
	author = {Güçlü, Umut and Thielen, Jordy and Hanke, Michael and van Gerven, Marcel},
	editor = {Lee, D. D. and Sugiyama, M. and Luxburg, U. V. and Guyon, I. and Garnett, R.},
	year = {2016},
	keywords = {fMRI, project.lstmMEG, CNN, machine learning, ANN, music, tag prediction},
	pages = {2101--2109},
}

@article{kandylaki_story_2019,
	title = {From story comprehension to the neurobiology of language},
	volume = {34},
	issn = {2327-3798},
	url = {https://doi.org/10.1080/23273798.2019.1584679},
	doi = {10.1080/23273798.2019.1584679},
	abstract = {Traditionally, language processing has been studied in tightly controlled experimental designs. However, in addition to raising questions about the generalisability of results to natural language use, these designs impose limitations on the types of research questions that can be examined. Accordingly, an increasing number of groups are now exploring new methodological approaches for studying language in rich contextual settings. This special issue aims to deliver an overview of the possibilities that are opened up by the use of context and naturalistic experimental designs. These include the ability to examine various – and hitherto understudied – populations, or linguistic phenomena on different levels (e.g. word, sentence, discourse) in the same naturalistic stimulus such as a story. Other special issue contributions focus on addressing the methodological challenges inherent to naturalistic experiments. Additionally, we stress the importance of interdisciplinary work and open sharing of data from naturalistic experiments in the neurobiology of language.},
	number = {4},
	urldate = {2020-01-29},
	journal = {Language, Cognition and Neuroscience},
	author = {Kandylaki, Katerina D. and Bornkessel-Schlesewsky, Ina},
	month = apr,
	year = {2019},
	pages = {405--410},
}

@techreport{pernet_best_2018,
	type = {preprint},
	title = {Best practices in data analysis and sharing in neuroimaging using {MEEG}},
	url = {https://osf.io/a8dhx},
	abstract = {Non-invasive neuroimaging methods, including magnetoencephalography and electroencephalography (MEEG), have been critical in advancing the understanding of brain function in healthy people and in individuals with neurological or psychiatric disorders. Currently, scientific practice is undergoing a tremendous change, aiming to improve both research reproducibility and transparency in data collection, documentation and analysis, and in manuscript review. To advance the practice of open science, the Organization for Human Brain Mapping created the Committee on Best Practice in Data Analysis and Sharing (COBIDAS), which produced a report for MRI-based data in 2016. This effort continues with the OHBM’s COBIDAS MEEG committee whose task was to create a similar document that describes best practice recommendations for MEEG data. The document was drafted by OHBM experts in MEEG, with input from the world-wide brain imaging community, including OHBM members who volunteered to help with this effort, as well as Executive Committee members of the International Federation for Clinical Neurophysiology. This document outlines the principles of performing open and reproducible research in MEEG. Not all MEEG data practices are described in this document. Instead, we propose principles that we believe are current best practice for most recordings and common analyses. Furthermore, we suggest reporting guidelines for Authors that will enable others in the field to fully understand and potentially replicate any study. This document should be helpful to Authors, Reviewers of manuscripts, as well as Editors of neuroscience journals.},
	urldate = {2020-01-29},
	institution = {Open Science Framework},
	author = {Pernet, Cyril R and Garrido, Marta and Gramfort, Alexandre and Maurits, Natasha and Michel, Christoph and Pang, Elizabeth and Salmelin, Riitta and Schoffelen, Jan Mathijs and Valdes-Sosa, Pedro A. and Puce, Aina},
	month = aug,
	year = {2018},
	doi = {10.31219/osf.io/a8dhx},
	keywords = {MEG, open science, EEG, methods},
}

@article{holdgraf_ieeg-bids_2019,
	title = {{iEEG}-{BIDS}, extending the {Brain} {Imaging} {Data} {Structure} specification to human intracranial electrophysiology},
	volume = {6},
	copyright = {2019 The Author(s)},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/s41597-019-0105-7},
	doi = {10.1038/s41597-019-0105-7},
	abstract = {The Brain Imaging Data Structure (BIDS) is a community-driven specification for organizing neuroscience data and metadata with the aim to make datasets more transparent, reusable, and reproducible. Intracranial electroencephalography (iEEG) data offer a unique combination of high spatial and temporal resolution measurements of the living human brain. To improve internal (re)use and external sharing of these unique data, we present a specification for storing and sharing iEEG data: iEEG-BIDS.},
	language = {en},
	number = {1},
	urldate = {2020-01-29},
	journal = {Scientific Data},
	author = {Holdgraf, Christopher and Appelhoff, Stefan and Bickel, Stephan and Bouchard, Kristofer and D’Ambrosio, Sasha and David, Olivier and Devinsky, Orrin and Dichter, Benjamin and Flinker, Adeen and Foster, Brett L. and Gorgolewski, Krzysztof J. and Groen, Iris and Groppe, David and Gunduz, Aysegul and Hamilton, Liberty and Honey, Christopher J. and Jas, Mainak and Knight, Robert and Lachaux, Jean-Philippe and Lau, Jonathan C. and Lee-Messer, Christopher and Lundstrom, Brian N. and Miller, Kai J. and Ojemann, Jeffrey G. and Oostenveld, Robert and Petridou, Natalia and Piantoni, Gio and Pigorini, Andrea and Pouratian, Nader and Ramsey, Nick F. and Stolk, Arjen and Swann, Nicole C. and Tadel, François and Voytek, Bradley and Wandell, Brian A. and Winawer, Jonathan and Whitaker, Kirstie and Zehl, Lyuba and Hermes, Dora},
	month = jun,
	year = {2019},
	keywords = {BIDS, methods, deepmeg.datapaper},
	pages = {1--6},
}

@book{buzsaki_brain_2019,
	address = {New York, NY},
	title = {The brain from inside out},
	isbn = {978-0-19-090538-5},
	publisher = {Oxford University Press},
	author = {Buzsáki, György},
	year = {2019},
	file = {Buzsáki - 2019 - The Brain from Inside Out.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Buzsáki - 2019 - The Brain from Inside Out.pdf:application/pdf},
}

@book{willems_cognitive_2015,
	address = {Cambridge, United Kingdom},
	title = {Cognitive neuroscience of natural language use},
	isbn = {978-1-107-04201-8},
	abstract = {"When we think of everyday language use, the first things that come to mind include colloquial conversations, reading and writing e-mails, sending text messages or reading a book. But can we study the brain basis of language as we use it in our daily lives? As a topic of study, the cognitive neuroscience of language is far removed from these language-in-use examples. However, recent developments in research and technology have made studying the neural underpinnings of naturally occurring language much more feasible. In this book a range of international experts provide a state-of-the-art overview of current approaches to making the cognitive neuroscience of language more 'natural' and closer to language use as it occurs in real life. The chapters explore topics including discourse comprehension, the study of dialogue, literature comprehension and the insights gained from looking at natural speech in neuropsychology"--Provided by publisher},
	publisher = {Cambridge University Press},
	editor = {Willems, Roel M.},
	year = {2015},
	keywords = {deepmeg.datapaper, naturalistic cognitive neuroscience},
}

@article{matusz_are_2018,
	title = {Are we ready for real-world neuroscience?},
	volume = {31},
	issn = {0898-929X},
	url = {https://doi.org/10.1162/jocn_e_01276},
	doi = {10.1162/jocn_e_01276},
	abstract = {Real-world environments are typically dynamic, complex, and multisensory in nature and require the support of top–down attention and memory mechanisms for us to be able to drive a car, make a shopping list, or pour a cup of coffee. Fundamental principles of perception and functional brain organization have been established by research utilizing well-controlled but simplified paradigms with basic stimuli. The last 30 years ushered a revolution in computational power, brain mapping, and signal processing techniques. Drawing on those theoretical and methodological advances, over the years, research has departed more and more from traditional, rigorous, and well-understood paradigms to directly investigate cognitive functions and their underlying brain mechanisms in real-world environments. These investigations typically address the role of one or, more recently, multiple attributes of real-world environments. Fundamental assumptions about perception, attention, or brain functional organization have been challenged—by studies adapting the traditional paradigms to emulate, for example, the multisensory nature or varying relevance of stimulation or dynamically changing task demands. Here, we present the state of the field within the emerging heterogeneous domain of real-world neuroscience. To be precise, the aim of this Special Focus is to bring together a variety of the emerging “real-world neuroscientific” approaches. These approaches differ in their principal aims, assumptions, or even definitions of “real-world neuroscience” research. Here, we showcase the commonalities and distinctive features of the different “real-world neuroscience” approaches. To do so, four early-career researchers and the speakers of the Cognitive Neuroscience Society 2017 Meeting symposium under the same title answer questions pertaining to the added value of such approaches in bringing us closer to accurate models of functional brain organization and cognitive functions.},
	number = {3},
	urldate = {2020-01-29},
	journal = {Journal of Cognitive Neuroscience},
	author = {Matusz, Pawel J. and Dikker, Suzanne and Huth, Alexander G. and Perrodin, Catherine},
	month = jun,
	year = {2018},
	keywords = {deepmeg.datapaper},
	pages = {327--338},
}

@article{hamilton_revolution_2018,
	title = {The revolution will not be controlled: natural stimuli in speech neuroscience},
	volume = {0},
	issn = {2327-3798},
	shorttitle = {The revolution will not be controlled},
	url = {https://doi.org/10.1080/23273798.2018.1499946},
	doi = {10.1080/23273798.2018.1499946},
	abstract = {Humans have a unique ability to produce and consume rich, complex, and varied language in order to communicate ideas to one another. Still, outside of natural reading, the most common methods for studying how our brains process speech or understand language use only isolated words or simple sentences. Recent studies have upset this status quo by employing complex natural stimuli and measuring how the brain responds to language as it is used. In this article we argue that natural stimuli offer many advantages over simplified, controlled stimuli for studying how language is processed by the brain. Furthermore, the downsides of using natural language stimuli can be mitigated using modern statistical and computational techniques.},
	number = {0},
	urldate = {2020-01-29},
	journal = {Language, Cognition and Neuroscience},
	author = {Hamilton, Liberty S. and Huth, Alexander G.},
	month = jul,
	year = {2018},
	keywords = {fMRI, naturalistic cognitive neuroscience},
	pages = {1--10},
}

@article{kim_simple_2019,
	title = {Simple framework for constructing functional spiking recurrent neural networks},
	volume = {116},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1905926116},
	doi = {10.1073/pnas.1905926116},
	abstract = {Cortical microcircuits exhibit complex recurrent architectures that possess dynamically rich properties. The neurons that make up these microcircuits communicate mainly via discrete spikes, and it is not clear how spikes give rise to dynamics that can be used to perform computationally challenging tasks. In contrast, continuous models of rate-coding neurons can be trained to perform complex tasks. Here, we present a simple framework to construct biologically realistic spiking recurrent neural networks (RNNs) capable of learning a wide range of tasks. Our framework involves training a continuous-variable rate RNN with important biophysical constraints and transferring the learned dynamics and constraints to a spiking RNN in a one-to-one manner. The proposed framework introduces only 1 additional parameter to establish the equivalence between rate and spiking RNN models. We also study other model parameters related to the rate and spiking networks to optimize the one-to-one mapping. By establishing a close relationship between rate and spiking models, we demonstrate that spiking RNNs could be constructed to achieve similar performance as their counterpart continuous rate networks.},
	language = {en},
	number = {45},
	urldate = {2020-01-27},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Kim, Robert and Li, Yinghao and Sejnowski, Terrence J.},
	month = nov,
	year = {2019},
	keywords = {RNN, backprop, spiking neural networks, SNN, proposal.SanD},
	pages = {22811--22820},
}

@article{hagoort_neurobiology_2019,
	title = {The neurobiology of language beyond single-word processing},
	volume = {366},
	issn = {0036-8075, 1095-9203},
	url = {http://www.sciencemag.org/lookup/doi/10.1126/science.aax0289},
	doi = {10.1126/science.aax0289},
	abstract = {In this Review, I propose a multiple-network view for the neurobiological basis of distinctly human language skills. A much more complex picture of interacting brain areas emerges than in the classical neurobiological model of language. This is because using language is more than single-word processing, and much goes on beyond the information given in the acoustic or orthographic tokens that enter primary sensory cortices. This requires the involvement of multiple networks with functionally nonoverlapping contributions.},
	language = {en},
	number = {6461},
	urldate = {2020-01-27},
	journal = {Science},
	author = {Hagoort, Peter},
	month = oct,
	year = {2019},
	keywords = {cognitive neuroscience, language},
	pages = {55--58},
}

@article{yi_encoding_2019,
	title = {The encoding of speech sounds in the superior temporal gyrus},
	volume = {102},
	issn = {0896-6273},
	url = {http://www.sciencedirect.com/science/article/pii/S0896627319303800},
	doi = {10.1016/j.neuron.2019.04.023},
	abstract = {The human superior temporal gyrus (STG) is critical for extracting meaningful linguistic features from speech input. Local neural populations are tuned to acoustic-phonetic features of all consonants and vowels and to dynamic cues for intonational pitch. These populations are embedded throughout broader functional zones that are sensitive to amplitude-based temporal cues. Beyond speech features, STG representations are strongly modulated by learned knowledge and perceptual goals. Currently, a major challenge is to understand how these features are integrated across space and time in the brain during natural speech comprehension. We present a theory that temporally recurrent connections within STG generate context-dependent phonological representations, spanning longer temporal sequences relevant for coherent percepts of syllables, words, and phrases.},
	language = {en},
	number = {6},
	urldate = {2020-01-19},
	journal = {Neuron},
	author = {Yi, Han Gyol and Leonard, Matthew K. and Chang, Edward F.},
	month = jun,
	year = {2019},
	keywords = {readme, ECoG, proposal.SanD, phonetics},
	pages = {1096--1110},
}

@book{hickok_neurobiology_2016,
	address = {Amsterdam ; Boston},
	title = {Neurobiology of language},
	isbn = {978-0-12-407794-2},
	abstract = {"The present volume not only provides a comprehensive guide to the state-of-the-art in research on neurobiology but also frames the major debates of our time in psychology and neuroscience more broadly. To what extent is the mind/brain embodied? What is the nature of neural computation? How do apparently diverse brain systems - perception, action, memory, attention - interact? Are they really as diverse as we once thought? How do neural systems break down and can they be repaired?"--},
	publisher = {Elsevier/AP, Academic Press is an imprint of Elsevier},
	editor = {Hickok, Gregory and Small, Steven Lawrence},
	year = {2016},
	note = {OCLC: ocn921978691},
}

@article{ball_signal_2009,
	title = {Signal quality of simultaneously recorded invasive and non-invasive {EEG}},
	volume = {46},
	issn = {10538119},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1053811909001827},
	doi = {10.1016/j.neuroimage.2009.02.028},
	language = {en},
	number = {3},
	urldate = {2020-01-26},
	journal = {NeuroImage},
	author = {Ball, Tonio and Kern, Markus and Mutschler, Isabella and Aertsen, Ad and Schulze-Bonhage, Andreas},
	month = jul,
	year = {2009},
	keywords = {EEG, methods, ECoG, proposal.SanD, artifacts, iEEG, SNR},
	pages = {708--716},
}

@article{parvizi_promises_2018,
	title = {Promises and limitations of human intracranial electroencephalography},
	volume = {21},
	issn = {1097-6256, 1546-1726},
	url = {http://www.nature.com/articles/s41593-018-0108-2},
	doi = {10.1038/s41593-018-0108-2},
	language = {en},
	number = {4},
	urldate = {2020-01-26},
	journal = {Nature Neuroscience},
	author = {Parvizi, Josef and Kastner, Sabine},
	month = apr,
	year = {2018},
	keywords = {methods, ECoG, review, proposal.SanD},
	pages = {474--483},
}

@article{ferguson_mechanisms_2020,
	title = {Mechanisms underlying gain modulation in the cortex},
	copyright = {2020 Springer Nature Limited},
	issn = {1471-0048},
	url = {http://www.nature.com/articles/s41583-019-0253-y},
	doi = {10.1038/s41583-019-0253-y},
	abstract = {Changes in cortical gain enable neurons to respond adaptively to changing inputs. In this Review, Ferguson and Cardin describe the mechanisms that modulate cortical gain, and its effects on and relevance for cognition and behaviour.},
	language = {en},
	urldate = {2020-01-19},
	journal = {Nature Reviews Neuroscience},
	author = {Ferguson, Katie A. and Cardin, Jessica A.},
	month = jan,
	year = {2020},
	keywords = {readme, proposal.SanD, gain modulation},
	pages = {1--13},
}

@article{martin_modelling_2020,
	title = {Modelling meaning composition from formalism to mechanism},
	volume = {375},
	url = {https://royalsocietypublishing.org/doi/10.1098/rstb.2019.0298},
	doi = {10.1098/rstb.2019.0298},
	abstract = {Human thought and language have extraordinary expressive power because meaningful parts can be assembled into more complex semantic structures. This partly underlies our ability to compose meanings into endlessly novel configurations, and sets us apart from other species and current computing devices. Crucially, human behaviour, including language use and linguistic data, indicates that composing parts into complex structures does not threaten the existence of constituent parts as independent units in the system: parts and wholes exist simultaneously yet independently from one another in the mind and brain. This independence is evident in human behaviour, but it seems at odds with what is known about the brain's exquisite sensitivity to statistical patterns: everyday language use is productive and expressive precisely because it can go beyond statistical regularities. Formal theories in philosophy and linguistics explain this fact by assuming that language and thought are compositional: systems of representations that separate a variable (or role) from its values (fillers), such that the meaning of a complex expression is a function of the values assigned to the variables. The debate on whether and how compositional systems could be implemented in minds, brains and machines remains vigorous. However, it has not yet resulted in mechanistic models of semantic composition: how, then, are the constituents of thoughts and sentences put and held together? We review and discuss current efforts at understanding this problem, and we chart possible routes for future research.This article is part of the theme issue ‘Towards mechanistic models of meaning composition’.},
	number = {1791},
	urldate = {2020-01-19},
	journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	author = {Martin, Andrea E. and Baggio, Giosuè},
	month = feb,
	year = {2020},
	pages = {20190298},
}

@article{friederici_language_2017,
	title = {Language, mind and brain},
	volume = {1},
	copyright = {2017 The Publisher},
	issn = {2397-3374},
	url = {http://www.nature.com/articles/s41562-017-0184-4},
	doi = {10.1038/s41562-017-0184-4},
	abstract = {Friederici et al. outline a view of the neural organization of language that is compatible with a description of language as a biologically determined computational mechanism that yields an infinite number of hierarchically structured expressions.},
	language = {en},
	number = {10},
	urldate = {2020-01-19},
	journal = {Nature Human Behaviour},
	author = {Friederici, Angela D. and Chomsky, Noam and Berwick, Robert C. and Moro, Andrea and Bolhuis, Johan J.},
	month = oct,
	year = {2017},
	keywords = {readme, theory},
	pages = {713--722},
}

@article{sinz_engineering_2019,
	title = {Engineering a less artificial intelligence},
	volume = {103},
	issn = {08966273},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627319307408},
	doi = {10.1016/j.neuron.2019.08.034},
	language = {en},
	number = {6},
	urldate = {2020-01-19},
	journal = {Neuron},
	author = {Sinz, Fabian H. and Pitkow, Xaq and Reimer, Jacob and Bethge, Matthias and Tolias, Andreas S.},
	month = sep,
	year = {2019},
	keywords = {readme, proposal.SanD},
	pages = {967--979},
}

@article{cybenko_approximation_1989,
	title = {Approximation by superpositions of a sigmoidal function},
	volume = {2},
	issn = {0932-4194, 1435-568X},
	url = {http://link.springer.com/10.1007/BF02551274},
	doi = {10.1007/BF02551274},
	language = {en},
	number = {4},
	urldate = {2020-01-19},
	journal = {Mathematics of Control, Signals, and Systems},
	author = {Cybenko, G.},
	month = dec,
	year = {1989},
	keywords = {readme, neural networks, machine learning, universal approximation},
	pages = {303--314},
}

@article{geman_neural_1992,
	title = {Neural networks and the bias/variance dilemma},
	volume = {4},
	issn = {0899-7667, 1530-888X},
	doi = {10.1162/neco.1992.4.1.1},
	language = {en},
	number = {1},
	urldate = {2018-06-01},
	journal = {Neural Computation},
	author = {Geman, Stuart and Bienenstock, Elie and Doursat, René},
	month = jan,
	year = {1992},
	keywords = {project.lstmMEG, readme, statistics, neural networks, statistical learning, bias-variance dilemma},
	pages = {1--58},
	file = {Geman et al. - 1992 - Neural Networks and the biasvariance dilemma.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Geman et al. - 1992 - Neural Networks and the biasvariance dilemma.pdf:application/pdf},
}

@article{frith_fast_2020,
	title = {Fast lane to slow science},
	volume = {24},
	issn = {13646613},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661319302426},
	doi = {10.1016/j.tics.2019.10.007},
	language = {en},
	number = {1},
	urldate = {2020-01-13},
	journal = {Trends in Cognitive Sciences},
	author = {Frith, Uta},
	month = jan,
	year = {2020},
	pages = {1--2},
	file = {Full Text:/Users/kriarm/Zotero/storage/BANC4MJX/Frith - 2020 - Fast Lane to Slow Science.pdf:application/pdf},
}

@article{holcombe_farewell_2019,
	title = {Farewell authors, hello contributors},
	volume = {571},
	copyright = {2019 Nature},
	url = {https://www.nature.com/articles/d41586-019-02084-8},
	doi = {10.1038/d41586-019-02084-8},
	abstract = {More disciplines must embrace a system of academic credit that rewards a greater range of roles more specifically, says Alex Holcombe.},
	language = {en},
	number = {7764},
	urldate = {2020-01-13},
	journal = {Nature},
	author = {Holcombe, Alex},
	month = jul,
	year = {2019},
	keywords = {open science},
	pages = {147--147},
}

@article{chien_constructing_2019,
	title = {Constructing and forgetting temporal context in the human cerebral cortex},
	copyright = {© 2019, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/761593v1},
	doi = {10.1101/761593},
	abstract = {{\textless}h3{\textgreater}Summary{\textless}/h3{\textgreater} {\textless}p{\textgreater}How does information from seconds earlier affect neocortical responses to new input? Here, we used empirical measurements and computational modeling to study the integration and forgetting of prior information. We found that when two groups of participants heard the same sentence in a narrative, preceded by different contexts, the neural responses of each group were initially different, but gradually fell into alignment. We observed a hierarchical gradient: sensory cortices aligned most quickly, followed by mid-level regions, while higher-order cortical regions aligned last. In some higher order regions, responses to the same sentence took more than 10 seconds to align. What kinds of computations can explain this hierarchical organization of contextual alignment? Passive linear integration models predict that regions which are slower to integrate new information should also be slower to forget old information. However, we found that higher order regions could rapidly forget prior context. The data were better captured by a model composed of hierarchical autoencoders in time (HAT). In HAT, cortical regions maintain a temporal context representation which is actively integrated with input at each moment, and this integration is gated by prediction error. These data and models suggest that sequences of information are combined throughout the cortical hierarchy using an active and gated integration process.{\textless}/p{\textgreater}},
	language = {en},
	urldate = {2020-01-04},
	journal = {bioRxiv},
	author = {Chien, Hsiang-Yun Sherry and Honey, Christopher J.},
	month = sep,
	year = {2019},
	keywords = {LSTM, project.lstmMEG, TRW, gating},
	pages = {761593},
}

@article{kim_spiking_2019,
	title = {Spiking recurrent networks as a model to probe neuronal timescales specific to working memory},
	copyright = {© 2019, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution-NonCommercial-NoDerivs 4.0 International), CC BY-NC-ND 4.0, as described at http://creativecommons.org/licenses/by-nc-nd/4.0/},
	url = {https://www.biorxiv.org/content/10.1101/842302v1},
	doi = {10.1101/842302},
	abstract = {{\textless}h3{\textgreater}Abstract{\textless}/h3{\textgreater} {\textless}p{\textgreater}Cortical neurons process and integrate information on multiple timescales. In addition, these timescales or temporal receptive fields display functional and hierarchical organization. For instance, areas important for working memory (WM), such as prefrontal cortex, utilize neurons with stable temporal receptive fields and long timescales to support reliable representations of stimuli. Despite of the recent advances in experimental techniques, the underlying mechanisms for the emergence of neuronal timescales long enough to support WM are unclear and challenging to investigate experimentally. Here, we demonstrate that spiking recurrent neural networks (RNNs) designed to perform a WM task reproduce previously observed experimental findings and that these models could be utilized in the future to study how neuronal timescales specific to WM emerge.{\textless}/p{\textgreater}},
	language = {en},
	urldate = {2020-01-04},
	journal = {bioRxiv},
	author = {Kim, Robert and Sejnowski, Terrence J.},
	month = nov,
	year = {2019},
	keywords = {working memory, spiking neural networks},
	pages = {842302},
	file = {Kim and Sejnowski - 2019 - Spiking recurrent networks as a model to probe neu.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Kim and Sejnowski - 2019 - Spiking recurrent networks as a model to probe neu.pdf:application/pdf},
}

@article{Hunter:2007,
	title = {Matplotlib: {A} {2D} graphics environment},
	volume = {9},
	doi = {10.1109/MCSE.2007.55},
	abstract = {Matplotlib is a 2D graphics package used for Python for application development, interactive scripting, and publication-quality image generation across user interfaces and operating systems.},
	number = {3},
	journal = {Computing in Science \& Engineering},
	author = {Hunter, J. D.},
	year = {2007},
	note = {tex.publisher: IEEE COMPUTER SOC},
	keywords = {open science, dataviz, open source},
	pages = {90--95},
}

@article{sholler_resistance_2019,
	title = {Resistance to adoption of best practices},
	doi = {10.31235/osf.io/qr8cz},
	abstract = {There are many recommendations of "best practices" for those doing data science, data-intensive research, and research in general. These documents usually present a particular vision of how people should work with data and computing, recommending specific tools, activities, mechanisms, and sensibilities. However, implementation of best (or better) practices in any setting is often met with resistance from individuals and groups, who perceive some drawbacks to the proposed changes to everyday practice. We offer some definitions of resistance, identify the sources of researchers' hesitancy to adopt new ways of working, and describe some of the ways resistance is manifested in data science teams. We then offer strategies for overcoming resistance based on our group members' experiences working alongside resistors or resisting change themselves. Our discussion concluded with many remaining questions left to tackle, some of which are listed at the end of this piece.},
	urldate = {2019-12-08},
	journal = {SocArXiv},
	author = {Sholler, Dan and Stoudt, Sara and Kennedy, Chris J. and Hoces de la Guardia, Fernando and Lanusse, Francois and Ram, Karthik and Ottoboni, Kellie and Stuart, Marla and Vareth, Maryam and Varoquaux, Nelle and Barter, Rebecca and Geiger, R. Stuart and Peterson, Scott and van der Walt, Stefan},
	month = mar,
	year = {2019},
	keywords = {open science, read, culture change},
}

@article{nowogrodzki_how_2019,
	title = {How to support open-source software and stay sane},
	volume = {571},
	copyright = {2019 Nature},
	url = {http://www.nature.com/articles/d41586-019-02046-0},
	doi = {10.1038/d41586-019-02046-0},
	abstract = {Releasing lab-built open-source software often involves a mountain of unforeseen work for the developers.},
	language = {en},
	number = {7763},
	urldate = {2019-12-08},
	journal = {Nature},
	author = {Nowogrodzki, Anna},
	month = jul,
	year = {2019},
	keywords = {open science, read, open source},
	pages = {133--134},
}

@incollection{bartling_open_2014,
	address = {Cham},
	title = {Open science: {One} term, five schools of thought},
	isbn = {978-3-319-00025-1 978-3-319-00026-8},
	shorttitle = {Open {Science}},
	url = {http://link.springer.com/10.1007/978-3-319-00026-8_2},
	language = {en},
	urldate = {2019-12-06},
	booktitle = {Opening {Science}},
	publisher = {Springer International Publishing},
	author = {Fecher, Benedikt and Friesike, Sascha},
	editor = {Bartling, Sönke and Friesike, Sascha},
	year = {2014},
	doi = {10.1007/978-3-319-00026-8_2},
	pages = {17--47},
}

@article{sandve_ten_2013,
	title = {Ten simple rules for reproducible computational research},
	volume = {9},
	issn = {1553-7358},
	url = {https://dx.plos.org/10.1371/journal.pcbi.1003285},
	doi = {10.1371/journal.pcbi.1003285},
	language = {en},
	number = {10},
	urldate = {2019-12-06},
	journal = {PLoS Computational Biology},
	author = {Sandve, Geir Kjetil and Nekrutenko, Anton and Taylor, James and Hovig, Eivind},
	editor = {Bourne, Philip E.},
	month = oct,
	year = {2013},
	keywords = {open science, reproducibility, readme},
	pages = {e1003285},
}

@article{sandve_ten_2013-1,
	title = {Ten {Simple} {Rules} for {Reproducible} {Computational} {Research}},
	volume = {9},
	issn = {1553-7358},
	url = {https://dx.plos.org/10.1371/journal.pcbi.1003285},
	doi = {10.1371/journal.pcbi.1003285},
	language = {en},
	number = {10},
	urldate = {2019-12-06},
	journal = {PLoS Computational Biology},
	author = {Sandve, Geir Kjetil and Nekrutenko, Anton and Taylor, James and Hovig, Eivind},
	editor = {Bourne, Philip E.},
	month = oct,
	year = {2013},
	pages = {e1003285},
	file = {Full Text:/Users/kriarm/Zotero/storage/PNGZTZ99/Sandve et al. - 2013 - Ten Simple Rules for Reproducible Computational Re.pdf:application/pdf},
}

@article{kidwell_badges_2016,
	title = {Badges to acknowledge open practices: {A} simple, low-cost, effective method for increasing transparency},
	volume = {14},
	issn = {1545-7885},
	shorttitle = {Badges to {Acknowledge} {Open} {Practices}},
	url = {https://dx.plos.org/10.1371/journal.pbio.1002456},
	doi = {10.1371/journal.pbio.1002456},
	language = {en},
	number = {5},
	urldate = {2019-12-06},
	journal = {PLOS Biology},
	author = {Kidwell, Mallory C. and Lazarević, Ljiljana B. and Baranski, Erica and Hardwicke, Tom E. and Piechowski, Sarah and Falkenberg, Lina-Sophia and Kennett, Curtis and Slowik, Agnieszka and Sonnleitner, Carina and Hess-Holden, Chelsey and Errington, Timothy M. and Fiedler, Susann and Nosek, Brian A.},
	editor = {Macleod, Malcolm R},
	month = may,
	year = {2016},
	keywords = {open science, readme, badges, incentive structure},
	pages = {e1002456},
}

@article{cook_promoting_2018,
	title = {Promoting {Open} {Science} to increase the trustworthiness of evidence in special education},
	volume = {85},
	issn = {0014-4029, 2163-5560},
	url = {http://journals.sagepub.com/doi/10.1177/0014402918793138},
	doi = {10.1177/0014402918793138},
	language = {en},
	number = {1},
	urldate = {2019-12-03},
	journal = {Exceptional Children},
	author = {Cook, Bryan G. and Lloyd, John Wills and Mellor, David and Nosek, Brian A. and Therrien, William J.},
	month = oct,
	year = {2018},
	keywords = {open science, education, public trust},
	pages = {104--118},
}

@article{jamieson_signaling_2019,
	title = {Signaling the trustworthiness of science},
	volume = {116},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1913039116},
	doi = {10.1073/pnas.1913039116},
	abstract = {Trust in science increases when scientists and the outlets certifying their work honor science’s norms. Scientists often fail to signal to other scientists and, perhaps more importantly, the public that these norms are being upheld. They could do so as they generate, certify, and react to each other’s findings: for example, by promoting the use and value of evidence, transparent reporting, self-correction, replication, a culture of critique, and controls for bias. A number of approaches for authors and journals would lead to more effective signals of trustworthiness at the article level. These include article badging, checklists, a more extensive withdrawal ontology, identity verification, better forward linking, and greater transparency.},
	language = {en},
	number = {39},
	urldate = {2019-12-03},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Jamieson, Kathleen Hall and McNutt, Marcia and Kiermer, Veronique and Sever, Richard},
	month = sep,
	year = {2019},
	keywords = {open science, readme},
	pages = {19231--19236},
	file = {Full Text:/Users/kriarm/Zotero/storage/RXZE4VWZ/Jamieson et al. - 2019 - Signaling the trustworthiness of science.pdf:application/pdf},
}

@article{stall_make_2019,
	title = {Make scientific data {FAIR}},
	volume = {570},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/d41586-019-01720-7},
	doi = {10.1038/d41586-019-01720-7},
	language = {en},
	number = {7759},
	urldate = {2019-12-03},
	journal = {Nature},
	author = {Stall, Shelley and Yarmey, Lynn and Cutcher-Gershenfeld, Joel and Hanson, Brooks and Lehnert, Kerstin and Nosek, Brian and Parsons, Mark and Robinson, Erin and Wyborn, Lesley},
	month = jun,
	year = {2019},
	keywords = {open science, readme, data sharing, FAIR},
	pages = {27--29},
}

@article{nosek_preregistration_2018,
	title = {The preregistration revolution},
	volume = {115},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1708274114},
	doi = {10.1073/pnas.1708274114},
	abstract = {Progress in science relies in part on generating hypotheses with existing observations and testing hypotheses with new observations. This distinction between postdiction and prediction is appreciated conceptually but is not respected in practice. Mistaking generation of postdictions with testing of predictions reduces the credibility of research findings. However, ordinary biases in human reasoning, such as hindsight bias, make it hard to avoid this mistake. An effective solution is to define the research questions and analysis plan before observing the research outcomes—a process called preregistration. Preregistration distinguishes analyses and outcomes that result from predictions from those that result from postdictions. A variety of practical strategies are available to make the best possible use of preregistration in circumstances that fall short of the ideal application, such as when the data are preexisting. Services are now available for preregistration across all disciplines, facilitating a rapid increase in the practice. Widespread adoption of preregistration will increase distinctiveness between hypothesis generation and hypothesis testing and will improve the credibility of research findings.},
	language = {en},
	number = {11},
	urldate = {2019-12-03},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Nosek, Brian A. and Ebersole, Charles R. and DeHaven, Alexander C. and Mellor, David T.},
	month = mar,
	year = {2018},
	keywords = {open science, readme, preregistration},
	pages = {2600--2606},
	file = {Full Text:/Users/kriarm/Zotero/storage/TVEQBGBP/Nosek et al. - 2018 - The preregistration revolution.pdf:application/pdf},
}

@article{munafo_manifesto_2017,
	title = {A manifesto for reproducible science},
	volume = {1},
	copyright = {2017 Macmillan Publishers Limited},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-016-0021;},
	doi = {10.1038/s41562-016-0021},
	abstract = {Leading voices in the reproducibility landscape call for the adoption of measures to optimize key elements of the scientific process.},
	language = {en},
	number = {1},
	urldate = {2019-12-03},
	journal = {Nature Human Behaviour},
	author = {Munafò, Marcus R. and Nosek, Brian A. and Bishop, Dorothy V. M. and Button, Katherine S. and Chambers, Christopher D. and Sert, Nathalie Percie du and Simonsohn, Uri and Wagenmakers, Eric-Jan and Ware, Jennifer J. and Ioannidis, John P. A.},
	month = jan,
	year = {2017},
	keywords = {open science, reproducibility},
	pages = {1--9},
}

@article{nosek_promoting_2015,
	title = {Promoting an open research culture},
	volume = {348},
	copyright = {Copyright © 2015, American Association for the Advancement of Science},
	issn = {0036-8075, 1095-9203},
	url = {https://science.sciencemag.org/content/348/6242/1422},
	doi = {10.1126/science.aab2374},
	abstract = {Author guidelines for journals could help to promote transparency, openness, and reproducibility
Author guidelines for journals could help to promote transparency, openness, and reproducibility},
	language = {en},
	number = {6242},
	urldate = {2019-12-03},
	journal = {Science},
	author = {Nosek, B. A. and Alter, G. and Banks, G. C. and Borsboom, D. and Bowman, S. D. and Breckler, S. J. and Buck, S. and Chambers, C. D. and Chin, G. and Christensen, G. and Contestabile, M. and Dafoe, A. and Eich, E. and Freese, J. and Glennerster, R. and Goroff, D. and Green, D. P. and Hesse, B. and Humphreys, M. and Ishiyama, J. and Karlan, D. and Kraut, A. and Lupia, A. and Mabry, P. and Madon, T. and Malhotra, N. and Mayo-Wilson, E. and McNutt, M. and Miguel, E. and Paluck, E. Levy and Simonsohn, U. and Soderberg, C. and Spellman, B. A. and Turitto, J. and VandenBos, G. and Vazire, S. and Wagenmakers, E. J. and Wilson, R. and Yarkoni, T.},
	month = jun,
	year = {2015},
	pmid = {26113702},
	keywords = {open science, readme},
	pages = {1422--1425},
}

@article{perkel_make_2019,
	title = {Make code accessible with these cloud services},
	volume = {575},
	copyright = {2019 Nature},
	url = {https://www.nature.com/articles/d41586-019-03366-x},
	doi = {10.1038/d41586-019-03366-x},
	abstract = {Container platforms let researchers run each other’s software — and check the results.},
	language = {en},
	urldate = {2019-12-03},
	journal = {Nature},
	author = {Perkel, Jeffrey M.},
	month = nov,
	year = {2019},
	keywords = {open science, reproducibility, read, cloud computing, scientific computing},
	pages = {247--248},
}

@article{madsen_visualizing_2019,
	title = {Visualizing memorization in {RNNs}},
	volume = {4},
	issn = {2476-0757},
	url = {https://distill.pub/2019/memorization-in-rnns},
	doi = {10.23915/distill.00016},
	abstract = {Inspecting gradient magnitudes in context can be a powerful tool to see when recurrent units use short-term or long-term contextual understanding.},
	language = {en},
	number = {3},
	urldate = {2019-11-26},
	journal = {Distill},
	author = {Madsen, Andreas},
	month = mar,
	year = {2019},
	pages = {e16},
	file = {Snapshot:/Users/kriarm/Zotero/storage/6B3RNAST/memorization-in-rnns.html:text/html},
}

@article{voytek_social_2017,
	title = {Social {Media}, {Open} {Science}, and {Data} {Science} {Are} {Inextricably} {Linked}},
	volume = {96},
	issn = {0896-6273},
	url = {https://www.cell.com/neuron/abstract/S0896-6273(17)31068-1},
	doi = {10.1016/j.neuron.2017.11.015},
	language = {English},
	number = {6},
	urldate = {2019-11-24},
	journal = {Neuron},
	author = {Voytek, Bradley},
	month = dec,
	year = {2017},
	pmid = {29224728},
	pages = {1219--1222},
}

@article{lyu_neural_2019,
	title = {Neural dynamics of semantic composition},
	volume = {116},
	copyright = {Copyright © 2019 the Author(s). Published by PNAS.. https://creativecommons.org/licenses/by/4.0/This open access article is distributed under Creative Commons Attribution License 4.0 (CC BY).},
	issn = {0027-8424, 1091-6490},
	url = {https://www.pnas.org/content/116/42/21318},
	doi = {10.1073/pnas.1903402116},
	abstract = {Human speech comprehension is remarkable for its immediacy and rapidity. The listener interprets an incrementally delivered auditory input, millisecond by millisecond as it is heard, in terms of complex multilevel representations of relevant linguistic and nonlinguistic knowledge. Central to this process are the neural computations involved in semantic combination, whereby the meanings of words are combined into more complex representations, as in the combination of a verb and its following direct object (DO) noun (e.g., “eat the apple”). These combinatorial processes form the backbone for incremental interpretation, enabling listeners to integrate the meaning of each word as it is heard into their dynamic interpretation of the current utterance. Focusing on the verb-DO noun relationship in simple spoken sentences, we applied multivariate pattern analysis and computational semantic modeling to source-localized electro/magnetoencephalographic data to map out the specific representational constraints that are constructed as each word is heard, and to determine how these constraints guide the interpretation of subsequent words in the utterance. Comparing context-independent semantic models of the DO noun with contextually constrained noun models reflecting the semantic properties of the preceding verb, we found that only the contextually constrained model showed a significant fit to the brain data. Pattern-based measures of directed connectivity across the left hemisphere language network revealed a continuous information flow among temporal, inferior frontal, and inferior parietal regions, underpinning the verb’s modification of the DO noun’s activated semantics. These results provide a plausible neural substrate for seamless real-time incremental interpretation on the observed millisecond time scales.},
	language = {en},
	number = {42},
	urldate = {2019-11-07},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Lyu, Bingjiang and Choi, Hun S. and Marslen-Wilson, William D. and Clarke, Alex and Randall, Billi and Tyler, Lorraine K.},
	month = oct,
	year = {2019},
	pmid = {31570590},
	keywords = {MEG, NLP, speech comprehension, semantics, topic modelling},
	pages = {21318--21327},
}

@inproceedings{paszke_automatic_2017,
	title = {Automatic differentiation in {PyTorch}},
	booktitle = {{NIPS} autodiff workshop},
	author = {Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},
	year = {2017},
	keywords = {deep learning, methods, autograd, pyTorch},
}

@article{anumanchipalli_speech_2019,
	title = {Speech synthesis from neural decoding of spoken sentences},
	volume = {568},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/s41586-019-1119-1},
	doi = {10.1038/s41586-019-1119-1},
	language = {en},
	number = {7753},
	urldate = {2019-11-08},
	journal = {Nature},
	author = {Anumanchipalli, Gopala K. and Chartier, Josh and Chang, Edward F.},
	month = apr,
	year = {2019},
	keywords = {LSTM, RNN, readme, decoding, reading, ECoG, BCI, paralysis},
	pages = {493--498},
}

@article{richards_deep_2019,
	title = {A deep learning framework for neuroscience},
	volume = {22},
	copyright = {2019 Springer Nature America, Inc.},
	issn = {1546-1726},
	url = {https://www.nature.com/articles/s41593-019-0520-2},
	doi = {10.1038/s41593-019-0520-2},
	abstract = {A deep network is best understood in terms of components used to design it—objective functions, architecture and learning rules—rather than unit-by-unit computation. Richards et al. argue that this inspires fruitful approaches to systems neuroscience.},
	language = {en},
	number = {11},
	urldate = {2019-11-04},
	journal = {Nature Neuroscience},
	author = {Richards, Blake A. and Lillicrap, Timothy P. and Beaudoin, Philippe and Bengio, Yoshua and Bogacz, Rafal and Christensen, Amelia and Clopath, Claudia and Costa, Rui Ponte and Berker, Archy de and Ganguli, Surya and Gillon, Colleen J. and Hafner, Danijar and Kepecs, Adam and Kriegeskorte, Nikolaus and Latham, Peter and Lindsay, Grace W. and Miller, Kenneth D. and Naud, Richard and Pack, Christopher C. and Poirazi, Panayiota and Roelfsema, Pieter and Sacramento, João and Saxe, Andrew and Scellier, Benjamin and Schapiro, Anna C. and Senn, Walter and Wayne, Greg and Yamins, Daniel and Zenke, Friedemann and Zylberberg, Joel and Therien, Denis and Kording, Konrad P.},
	month = nov,
	year = {2019},
	pages = {1761--1770},
	file = {Richards et al. - 2019 - A deep learning framework for neuroscience.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Richards et al. - 2019 - A deep learning framework for neuroscience.pdf:application/pdf},
}

@article{wolf_hugging_2019,
	title = {{HuggingFace}'s transformers: state-of-the-art natural language processing},
	volume = {abs/1910.03771},
	journal = {ArXiv},
	author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R'emi and Funtowicz, Morgan and Brew, Jamie},
	year = {2019},
	keywords = {NLP, BERT, methods},
}

@book{shorto_island_2005,
	address = {New York},
	title = {The island at the center of the world},
	isbn = {978-1-4000-7867-7},
	language = {English},
	publisher = {Vintage Books},
	author = {Shorto, Russell},
	year = {2005},
	note = {OCLC: 999571231},
	keywords = {history, New Amsterdam, New York, the Netherlands},
}

@article{petriglieri_thriving_2018,
	title = {Thriving in the gig economy},
	issn = {0017-8012},
	url = {https://hbr.org/2018/03/thriving-in-the-gig-economy},
	abstract = {How successful freelancers manage the uncertainty},
	number = {March–April 2018},
	urldate = {2019-10-19},
	journal = {Harvard Business Review},
	author = {Petriglieri, Gianpiero and Ashford, Susan J. and Wrzesniewski, Amy},
	month = mar,
	year = {2018},
	keywords = {freelance, gig economy, productivity},
	file = {Snapshot:/Users/kriarm/Zotero/storage/BQ6VQK89/thriving-in-the-gig-economy.html:text/html},
}

@article{niehaus_transitioning_2019,
	title = {Transitioning from postdoc researcher to gig-economy scientist},
	copyright = {2019 Nature},
	url = {http://www.nature.com/articles/d41586-019-03021-5},
	doi = {10.1038/d41586-019-03021-5},
	abstract = {How Amanda Niehaus built a brand in preparation for the new world of scientific work.},
	language = {en},
	urldate = {2019-10-19},
	journal = {Nature},
	author = {Niehaus, Amanda},
	month = oct,
	year = {2019},
	keywords = {read, freelance, gig economy, post-doc},
	file = {Snapshot:/Users/kriarm/Zotero/storage/V6KNFPFH/d41586-019-03021-5.html:text/html},
}

@article{roche_environmental_2019,
	title = {Environmental sciences benefit from robust evidence irrespective of speed},
	volume = {696},
	issn = {00489697},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0048969719339774},
	doi = {10.1016/j.scitotenv.2019.134000},
	language = {en},
	urldate = {2019-10-05},
	journal = {Science of The Total Environment},
	author = {Roche, Dominique G. and Bennett, Joseph R. and Provencher, Jennifer and Rytwinski, Trina and Haddaway, Neal R. and Cooke, Steven J.},
	month = dec,
	year = {2019},
	keywords = {open science, read, environmental science, slow science},
	pages = {134000},
}

@article{gross_good_2013,
	title = {Good practice for conducting and reporting {MEG} research},
	volume = {65},
	issn = {10538119},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1053811912009895},
	doi = {10.1016/j.neuroimage.2012.10.001},
	language = {en},
	urldate = {2019-09-10},
	journal = {NeuroImage},
	author = {Gross, Joachim and Baillet, Sylvain and Barnes, Gareth R. and Henson, Richard N. and Hillebrand, Arjan and Jensen, Ole and Jerbi, Karim and Litvak, Vladimir and Maess, Burkhard and Oostenveld, Robert and Parkkonen, Lauri and Taylor, Jason R. and van Wassenhove, Virginie and Wibral, Michael and Schoffelen, Jan-Mathijs},
	month = jan,
	year = {2013},
	keywords = {MEG, methods, read, preprocessing},
	pages = {349--363},
}

@article{lansner_associative_2009,
	title = {Associative memory models: from the cell-assembly theory to biophysically detailed cortex simulations},
	volume = {32},
	issn = {01662236},
	shorttitle = {Associative memory models},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0166223609000174},
	doi = {10.1016/j.tins.2008.12.002},
	language = {en},
	number = {3},
	urldate = {2019-09-09},
	journal = {Trends in Neurosciences},
	author = {Lansner, Anders},
	month = mar,
	year = {2009},
	keywords = {working memory, read, project.lsnn, attractor network},
	pages = {178--186},
}

@article{sandberg_working_2003,
	title = {A working memory model based on fast {Hebbian} learning},
	volume = {14},
	issn = {0954-898X},
	url = {https://doi.org/10.1088/0954-898X_14_4_309},
	doi = {10.1088/0954-898X_14_4_309},
	abstract = {Recent models of the oculomotor delayed response task have been based on the assumption that working memory is stored as a persistent activity state (a ‘bump’ state). The delay activity is maintained by a finely tuned synaptic weight matrix producing a line attractor. Here we present an alternative hypothesis, that fast Hebbian synaptic plasticity is the mechanism underlying working memory. A computational model demonstrates a working memory function that is more resistant to distractors and network inhomogeneity compared to previous models, and that is also capable of storing multiple memories.},
	number = {4},
	urldate = {2019-09-09},
	journal = {Network: Computation in Neural Systems},
	author = {Sandberg, A. and Tegnér, J. and Lansner, A.},
	month = jan,
	year = {2003},
	pmid = {14653503},
	keywords = {working memory, readme, project.lsnn, delay activity, Hebbian learning},
	pages = {789--802},
}

@book{sterling_principles_2015,
	address = {Cambridge, Massachusetts},
	title = {Principles of neural design},
	isbn = {978-0-262-02870-7},
	publisher = {The MIT Press},
	author = {Sterling, Peter and Laughlin, Simon},
	year = {2015},
	keywords = {readme},
}

@article{fiebig_spiking_2017,
	title = {A spiking working memory model based on {Hebbian} short-term potentiation},
	volume = {37},
	issn = {0270-6474, 1529-2401},
	url = {http://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.1989-16.2016},
	doi = {10.1523/JNEUROSCI.1989-16.2016},
	language = {en},
	number = {1},
	urldate = {2019-09-09},
	journal = {The Journal of Neuroscience},
	author = {Fiebig, Florian and Lansner, Anders},
	month = jan,
	year = {2017},
	keywords = {working memory, readme, project.lsnn, spiking neurons, Hebbian learning},
	pages = {83--96},
}

@article{zylberberg_mechanisms_2017,
	title = {Mechanisms of persistent activity in cortical circuits: {Possible} neural substrates for working memory},
	volume = {40},
	shorttitle = {Mechanisms of {Persistent} {Activity} in {Cortical} {Circuits}},
	url = {https://doi.org/10.1146/annurev-neuro-070815-014006},
	doi = {10.1146/annurev-neuro-070815-014006},
	abstract = {A commonly observed neural correlate of working memory is firing that persists after the triggering stimulus disappears. Substantial effort has been devoted to understanding the many potential mechanisms that may underlie memory-associated persistent activity. These rely either on the intrinsic properties of individual neurons or on the connectivity within neural circuits to maintain the persistent activity. Nevertheless, it remains unclear which mechanisms are at play in the many brain areas involved in working memory. Herein, we first summarize the palette of different mechanisms that can generate persistent activity. We then discuss recent work that asks which mechanisms underlie persistent activity in different brain areas. Finally, we discuss future studies that might tackle this question further. Our goal is to bridge between the communities of researchers who study either single-neuron biophysical, or neural circuit, mechanisms that can generate the persistent activity that underlies working memory.},
	number = {1},
	urldate = {2019-09-03},
	journal = {Annual Review of Neuroscience},
	author = {Zylberberg, Joel and Strowbridge, Ben W.},
	year = {2017},
	pmid = {28772102},
	pages = {603--627},
	file = {Zylberberg and Strowbridge - 2017 - Mechanisms of persistent activity in cortical circ.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Zylberberg and Strowbridge - 2017 - Mechanisms of persistent activity in cortical circ.pdf:application/pdf},
}

@article{runyan_distinct_2017,
	title = {Distinct timescales of population coding across cortex},
	volume = {548},
	copyright = {2017 Nature Publishing Group},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature23020},
	doi = {10.1038/nature23020},
	abstract = {The cortex represents information across widely varying timescales1,2,3,4,5. For instance, sensory cortex encodes stimuli that fluctuate over few tens of milliseconds6,7, whereas in association cortex behavioural choices can require the maintenance of information over seconds8,9. However, it remains poorly understood whether diverse timescales result mostly from features intrinsic to individual neurons or from neuronal population activity. This question remains unanswered, because the timescales of coding in populations of neurons have not been studied extensively, and population codes have not been compared systematically across cortical regions. Here we show that population codes can be essential to achieve long coding timescales. Furthermore, we find that the properties of population codes differ between sensory and association cortices. We compared coding for sensory stimuli and behavioural choices in auditory cortex and posterior parietal cortex as mice performed a sound localization task. Auditory stimulus information was stronger in auditory cortex than in posterior parietal cortex, and both regions contained choice information. Although auditory cortex and posterior parietal cortex coded information by tiling in time neurons that were transiently informative for approximately 200 milliseconds, the areas had major differences in functional coupling between neurons, measured as activity correlations that could not be explained by task events. Coupling among posterior parietal cortex neurons was strong and extended over long time lags, whereas coupling among auditory cortex neurons was weak and short-lived. Stronger coupling in posterior parietal cortex led to a population code with long timescales and a representation of choice that remained consistent for approximately 1 second. In contrast, auditory cortex had a code with rapid fluctuations in stimulus and choice information over hundreds of milliseconds. Our results reveal that population codes differ across cortex and that coupling is a variable property of cortical populations that affects the timescale of information coding and the accuracy of behaviour.},
	language = {en},
	number = {7665},
	urldate = {2019-08-29},
	journal = {Nature},
	author = {Runyan, Caroline A. and Piasini, Eugenio and Panzeri, Stefano and Harvey, Christopher D.},
	month = aug,
	year = {2017},
	keywords = {readme, timescales, rodent},
	pages = {92--96},
	file = {Runyan et al. - 2017 - Distinct timescales of population coding across co.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Runyan et al. - 2017 - Distinct timescales of population coding across co.pdf:application/pdf},
}

@article{bellec_solution_2019,
	title = {A solution to the learning dilemma for recurrent networks of spiking neurons},
	copyright = {© 2019, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
	url = {https://www.biorxiv.org/content/10.1101/738385v1},
	doi = {10.1101/738385},
	abstract = {{\textless}p{\textgreater}Recurrently connected networks of spiking neurons underlie the astounding information processing capabilities of the brain. But in spite of extensive research, it has remained open how learning through synaptic plasticity could be organized in such networks. We argue that two pieces of this puzzle were provided by experimental data from neuroscience. A new mathematical insight tells us how they need to be combined to enable network learning through gradient descent. The resulting learning method -- called e-prop -- approaches the performance of BPTT (backpropagation through time), the best known method for training recurrent neural networks in machine learning. But in contrast to BPTT, e-prop is biologically plausible. In addition, it elucidates how brain-inspired new computer chips -- that are drastically more energy efficient -- can be enabled to learn.{\textless}/p{\textgreater}},
	language = {en},
	urldate = {2019-08-29},
	journal = {bioRxiv},
	author = {Bellec, Guillaume and Scherr, Franz and Subramoney, Anand and Hajek, Elias and Salaj, Darjan and Legenstein, Robert and Maass, Wolfgang},
	month = aug,
	year = {2019},
	keywords = {BPTT, backprop, readme, project.lsnn, LIF, spiking neurons, spike-rate adaptation, RSNN},
	pages = {738385},
}

@incollection{ashby_principles_1962,
	address = {London, UK},
	title = {Principles of the self-organizing system},
	url = {http://link.springer.com/10.1007/978-1-4899-0718-9_38},
	language = {en},
	urldate = {2019-08-27},
	booktitle = {Transactions of the {University} of {Illinois} {Symposium}},
	publisher = {Pergamon Press},
	author = {Ashby, W. Ross},
	editor = {Von Foerster, H. and Zopf, G. W.},
	year = {1962},
	doi = {10.1007/978-1-4899-0718-9_38},
	pages = {255--278},
	file = {Ashby - 1991 - Principles of the Self-Organizing System.pdf:/Users/kriarm/Zotero/storage/UGHJ92EA/Ashby - 1991 - Principles of the Self-Organizing System.pdf:application/pdf},
}

@article{koch_biological_2015,
	title = {A biological imitation game},
	volume = {163},
	issn = {0092-8674},
	url = {http://www.sciencedirect.com/science/article/pii/S0092867415012623},
	doi = {10.1016/j.cell.2015.09.045},
	abstract = {The digital reconstruction of a slice of rat somatosensory cortex from the Blue Brain Project provides the most complete simulation of a piece of excitable brain matter to date. To place these efforts in context and highlight their strengths and limitations, we introduce a Biological Imitation Game, based on Alan Turing’s Imitation Game, that operationalizes the difference between real and simulated brains.},
	number = {2},
	urldate = {2019-08-27},
	journal = {Cell},
	author = {Koch, Christof and Buice, Michael A.},
	month = oct,
	year = {2015},
	keywords = {readme},
	pages = {277--280},
}

@article{bauer_quiet_2015,
	title = {The quiet revolution of numerical weather prediction},
	volume = {525},
	copyright = {2015 Nature Publishing Group},
	issn = {1476-4687},
	url = {https://www.nature.com/articles/nature14956},
	doi = {10.1038/nature14956},
	abstract = {Advances in numerical weather prediction represent a quiet revolution because they have resulted from a steady accumulation of scientific knowledge and technological advances over many years that, with only a few exceptions, have not been associated with the aura of fundamental physics breakthroughs. Nonetheless, the impact of numerical weather prediction is among the greatest of any area of physical science. As a computational problem, global weather prediction is comparable to the simulation of the human brain and of the evolution of the early Universe, and it is performed every day at major operational centres across the world.},
	language = {en},
	number = {7567},
	urldate = {2019-08-27},
	journal = {Nature},
	author = {Bauer, Peter and Thorpe, Alan and Brunet, Gilbert},
	month = sep,
	year = {2015},
	keywords = {readme, explanation, thesis.introduction, simulation, understanding, weather, weather prediction},
	pages = {47--55},
}

@article{brodbeck_rapid_2018,
	title = {Rapid transformation from auditory to linguistic representations of continuous speech},
	volume = {28},
	issn = {0960-9822},
	url = {http://www.sciencedirect.com/science/article/pii/S096098221831409X},
	doi = {10.1016/j.cub.2018.10.042},
	abstract = {Summary
During speech perception, a central task of the auditory cortex is to analyze complex acoustic patterns to allow detection of the words that encode a linguistic message [1]. It is generally thought that this process includes at least one intermediate, phonetic, level of representations [2, 3, 4, 5, 6], localized bilaterally in the superior temporal lobe [7, 8, 9]. Phonetic representations reflect a transition from acoustic to linguistic information, classifying acoustic patterns into linguistically meaningful units, which can serve as input to mechanisms that access abstract word representations [10, 11]. While recent research has identified neural signals arising from successful recognition of individual words in continuous speech [12, 13, 14, 15], no explicit neurophysiological signal has been found demonstrating the transition from acoustic and/or phonetic to symbolic, lexical representations. Here, we report a response reflecting the incremental integration of phonetic information for word identification, dominantly localized to the left temporal lobe. The short response latency, approximately 114 ms relative to phoneme onset, suggests that phonetic information is used for lexical processing as soon as it becomes available. Responses also tracked word boundaries, confirming previous reports of immediate lexical segmentation [16, 17]. These new results were further investigated using a cocktail-party paradigm [18, 19] in which participants listened to a mix of two talkers, attending to one and ignoring the other. Analysis indicates neural lexical processing of only the attended, but not the unattended, speech stream. Thus, while responses to acoustic features reflect attention through selective amplification of attended speech, responses consistent with a lexical processing model reveal categorically selective processing.},
	number = {24},
	urldate = {2019-08-23},
	journal = {Current Biology},
	author = {Brodbeck, Christian and Hong, L. Elliot and Simon, Jonathan Z.},
	month = dec,
	year = {2018},
	keywords = {MEG, RNN, readme, entropy, surprisal, neural networks},
	pages = {3976--3983.e5},
}

@article{bernardy_using_2017,
	title = {Using deep neural networks to learn syntactic agreement},
	volume = {15},
	copyright = {Copyright (c) 2017 LiLT (Linguistic Issues in Language Technology)},
	issn = {1945-3604},
	url = {http://csli-lilt.stanford.edu/ojs/index.php/LiLT/article/view/94},
	abstract = {We consider the extent to which different deep neural network (DNN) configurations can learn syntactic relations, by taking up Linzen et al.’s (2016) work on subject-verb agreement with LSTM RNNs. We test their methods on a much larger corpus than they used (a {\textasciitilde}24 million example part of the WaCky corpus, instead of their {\textasciitilde}1.35 million example corpus, both drawn from Wikipedia). We experiment with several different DNN architectures (LSTM RNNs, GRUs, and CNNs), and alternative parameter settings for these systems (vocabulary size, training to test ratio, number of layers, memory size, drop out rate, and lexical embedding dimension size). We also try out our own unsupervised DNN language model. Our results are broadly compatible with those that Linzen et al. report. However, we discovered some interesting, and in some cases, surprising features of DNNs and language models in their performance of the agreement learning task. In particular, we found that DNNs require large vocabularies to form substantive lexical embeddings in order to learn structural patterns. This finding has interesting consequences for our understanding of the way in which DNNs represent syntactic information. It suggests that DNNs learn syntactic patterns more efficiently through rich lexical embeddings, with semantic as well as syntactic cues, than from training on lexically impoverished strings that highlight structural patterns.},
	language = {en},
	number = {0},
	urldate = {2019-08-23},
	journal = {LiLT (Linguistic Issues in Language Technology)},
	author = {Bernardy, Jean-Philippe and Lappin, Shalom},
	month = nov,
	year = {2017},
	keywords = {syntax, language model, readme, computational linguistics, ANN, DNN},
	file = {Full Text PDF:/Users/kriarm/Zotero/storage/4RX5AZG8/Bernardy and Lappin - 2017 - Using Deep Neural Networks to Learn Syntactic Agre.pdf:application/pdf;Snapshot:/Users/kriarm/Zotero/storage/KDAMYIKD/94.html:text/html},
}

@article{conde-sousa_working_2013,
	title = {A working memory model for serial order that stores information in the intrinsic excitability properties of neurons},
	volume = {35},
	issn = {0929-5313, 1573-6873},
	url = {http://link.springer.com/10.1007/s10827-013-0447-7},
	doi = {10.1007/s10827-013-0447-7},
	language = {en},
	number = {2},
	urldate = {2019-08-22},
	journal = {Journal of Computational Neuroscience},
	author = {Conde-Sousa, Eduardo and Aguiar, Paulo},
	month = oct,
	year = {2013},
	keywords = {working memory, readme, intrinsic plasticity, Hodgkin-Huxley model, 1},
	pages = {187--199},
}

@article{mozzachiodi_more_2010,
	title = {More than synaptic plasticity: role of nonsynaptic plasticity in learning and memory},
	volume = {33},
	issn = {0166-2236},
	shorttitle = {More than synaptic plasticity},
	url = {http://www.sciencedirect.com/science/article/pii/S0166223609001702},
	doi = {10.1016/j.tins.2009.10.001},
	abstract = {Decades of research on the cellular mechanisms of memory have led to the widely held view that memories are stored as modifications of synaptic strength. These changes involve presynaptic processes, such as direct modulation of the release machinery, or postsynaptic processes, such as modulation of receptor properties. Parallel studies have revealed that memories might also be stored by nonsynaptic processes, such as modulation of voltage-dependent membrane conductances, which are expressed as changes in neuronal excitability. Although in some cases nonsynaptic changes can function as part of the engram itself, they might also serve as mechanisms through which a neural circuit is set to a permissive state to facilitate synaptic modifications that are necessary for memory storage.},
	number = {1},
	urldate = {2019-08-22},
	journal = {Trends in Neurosciences},
	author = {Mozzachiodi, Riccardo and Byrne, John H.},
	month = jan,
	year = {2010},
	pages = {17--26},
}

@article{zhang_other_2003,
	title = {The other side of the engram: experience-driven changes in neuronal intrinsic excitability},
	volume = {4},
	copyright = {2003 Nature Publishing Group},
	issn = {1471-0048},
	shorttitle = {The other side of the engram},
	url = {https://www.nature.com/articles/nrn1248},
	doi = {10.1038/nrn1248},
	abstract = {In addition to synaptic plasticity, which confers neurons with the ability to modify the strength of individual synapses, nerve cells also possess forms of intrinsic plasticity (changes in intrinsic excitability), which affect largest ensembles of synapses and might affect the whole cell. This form of plasticity might endow neurons with an additional capacity to store information.
                  
                  
                    Different learning tasks induce changes in intrinsic excitability in several vertebrate and invertebrate species. In many cases, these changes manifest as reductions in spike threshold, spike accommodation and amplitude of burst-evoked afterhyperpolarization, all of which point to the modulation of K+ channels as one potential underlying mechanism.
                  
                  
                    Forms of experience-dependent plasticity other than learning also elicit intrinsic plasticity, which share similar mechanisms as learning-mediated plastic changes. These forms of experience include adaptive and maladaptive states, such as seizures.
                  
                  
                    Studies in cell culture and brain slices have shown that it is possible to study intrinsic excitability in vitro. These studies have pointed to a series of K+, Ca2+ and Na+ conductances as possible molecular substrates of the plastic changes.
                  
                  
                    The signal transduction cascades mediating the conductance changes that seem to be crucial for intrinsic plasticity remain unknown for most model systems. Ca2+/calmodulin-dependent protein kinase II and other kinases, the action of G-proteins, and the release of intracellular Ca2+ have been proposed, but the definitive experiments remain to be reported.
                  
                  
                    Many changes remain to be answered in this nascent field. What is the relationship between intrinsic and synaptic plasticities, particularly in cases when both phenomena seem to co-exist? What is the duration of intrinsic plasticity? Does it really function to encode information? If so, what kind of memories could it store? These and many other issues should generate as much attention of intrinsic excitability changes as there has been on synaptic plasticity.
                  
                
               In addition to synaptic plasticity, which confers neurons with the ability to modify the strength of individual synapses, nerve cells also possess forms of intrinsic plasticity (changes in intrinsic excitability), which affect largest ensembles of synapses and might affect the whole cell. This form of plasticity might endow neurons with an additional capacity to store information. Different learning tasks induce changes in intrinsic excitability in several vertebrate and invertebrate species. In many cases, these changes manifest as reductions in spike threshold, spike accommodation and amplitude of burst-evoked afterhyperpolarization, all of which point to the modulation of K+ channels as one potential underlying mechanism. Forms of experience-dependent plasticity other than learning also elicit intrinsic plasticity, which share similar mechanisms as learning-mediated plastic changes. These forms of experience include adaptive and maladaptive states, such as seizures. Studies in cell culture and brain slices have shown that it is possible to study intrinsic excitability in vitro. These studies have pointed to a series of K+, Ca2+ and Na+ conductances as possible molecular substrates of the plastic changes. The signal transduction cascades mediating the conductance changes that seem to be crucial for intrinsic plasticity remain unknown for most model systems. Ca2+/calmodulin-dependent protein kinase II and other kinases, the action of G-proteins, and the release of intracellular Ca2+ have been proposed, but the definitive experiments remain to be reported. Many changes remain to be answered in this nascent field. What is the relationship between intrinsic and synaptic plasticities, particularly in cases when both phenomena seem to co-exist? What is the duration of intrinsic plasticity? Does it really function to encode information? If so, what kind of memories could it store? These and many other issues should generate as much attention of intrinsic excitability changes as there has been on synaptic plasticity.},
	language = {en},
	number = {11},
	urldate = {2019-08-22},
	journal = {Nature Reviews Neuroscience},
	author = {Zhang, Wei and Linden, David J.},
	month = nov,
	year = {2003},
	keywords = {working memory, project.lsnn, intrinsic plasticity, processing memory, review, engram},
	pages = {885--900},
}

@article{barak_working_2014,
	title = {Working models of working memory},
	volume = {25},
	issn = {09594388},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0959438813002158},
	doi = {10.1016/j.conb.2013.10.008},
	language = {en},
	urldate = {2019-08-20},
	journal = {Current Opinion in Neurobiology},
	author = {Barak, Omri and Tsodyks, Misha},
	month = apr,
	year = {2014},
	keywords = {working memory, read, processing memory, spiking neurons},
	pages = {20--24},
}

@book{bezjak_open_2018,
	title = {Open science training handbook},
	copyright = {Creative Commons Zero - CC0 1.0, Open Access},
	url = {https://zenodo.org/record/1212496},
	abstract = {{\textless}strong{\textgreater}For a readable version of the book, please visit https://book.fosteropenscience.eu{\textless}/strong{\textgreater}

A group of fourteen authors came together in February 2018 at the TIB (German National Library of Science and Technology) in Hannover to create an open, living handbook on Open Science training. High-quality trainings are fundamental when aiming at a cultural change towards the implementation of Open Science principles. Teaching resources provide great support for Open Science instructors and trainers. The Open Science training handbook will be a key resource and a first step towards developing Open Access and Open Science curricula and andragogies. Supporting and connecting an emerging Open Science community that wishes to pass on their knowledge as multipliers, the handbook will enrich training activities and unlock the community’s full potential.

In this first release of the Open Science Training Handbook, some initial feedback from the community is already included.},
	language = {en},
	urldate = {2019-08-19},
	publisher = {Zenodo},
	author = {Bezjak, Sonja and Clyburne-Sherin, April and Conzett, Philipp and Fernandes, Pedro and Görögh, Edit and Helbig, Kerstin and Kramer, Bianca and Labastida, Ignasi and Niemeyer, Kyle and Psomopoulos, Fotis and Ross-Hellauer, Tony and Schneider, René and Tennant, Jon and Verbakel, Ellen and Brinken, Helene and Heller, Lambert},
	month = apr,
	year = {2018},
	doi = {10.5281/zenodo.1212496},
	keywords = {open science},
}

@article{allen_open_2019,
	title = {Open science challenges, benefits and tips in early career and beyond},
	volume = {17},
	issn = {1545-7885},
	url = {http://dx.plos.org/10.1371/journal.pbio.3000246},
	doi = {10.1371/journal.pbio.3000246},
	language = {en},
	number = {5},
	urldate = {2019-08-19},
	journal = {PLOS Biology},
	author = {Allen, Christopher and Mehler, David M. A.},
	month = may,
	year = {2019},
	keywords = {open science, readme, ECR, registered report},
	pages = {e3000246},
}

@article{mckiernan_how_2016,
	title = {How open science helps researchers succeed},
	volume = {5},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.16800},
	doi = {10.7554/eLife.16800},
	abstract = {Open access, open data, open source and other open scholarship practices are growing in popularity and necessity. However, widespread adoption of these practices has not yet been achieved. One reason is that researchers are uncertain about how sharing their work will affect their careers. We review literature demonstrating that open research is associated with increases in citations, media attention, potential collaborators, job opportunities and funding opportunities. These findings are evidence that open research practices bring significant benefits to researchers relative to more traditional closed practices.},
	urldate = {2019-08-19},
	journal = {eLife},
	author = {McKiernan, Erin C and Bourne, Philip E and Brown, C Titus and Buck, Stuart and Kenall, Amye and Lin, Jennifer and McDougall, Damon and Nosek, Brian A and Ram, Karthik and Soderberg, Courtney K and Spies, Jeffrey R and Thaney, Kaitlin and Updegrove, Andrew and Woo, Kara H and Yarkoni, Tal},
	editor = {Rodgers, Peter},
	month = jul,
	year = {2016},
	keywords = {open science, readme, career},
	pages = {e16800},
}

@article{hinton_learning_2007,
	title = {Learning multiple layers of representation},
	volume = {11},
	issn = {13646613},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661307002173},
	doi = {10.1016/j.tics.2007.09.004},
	language = {en},
	number = {10},
	urldate = {2019-08-18},
	journal = {Trends in Cognitive Sciences},
	author = {Hinton, Geoffrey E.},
	month = oct,
	year = {2007},
	keywords = {deep learning, readme, neural networks},
	pages = {428--434},
}

@book{goodfellow_deep_2016,
	title = {Deep learning},
	publisher = {MIT Press},
	author = {Goodfellow, Ian and Bengio, Yoshua and Courville, Aaron},
	year = {2016},
	note = {http://www.deeplearningbook.org },
}

@inproceedings{graves_speech_2013,
	address = {Vancouver, BC, Canada},
	title = {Speech recognition with deep recurrent neural networks},
	isbn = {978-1-4799-0356-6},
	url = {http://ieeexplore.ieee.org/document/6638947/},
	doi = {10.1109/ICASSP.2013.6638947},
	urldate = {2019-08-14},
	booktitle = {2013 {IEEE} {International} {Conference} on {Acoustics}, {Speech} and {Signal} {Processing}},
	publisher = {IEEE},
	author = {Graves, Alex and Mohamed, Abdel-rahman and Hinton, Geoffrey},
	month = may,
	year = {2013},
	keywords = {LSTM, RNN, readme, speech recognition},
	pages = {6645--6649},
}

@inproceedings{cho_properties_2014,
	address = {Doha, Qatar},
	title = {On the properties of neural machine translation: encoder–decoder approaches},
	shorttitle = {On the {Properties} of {Neural} {Machine} {Translation}},
	url = {https://www.aclweb.org/anthology/W14-4012},
	doi = {10.3115/v1/W14-4012},
	urldate = {2019-08-14},
	booktitle = {Proceedings of {SSST}-8, {Eighth} {Workshop} on {Syntax}, {Semantics} and {Structure} in {Statistical} {Translation}},
	publisher = {Association for Computational Linguistics},
	author = {Cho, Kyunghyun and van Merriënboer, Bart and Bahdanau, Dzmitry and Bengio, Yoshua},
	month = oct,
	year = {2014},
	keywords = {deep learning, LSTM, recurrent neural network, GRU, machine translation},
	pages = {103--111},
}

@article{kriegeskorte_neural_2019,
	title = {Neural network models and deep learning},
	volume = {29},
	issn = {09609822},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0960982219302040},
	doi = {10.1016/j.cub.2019.02.034},
	language = {en},
	number = {7},
	urldate = {2019-08-14},
	journal = {Current Biology},
	author = {Kriegeskorte, Nikolaus and Golan, Tal},
	month = apr,
	year = {2019},
	keywords = {deep learning, explanation, read, neural networks, machine learning, DNN},
	pages = {R231--R236},
}

@article{clark_what_2019,
	title = {What does {BERT} look at? {An} analysis of {BERT}'s attention},
	shorttitle = {What {Does} {BERT} {Look} {At}?},
	url = {http://arxiv.org/abs/1906.04341},
	abstract = {Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data. Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classifiers). Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT. BERT's attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors. We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT's attention.},
	urldate = {2019-08-12},
	journal = {arXiv:1906.04341 [cs]},
	author = {Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D.},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.04341},
	keywords = {language model, NLP, BERT, attention, readme, transformer, computational linguistics},
}

@article{goldberg_assessing_2019,
	title = {Assessing {BERT}'s syntactic abilities},
	url = {http://arxiv.org/abs/1901.05287},
	abstract = {I assess the extent to which the recently introduced BERT model captures English syntactic phenomena, using (1) naturally-occurring subject-verb agreement stimuli; (2) "coloreless green ideas" subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection; and (3) manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena. The BERT model performs remarkably well on all cases.},
	urldate = {2019-08-12},
	journal = {arXiv:1901.05287 [cs]},
	author = {Goldberg, Yoav},
	month = jan,
	year = {2019},
	note = {arXiv: 1901.05287},
	keywords = {language model, NLP, LSTM, BERT, readme},
}

@article{jat_relating_2019,
	title = {Relating simple sentence representations in deep neural networks and the brain},
	url = {http://arxiv.org/abs/1906.11861},
	abstract = {What is the relationship between sentence representations learned by deep recurrent models against those encoded by the brain? Is there any correspondence between hidden layers of these recurrent models and brain regions when processing sentences? Can these deep models be used to synthesize brain data which can then be utilized in other extrinsic tasks? We investigate these questions using sentences with simple syntax and semantics (e.g., The bone was eaten by the dog.). We consider multiple neural network architectures, including recently proposed ELMo and BERT. We use magnetoencephalography (MEG) brain recording data collected from human subjects when they were reading these simple sentences. Overall, we find that BERT's activations correlate the best with MEG brain data. We also find that the deep network representation can be used to generate brain data from new sentences to augment existing brain data. To the best of our knowledge, this is the first work showing that the MEG brain recording when reading a word in a sentence can be used to distinguish earlier words in the sentence. Our exploration is also the first to use deep neural network representations to generate synthetic brain data and to show that it helps in improving subsequent stimuli decoding task accuracy.},
	urldate = {2019-08-12},
	journal = {arXiv:1906.11861 [cs]},
	author = {Jat, Sharmistha and Tang, Hao and Talukdar, Partha and Mitchell, Tom},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.11861},
	keywords = {MEG, language model, LSTM, RNN, BERT, project.lstmMEG, machine learning},
}

@article{enguehard_exploring_2017,
	title = {Exploring the syntactic abilities of {RNNs} with multi-task learning},
	url = {http://arxiv.org/abs/1706.03542},
	abstract = {Recent work has explored the syntactic abilities of RNNs using the subject-verb agreement task, which diagnoses sensitivity to sentence structure. RNNs performed this task well in common cases, but faltered in complex sentences (Linzen et al., 2016). We test whether these errors are due to inherent limitations of the architecture or to the relatively indirect supervision provided by most agreement dependencies in a corpus. We trained a single RNN to perform both the agreement task and an additional task, either CCG supertagging or language modeling. Multi-task training led to significantly lower error rates, in particular on complex sentences, suggesting that RNNs have the ability to evolve more sophisticated syntactic representations than shown before. We also show that easily available agreement training data can improve performance on other syntactic tasks, in particular when only a limited amount of training data is available for those tasks. The multi-task paradigm can also be leveraged to inject grammatical knowledge into language models.},
	urldate = {2019-08-12},
	journal = {arXiv:1706.03542 [cs]},
	author = {Enguehard, Emile and Goldberg, Yoav and Linzen, Tal},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.03542},
	keywords = {readme},
}

@book{goldberg_neural_2017,
	address = {San Rafael},
	series = {Synthesis lectures on human language technologies},
	title = {Neural network methods for natural language processing},
	isbn = {978-1-62705-295-5 978-1-68173-235-0 978-1-62705-298-6},
	language = {eng},
	number = {37},
	publisher = {Morgan \& Claypool Publishers},
	author = {Goldberg, Yoav},
	year = {2017},
	note = {OCLC: 990794614},
	keywords = {NLP, readme, neural networks, machine learning},
	file = {Goldberg - 2017 - Neural Network Methods in Natural Language Processing.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Goldberg - 2017 - Neural Network Methods in Natural Language Processing.pdf:application/pdf},
}

@book{pollan_how_2018,
	title = {How to change your mind: the new science of psychedelics},
	isbn = {978-0-241-29422-2},
	shorttitle = {How to change your mind},
	abstract = {When LSD was first discovered in the 1940s, it seemed to researchers, scientists and doctors as if the world might be on the cusp of psychological revolution. It promised to shed light on the deep mysteries of consciousness, as well as offer relief to addicts and the mentally ill. But in the 1960s, with the vicious backlash against the counter-culture, all further research was banned. In recent years, however, work has quietly begun again on the amazing potential of LSD, psilocybin and DMT. Could these drugs in fact improve the lives of many people? Diving deep into this extraordinary world and putting himself forwardas a guinea-pig, Michael Pollan has written a remarkable history of psychedelics and a compelling portrait of the new generation of scientists fascinated by the implications of these drugs. How to Change Your Mind is a report from what could very well be the future of human consciousness.},
	language = {English},
	author = {Pollan, Michael},
	year = {2018},
	note = {OCLC: 1048243189},
	keywords = {readme},
}

@book{bishop_neural_1995,
	address = {New York, NY, USA},
	title = {Neural networks for pattern recognition},
	isbn = {0-19-853864-2},
	publisher = {Oxford University Press, Inc.},
	author = {Bishop, Christopher M.},
	year = {1995},
	note = {bishop\_neural\_1995},
}

@article{maass_real-time_2002,
	title = {Real-time computing without stable states: {A} new framework for neural computation based on perturbations},
	volume = {14},
	issn = {0899-7667, 1530-888X},
	shorttitle = {Real-{Time} {Computing} {Without} {Stable} {States}},
	url = {http://www.mitpressjournals.org/doi/10.1162/089976602760407955},
	doi = {10.1162/089976602760407955},
	language = {en},
	number = {11},
	urldate = {2019-08-01},
	journal = {Neural Computation},
	author = {Maass, Wolfgang and Natschläger, Thomas and Markram, Henry},
	month = nov,
	year = {2002},
	keywords = {readme, neural networks, spiking neurons, reservoir computing},
	pages = {2531--2560},
}

@article{sussillo_neural_2014,
	title = {Neural circuits as computational dynamical systems},
	volume = {25},
	issn = {09594388},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0959438814000166},
	doi = {10.1016/j.conb.2014.01.008},
	language = {en},
	urldate = {2019-08-01},
	journal = {Current Opinion in Neurobiology},
	author = {Sussillo, David},
	month = apr,
	year = {2014},
	keywords = {deep learning, RNN, readme, dynamical system, machine learning},
	pages = {156--163},
}

@incollection{krizhevsky_imagenet_2012,
	title = {{ImageNet} classification with deep convolutional neural networks},
	url = {http://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 25},
	publisher = {Curran Associates, Inc.},
	author = {Krizhevsky, Alex and Sutskever, Ilya and Hinton, Geoffrey E},
	editor = {Pereira, F. and Burges, C. J. C. and Bottou, L. and Weinberger, K. Q.},
	year = {2012},
	note = {krizhevsky\_imagenet\_2012},
	keywords = {deep learning, imageNet, readme, neural networks, convolutional neural networks},
	pages = {1097--1105},
}

@misc{colah_neural_2015,
	title = {Neural networks, types, and functional programming},
	url = {http://colah.github.io/posts/2015-09-NN-Types-FP/},
	urldate = {2019-08-01},
	journal = {colah's blog},
	author = {Colah, Chris},
	month = sep,
	year = {2015},
	file = {Neural Networks, Types, and Functional Programming -- colah's blog:/Users/kriarm/Zotero/storage/X5PN428P/2015-09-NN-Types-FP.html:text/html},
}

@inproceedings{jurafsky_probabilistic_2003,
	title = {Probabilistic modeling in psycholinguistics: linguistic comprehension and production},
	booktitle = {Probabilistic linguistics},
	publisher = {MIT Press},
	author = {Jurafsky, Dan},
	year = {2003},
	pages = {39--96},
}

@article{hale_what_2011,
	title = {What a rational parser would do},
	volume = {35},
	issn = {03640213},
	url = {http://doi.wiley.com/10.1111/j.1551-6709.2010.01145.x},
	doi = {10.1111/j.1551-6709.2010.01145.x},
	language = {en},
	number = {3},
	urldate = {2019-07-22},
	journal = {Cognitive Science},
	author = {Hale, John T.},
	month = apr,
	year = {2011},
	keywords = {read},
	pages = {399--443},
}

@article{lillicrap_what_2019,
	title = {What does it mean to understand a neural network?},
	url = {http://arxiv.org/abs/1907.06374},
	abstract = {We can define a neural network that can learn to recognize objects in less than 100 lines of code. However, after training, it is characterized by millions of weights that contain the knowledge about many object types across visual scenes. Such networks are thus dramatically easier to understand in terms of the code that makes them than the resulting properties, such as tuning or connections. In analogy, we conjecture that rules for development and learning in brains may be far easier to understand than their resulting properties. The analogy suggests that neuroscience would benefit from a focus on learning and development.},
	urldate = {2019-07-18},
	journal = {arXiv:1907.06374 [cs, q-bio, stat]},
	author = {Lillicrap, Timothy P. and Kording, Konrad P.},
	month = jul,
	year = {2019},
	note = {arXiv: 1907.06374},
	keywords = {explanation, read, neural networks, machine learning},
	file = {arXiv\:1907.06374 PDF:/Users/kriarm/Zotero/storage/SHEPA8WV/Lillicrap and Kording - 2019 - What does it mean to understand a neural network.pdf:application/pdf},
}

@article{galassi_attention_2019,
	title = {Attention, please! {A} critical review of neural attention models in natural language processing},
	url = {http://arxiv.org/abs/1902.02181},
	abstract = {Attention is an increasingly popular mechanism used in a wide range of neural architectures. Because of the fast-paced advances in this domain, a systematic overview of attention is still missing. In this article, we define a unified model for attention architectures for natural language processing, with a focus on architectures designed to work with vector representation of the textual data. We discuss the dimensions along which proposals differ, the possible uses of attention, and chart the major research activities and open challenges in the area.},
	urldate = {2019-07-16},
	journal = {arXiv:1902.02181 [cs, stat]},
	author = {Galassi, Andrea and Lippi, Marco and Torroni, Paolo},
	month = feb,
	year = {2019},
	note = {arXiv: 1902.02181},
	keywords = {read},
	file = {arXiv.org Snapshot:/Users/kriarm/Zotero/storage/DZXYQ2I3/1902.html:text/html},
}

@book{koch_biophysics_2004,
	address = {New York},
	edition = {1. issue as an Oxford Univ. Press paperback},
	series = {Computational neuroscience},
	title = {Biophysics of computation: information processing in single neurons},
	isbn = {978-0-19-518199-9},
	shorttitle = {Biophysics of computation},
	language = {eng},
	publisher = {Oxford Univ. Press},
	author = {Koch, Christof},
	year = {2004},
	note = {OCLC: 254577920},
}

@incollection{bickerton_reflections_2009,
	title = {Reflections on the neurobiology of syntax},
	isbn = {978-0-262-01356-7},
	url = {http://mitpress.universitypressscholarship.com/view/10.7551/mitpress/9780262013567.001.0001/upso-9780262013567-chapter-13},
	urldate = {2019-07-04},
	booktitle = {Biological {Foundations} and {Origin} of {Syntax}},
	publisher = {The MIT Press},
	author = {Hagoort, Peter},
	editor = {Bickerton, Derek and Szathmáry, Eörs},
	month = sep,
	year = {2009},
	doi = {10.7551/mitpress/9780262013567.003.0013},
	keywords = {syntax, readme, explanation},
	pages = {278--298},
}

@book{hansen_meg:_2010,
	title = {{MEG}: {An} introduction to methods},
	isbn = {978-0-19-530723-8},
	shorttitle = {{MEG}},
	url = {http://www.oxfordscholarship.com/view/10.1093/acprof:oso/9780195307238.001.0001/acprof-9780195307238},
	urldate = {2019-07-04},
	publisher = {Oxford University Press},
	editor = {Hansen, Peter and Kringelbach, Morten and Salmelin, Riitta},
	month = jun,
	year = {2010},
	doi = {10.1093/acprof:oso/9780195307238.001.0001},
	keywords = {MEG, methods},
}

@book{forstmann_introduction_2015,
	address = {New York, NY},
	title = {An {Introduction} to model-based cognitive neuroscience},
	isbn = {978-1-4939-2235-2 978-1-4939-2236-9},
	url = {http://link.springer.com/10.1007/978-1-4939-2236-9},
	language = {en},
	urldate = {2019-07-04},
	publisher = {Springer New York},
	editor = {Forstmann, Birte U. and Wagenmakers, Eric-Jan},
	year = {2015},
	doi = {10.1007/978-1-4939-2236-9},
	keywords = {computational cognitive neuroscience, model-based},
}

@article{poldrack_mapping_2010,
	title = {Mapping mental function to brain structure: {How} can cognitive neuroimaging succeed?},
	volume = {5},
	issn = {1745-6916, 1745-6924},
	shorttitle = {Mapping {Mental} {Function} to {Brain} {Structure}},
	url = {http://journals.sagepub.com/doi/10.1177/1745691610388777},
	doi = {10.1177/1745691610388777},
	language = {en},
	number = {6},
	urldate = {2019-07-04},
	journal = {Perspectives on Psychological Science},
	author = {Poldrack, Russell A.},
	month = nov,
	year = {2010},
	keywords = {cognitive neuroscience, fMRI, readme, mapping, cognitive neuroimaging},
	pages = {753--761},
	file = {Accepted Version:/Users/kriarm/Zotero/storage/KI46ZB4J/Poldrack - 2010 - Mapping Mental Function to Brain Structure How Ca.pdf:application/pdf},
}

@article{logothetis_what_2008,
	title = {What we can do and what we cannot do with {fMRI}},
	volume = {453},
	issn = {0028-0836, 1476-4687},
	url = {http://www.nature.com/articles/nature06976},
	doi = {10.1038/nature06976},
	language = {en},
	number = {7197},
	urldate = {2019-07-04},
	journal = {Nature},
	author = {Logothetis, Nikos K.},
	month = jun,
	year = {2008},
	keywords = {fMRI, methods, read, MRI},
	pages = {869--878},
}

@article{debanne_plasticity_2019,
	title = {Plasticity of intrinsic neuronal excitability},
	volume = {54},
	issn = {09594388},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0959438818300709},
	doi = {10.1016/j.conb.2018.09.001},
	language = {en},
	urldate = {2019-07-01},
	journal = {Current Opinion in Neurobiology},
	author = {Debanne, Dominique and Inglebert, Yanis and Russier, Michaël},
	month = feb,
	year = {2019},
	keywords = {intrinsic plasticity, plasticity},
	pages = {73--82},
}

@article{bastos_laminar_2018,
	title = {Laminar recordings in frontal cortex suggest distinct layers for maintenance and control of working memory},
	volume = {115},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1710323115},
	doi = {10.1073/pnas.1710323115},
	language = {en},
	number = {5},
	urldate = {2019-06-28},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Bastos, André M. and Loonis, Roman and Kornblith, Simon and Lundqvist, Mikael and Miller, Earl K.},
	month = jan,
	year = {2018},
	keywords = {readme, beta, gamma, gating, cortical circuits},
	pages = {1117--1122},
}

@article{marder_understanding_2015,
	title = {Understanding brains: {Details}, intuition, and big data},
	volume = {13},
	issn = {1545-7885},
	shorttitle = {Understanding {Brains}},
	doi = {10.1371/journal.pbio.1002147},
	language = {en},
	number = {5},
	urldate = {2019-06-27},
	journal = {PLOS Biology},
	author = {Marder, Eve},
	month = may,
	year = {2015},
	keywords = {readme, explanation, computational neuroscience, big data, philosohpy of neuroscience},
	pages = {e1002147},
}

@article{craver_are_2018-1,
	title = {Are {More} {Details} {Better}? {On} the {Norms} of {Completeness} for {Mechanistic} {Explanations}},
	issn = {0007-0882, 1464-3537},
	shorttitle = {Are {More} {Details} {Better}?},
	url = {https://academic.oup.com/bjps/advance-article/doi/10.1093/bjps/axy015/4816342},
	doi = {10.1093/bjps/axy015},
	abstract = {Completeness is an important but misunderstood norm of explanation. It has recently been argued that mechanistic accounts of scientiﬁc explanation are committed to the thesis that models are complete only if they describe everything about a mechanism and, as a corollary, that incomplete models are always improved by adding more details. If so, mechanistic accounts are at odds with the obvious and important role of abstraction in scientiﬁc modelling. We respond to this characterization of the mechanist’s views about abstraction and articulate norms of completeness for mechanistic explanations that have no such unwanted implications.},
	language = {en},
	urldate = {2019-06-27},
	journal = {The British Journal for the Philosophy of Science},
	author = {Craver, Carl F and Kaplan, David M},
	month = jan,
	year = {2018},
}

@article{mongillo_synaptic_2008,
	title = {Synaptic theory of working memory},
	volume = {319},
	issn = {0036-8075, 1095-9203},
	doi = {10.1126/science.1150769},
	language = {en},
	number = {5869},
	urldate = {2019-06-26},
	journal = {Science},
	author = {Mongillo, G. and Barak, O. and Tsodyks, M.},
	month = mar,
	year = {2008},
	pages = {1543--1546},
}

@article{masse_circuit_2019,
	title = {Circuit mechanisms for the maintenance and manipulation of information in working memory},
	volume = {22},
	copyright = {2019 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1546-1726},
	url = {http://www.nature.com/articles/s41593-019-0414-3},
	doi = {10.1038/s41593-019-0414-3},
	abstract = {The role of persistent spiking activity in working memory has recently come under debate. Here the authors use biologically realistic recurrent neural networks to explain why the strength of persistent activity can vary markedly between tasks.},
	language = {En},
	number = {7},
	urldate = {2019-06-26},
	journal = {Nature Neuroscience},
	author = {Masse, Nicolas Y. and Yang, Guangyu R. and Song, H. Francis and Wang, Xiao-Jing and Freedman, David J.},
	month = jul,
	year = {2019},
	pages = {1159},
}

@article{miller_working_2018,
	title = {Working {Memory} 2.0},
	volume = {100},
	issn = {08966273},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627318308250},
	doi = {10.1016/j.neuron.2018.09.023},
	language = {en},
	number = {2},
	urldate = {2019-06-13},
	journal = {Neuron},
	author = {Miller, Earl K. and Lundqvist, Mikael and Bastos, André M.},
	month = oct,
	year = {2018},
	keywords = {reading},
	pages = {463--475},
	file = {Miller et al. - 2018 - Working Memory 2.0.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Miller et al. - 2018 - Working Memory 2.0.pdf:application/pdf},
}

@article{buzsaki_origin_2012,
	title = {The origin of extracellular fields and currents — {EEG}, {ECoG}, {LFP} and spikes},
	volume = {13},
	issn = {1471-003X, 1471-0048},
	url = {http://www.nature.com/articles/nrn3241},
	doi = {10.1038/nrn3241},
	language = {en},
	number = {6},
	urldate = {2019-06-11},
	journal = {Nature Reviews Neuroscience},
	author = {Buzsáki, György and Anastassiou, Costas A. and Koch, Christof},
	month = jun,
	year = {2012},
	keywords = {MEG, readme, gamma, LFP, neuronal oscillations},
	pages = {407--420},
	file = {Buzsaki et al - 2012 - The origin of extracellular fields and currents.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Buzsaki et al - 2012 - The origin of extracellular fields and currents.pdf:application/pdf},
}

@article{sreenivasan_what_2019,
	title = {The what, where and how of delay activity},
	issn = {1471-003X, 1471-0048},
	url = {http://www.nature.com/articles/s41583-019-0176-7},
	doi = {10.1038/s41583-019-0176-7},
	language = {en},
	urldate = {2019-06-10},
	journal = {Nature Reviews Neuroscience},
	author = {Sreenivasan, Kartik K. and D’Esposito, Mark},
	month = may,
	year = {2019},
	keywords = {readme},
	file = {Sreenivasan and d'Esposito - 2019 - The what, where and why of delay activity.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Sreenivasan and d'Esposito - 2019 - The what, where and why of delay activity.pdf:application/pdf},
}

@article{weichwald_causal_2015,
	title = {Causal interpretation rules for encoding and decoding models in neuroimaging},
	volume = {110},
	issn = {10538119},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S105381191500052X},
	doi = {10.1016/j.neuroimage.2015.01.036},
	language = {en},
	urldate = {2019-06-09},
	journal = {NeuroImage},
	author = {Weichwald, Sebastian and Meyer, Timm and Özdenizci, Ozan and Schölkopf, Bernhard and Ball, Tonio and Grosse-Wentrup, Moritz},
	month = apr,
	year = {2015},
	keywords = {readme, machine learning, decoding models, encoding models, causality},
	pages = {48--59},
}

@article{friston_statistical_1994,
	title = {Statistical parametric maps in functional imaging: {A} general linear approach},
	volume = {2},
	issn = {10659471},
	shorttitle = {Statistical parametric maps in functional imaging},
	url = {http://doi.wiley.com/10.1002/hbm.460020402},
	doi = {10.1002/hbm.460020402},
	language = {en},
	number = {4},
	urldate = {2019-06-09},
	journal = {Human Brain Mapping},
	author = {Friston, K. J. and Holmes, A. P. and Worsley, K. J. and Poline, J.-P. and Frith, C. D. and Frackowiak, R. S. J.},
	year = {1994},
	keywords = {fMRI, encoding models, multivariate, regression, SPM},
	pages = {189--210},
}

@article{barrett_analyzing_2019,
	title = {Analyzing biological and artificial neural networks: challenges with opportunities for synergy?},
	volume = {55},
	issn = {09594388},
	shorttitle = {Analyzing biological and artificial neural networks},
	doi = {10.1016/j.conb.2019.01.007},
	language = {en},
	urldate = {2019-06-08},
	journal = {Current Opinion in Neurobiology},
	author = {Barrett, David GT and Morcos, Ari S and Macke, Jakob H},
	month = apr,
	year = {2019},
	keywords = {project.lstmMEG, explanation, read, interpretability},
	pages = {55--64},
}

@article{kriegeskorte_interpreting_2019,
	title = {Interpreting encoding and decoding models},
	volume = {55},
	issn = {09594388},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0959438818301004},
	doi = {10.1016/j.conb.2019.04.002},
	language = {en},
	urldate = {2019-05-27},
	journal = {Current Opinion in Neurobiology},
	author = {Kriegeskorte, Nikolaus and Douglas, Pamela K},
	month = apr,
	year = {2019},
	keywords = {read, decoding, encoding, linear model, neural readout},
	pages = {167--179},
}

@article{lillicrap_backpropagation_2019,
	title = {Backpropagation through time and the brain},
	volume = {55},
	issn = {0959-4388},
	url = {http://www.sciencedirect.com/science/article/pii/S0959438818302009},
	doi = {10.1016/j.conb.2019.01.011},
	abstract = {It has long been speculated that the backpropagation-of-error algorithm (backprop) may be a model of how the brain learns. Backpropagation-through-tim…},
	language = {en},
	urldate = {2019-05-26},
	journal = {Current Opinion in Neurobiology},
	author = {Lillicrap, Timothy P and Santoro, Adam},
	month = apr,
	year = {2019},
	keywords = {BPTT, backprop, read, neural networks, TCA, temporal credit assignment},
	pages = {82--89},
}

@article{armeni_frequency-specific_2019,
	title = {Frequency-specific brain dynamics related to prediction during language comprehension},
	issn = {10538119},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1053811919304057},
	doi = {10.1016/j.neuroimage.2019.04.083},
	language = {en},
	urldate = {2019-05-19},
	journal = {NeuroImage},
	author = {Armeni, Kristijan and Willems, Roel M. and van den Bosch, Antal and Schoffelen, Jan-Mathijs},
	month = may,
	year = {2019},
	keywords = {MEG, entropy, surprisal, neural oscillations, narrative comprehension},
}

@article{devlin_stimulating_2007,
	title = {Stimulating language: insights from {TMS}},
	volume = {130},
	issn = {0006-8950, 1460-2156},
	shorttitle = {Stimulating language},
	doi = {10.1093/brain/awl331},
	language = {en},
	number = {3},
	urldate = {2019-05-19},
	journal = {Brain},
	author = {Devlin, J. T. and Watkins, K. E.},
	month = mar,
	year = {2007},
	pages = {610--622},
}

@article{hickok_cortical_2007,
	title = {The cortical organization of speech processing},
	volume = {8},
	issn = {1471-003X, 1471-0048},
	doi = {10.1038/nrn2113},
	language = {en},
	number = {5},
	urldate = {2019-05-19},
	journal = {Nature Reviews Neuroscience},
	author = {Hickok, Gregory and Poeppel, David},
	month = may,
	year = {2007},
	keywords = {readme, review},
	pages = {393--402},
}

@article{hagoort_muc_2013,
	title = {{MUC} ({Memory}, {Unification}, {Control}) and beyond},
	volume = {4},
	issn = {1664-1078},
	url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2013.00416/abstract},
	doi = {10.3389/fpsyg.2013.00416},
	urldate = {2019-05-19},
	journal = {Frontiers in Psychology},
	author = {Hagoort, Peter},
	year = {2013},
}

@article{friederici_cortical_2012,
	title = {The cortical language circuit: from auditory perception to sentence comprehension},
	volume = {16},
	issn = {13646613},
	shorttitle = {The cortical language circuit},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661312000794},
	doi = {10.1016/j.tics.2012.04.001},
	language = {en},
	number = {5},
	urldate = {2019-05-19},
	journal = {Trends in Cognitive Sciences},
	author = {Friederici, Angela D.},
	month = may,
	year = {2012},
	keywords = {readme},
	pages = {262--268},
}

@article{oreilly_making_2006,
	title = {Making working memory work: {A} computational model of learning in the prefrontal cortex and basal ganglia},
	volume = {18},
	issn = {0899-7667},
	shorttitle = {Making {Working} {Memory} {Work}},
	url = {https://doi.org/10.1162/089976606775093909},
	doi = {10.1162/089976606775093909},
	abstract = {The prefrontal cortex has long been thought to subserve both working memory (the holding of information online for processing) and executive functions (deciding how to manipulate working memory and perform processing). Although many computational models of working memory have been developed, the mechanistic basis of executive function remains elusive, often amounting to a homunculus. This article presents an attempt to deconstruct this homunculus through powerful learning mechanisms that allow a computational model of the prefrontal cortex to control both itself and other brain areas in a strategic, task-appropriate manner. These learning mechanisms are based on subcortical structures in the midbrain, basal ganglia, and amygdala, which together form an actor-critic architecture. The critic system learns which prefrontal representations are task relevant and trains the actor, which in turn provides a dynamic gating mechanism for controlling working memory updating. Computationally, the learning mechanism is designed to simultaneously solve the temporal and structural credit assignment problems. The model's performance compares favorably with standard backpropagation-based temporal learning mechanisms on the challenging 1-2-AX working memory task and other benchmark working memory tasks.},
	number = {2},
	urldate = {2018-07-27},
	journal = {Neural Computation},
	author = {O'Reilly, Randall C. and Frank, Michael J.},
	month = feb,
	year = {2006},
	keywords = {LSTM, working memory, project.lstmMEG, readme, neural networks, project.lsnn, 1-2-AX, recurrent neural network, memory, basal ganglia, PFC},
	pages = {283--328},
	file = {O'Reilly and Frank - 2006 - Making working memory work.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/O'Reilly and Frank - 2006 - Making working memory work.pdf:application/pdf},
}

@article{pedregosa_scikit-learn:_2011,
	title = {Scikit-learn: {Machine} learning in python},
	volume = {12},
	journal = {Journal of Machine Learning Research},
	author = {Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V. and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P. and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
	year = {2011},
	keywords = {methods, machine learning, python},
	pages = {2825--2830},
}

@article{bhattasali_localising_2019,
	title = {Localising memory retrieval and syntactic composition: an {fMRI} study of naturalistic language comprehension},
	volume = {34},
	issn = {2327-3798, 2327-3801},
	shorttitle = {Localising memory retrieval and syntactic composition},
	url = {https://www.tandfonline.com/doi/full/10.1080/23273798.2018.1518533},
	doi = {10.1080/23273798.2018.1518533},
	language = {en},
	number = {4},
	urldate = {2019-04-21},
	journal = {Language, Cognition and Neuroscience},
	author = {Bhattasali, Shohini and Fabre, Murielle and Luh, Wen-Ming and Al Saied, Hazem and Constant, Mathieu and Pallier, Christophe and Brennan, Jonathan R. and Spreng, R. Nathan and Hale, John},
	month = apr,
	year = {2019},
	keywords = {syntax, project.lstmMEG, readme, idioms},
	pages = {491--510},
}

@article{mak_mental_2019,
	title = {Mental simulation during literary reading: {Individual} differences revealed with eye-tracking},
	volume = {34},
	issn = {2327-3798, 2327-3801},
	shorttitle = {Mental simulation during literary reading},
	url = {https://www.tandfonline.com/doi/full/10.1080/23273798.2018.1552007},
	doi = {10.1080/23273798.2018.1552007},
	language = {en},
	number = {4},
	urldate = {2019-04-10},
	journal = {Language, Cognition and Neuroscience},
	author = {Mak, Marloes and Willems, Roel M.},
	month = apr,
	year = {2019},
	keywords = {project.lstmMEG, readme, eye-tracking, narrative comprehension, mental simulation},
	pages = {511--535},
}

@article{kuijpers_exploring_2014,
	title = {Exploring absorbing reading experiences: {Developing} and validating a self-report scale to measure story world absorption},
	volume = {4},
	issn = {2210-4372, 2210-4380},
	shorttitle = {Exploring absorbing reading experiences},
	url = {http://www.jbe-platform.com/content/journals/10.1075/ssol.4.1.05kui},
	doi = {10.1075/ssol.4.1.05kui},
	language = {en},
	number = {1},
	urldate = {2019-04-10},
	journal = {Scientific Study of Literature},
	author = {Kuijpers, Moniek M. and Hakemulder, Frank and Tan, Ed S. and Doicaru, Miruna M.},
	year = {2014},
	keywords = {project.lstmMEG, readme, reading comprehension, absorption, behaviour, experience, narrative absorption},
	pages = {89--122},
}

@article{knoop_mapping_2016,
	title = {Mapping the aesthetic space of literature “from below”},
	volume = {56},
	issn = {0304422X},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0304422X16000127},
	doi = {10.1016/j.poetic.2016.02.001},
	language = {en},
	urldate = {2019-04-10},
	journal = {Poetics},
	author = {Knoop, Christine A. and Wagner, Valentin and Jacobsen, Thomas and Menninghaus, Winfried},
	month = jun,
	year = {2016},
	keywords = {readme, empirical aesthetics, narrative appreciation},
	pages = {35--49},
}

@article{meyer_flexible_2017,
	title = {Flexible head-casts for high spatial precision {MEG}},
	volume = {276},
	issn = {01650270},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0165027016302746},
	doi = {10.1016/j.jneumeth.2016.11.009},
	language = {en},
	urldate = {2019-04-10},
	journal = {Journal of Neuroscience Methods},
	author = {Meyer, Sofie S. and Bonaiuto, James and Lim, Mark and Rossiter, Holly and Waters, Sheena and Bradbury, David and Bestmann, Sven and Brookes, Matthew and Callaghan, Martina F. and Weiskopf, Nikolaus and Barnes, Gareth R.},
	month = jan,
	year = {2017},
	keywords = {MEG, project.lstmMEG},
	pages = {38--45},
}

@article{cho_learning_2014,
	title = {Learning phrase representations using {RNN} encoder-decoder for statistical machine translation},
	url = {http://arxiv.org/abs/1406.1078},
	abstract = {In this paper, we propose a novel neural network model called RNN Encoder–Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a ﬁxedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder–Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
	language = {en},
	urldate = {2019-04-01},
	journal = {arXiv:1406.1078 [cs, stat]},
	author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
	month = jun,
	year = {2014},
	note = {arXiv: 1406.1078},
	keywords = {RNN, project.lstmMEG, neural networks, machine learning, GRU},
}

@article{bzdok_exploration_2019,
	title = {Exploration, inference, and prediction in neuroscience and biomedicine},
	volume = {42},
	issn = {01662236},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0166223619300074},
	doi = {10.1016/j.tins.2019.02.001},
	language = {en},
	number = {4},
	urldate = {2019-03-28},
	journal = {Trends in Neurosciences},
	author = {Bzdok, Danilo and Ioannidis, John P.A.},
	month = apr,
	year = {2019},
	keywords = {deep learning, statistics, neural networks, machine learning, inference, statistical learning, data science},
	pages = {251--262},
}

@article{cichy_deep_2019,
	title = {Deep neural networks as scientific models},
	volume = {23},
	issn = {13646613},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661319300348},
	doi = {10.1016/j.tics.2019.01.009},
	language = {en},
	number = {4},
	urldate = {2019-03-20},
	journal = {Trends in Cognitive Sciences},
	author = {Cichy, Radoslaw M. and Kaiser, Daniel},
	month = apr,
	year = {2019},
	keywords = {readme},
	pages = {305--317},
	file = {Cichy and Kaiser - 2019 - Neural networks as scientific models.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Cichy and Kaiser - 2019 - Neural networks as scientific models.pdf:application/pdf},
}

@article{brennan_hierarchical_2019,
	title = {Hierarchical structure guides rapid linguistic predictions during naturalistic listening},
	volume = {14},
	issn = {1932-6203},
	url = {http://dx.plos.org/10.1371/journal.pone.0207741},
	doi = {10.1371/journal.pone.0207741},
	language = {en},
	number = {1},
	urldate = {2019-03-14},
	journal = {PLOS ONE},
	author = {Brennan, Jonathan R. and Hale, John T.},
	editor = {Bolhuis, Johan J},
	month = jan,
	year = {2019},
	keywords = {read},
	pages = {e0207741},
}

@article{giraud_cortical_2012,
	title = {Cortical oscillations and speech processing: emerging computational principles and operations},
	volume = {15},
	issn = {1097-6256, 1546-1726},
	shorttitle = {Cortical oscillations and speech processing},
	url = {http://www.nature.com/articles/nn.3063},
	doi = {10.1038/nn.3063},
	language = {en},
	number = {4},
	urldate = {2019-03-14},
	journal = {Nature Neuroscience},
	author = {Giraud, Anne-Lise and Poeppel, David},
	month = apr,
	year = {2012},
	keywords = {MEG, read, gamma, theta, thesis.introduction, delta, speech, entrainment},
	pages = {511--517},
}

@article{ding_temporal_2017,
	title = {Temporal modulations in speech and music},
	volume = {81},
	issn = {01497634},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0149763416305668},
	doi = {10.1016/j.neubiorev.2017.02.011},
	language = {en},
	urldate = {2019-03-12},
	journal = {Neuroscience \& Biobehavioral Reviews},
	author = {Ding, Nai and Patel, Aniruddh D. and Chen, Lin and Butler, Henry and Luo, Cheng and Poeppel, David},
	month = oct,
	year = {2017},
	keywords = {neural oscillations, theta, thesis.introduction, delta, entrainment, music, jazz},
	pages = {181--187},
}

@article{martinolli_multi-timescale_2018,
	title = {Multi-timescale memory dynamics extend task repertoire in a reinforcement learning network with attention-gated memory},
	volume = {12},
	issn = {1662-5188},
	url = {https://www.frontiersin.org/article/10.3389/fncom.2018.00050/full},
	doi = {10.3389/fncom.2018.00050},
	urldate = {2019-03-07},
	journal = {Frontiers in Computational Neuroscience},
	author = {Martinolli, Marco and Gerstner, Wulfram and Gilra, Aditya},
	month = jul,
	year = {2018},
	keywords = {project.lstmMEG, readme, project.lsnn, 1-2-AX},
}

@article{alexander_hierarchical_2015,
	title = {Hierarchical error representation: {A} computational model of anterior cingulate and dorsolateral prefrontal cortex},
	volume = {27},
	issn = {0899-7667, 1530-888X},
	shorttitle = {Hierarchical {Error} {Representation}},
	url = {http://www.mitpressjournals.org/doi/10.1162/NECO_a_00779},
	doi = {10.1162/NECO_a_00779},
	language = {en},
	number = {11},
	urldate = {2019-03-07},
	journal = {Neural Computation},
	author = {Alexander, William H. and Brown, Joshua W.},
	month = nov,
	year = {2015},
	keywords = {working memory, readme, neural networks, project.lsnn, 1-2-AX, ACC},
	pages = {2354--2410},
	file = {Alexander and Brown - 2015 - Hierarchical error representation A computational.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Alexander and Brown - 2015 - Hierarchical error representation A computational.pdf:application/pdf},
}

@article{buranyi_dehumanising_2018,
	chapter = {Inequality},
	title = {'{Dehumanising}, impenetrable, frustrating': the grim reality of job hunting in the age of {AI}},
	issn = {0261-3077},
	shorttitle = {'{Dehumanising}, impenetrable, frustrating'},
	url = {https://www.theguardian.com/inequality/2018/mar/04/dehumanising-impenetrable-frustrating-the-grim-reality-of-job-hunting-in-the-age-of-ai},
	abstract = {Recruitment has become increasingly automated, with everything from facial expressions to vocal tone analysed by algorithms and artificial intelligence. But what’s the cost to workforce diversity – and workers themselves?},
	language = {en-GB},
	urldate = {2019-03-04},
	journal = {The Guardian},
	author = {Buranyi, Stephen},
	month = mar,
	year = {2018},
	keywords = {AI, artificial intelligence, explainability, HR, human resources, xAI},
	file = {Snapshot:/Users/kriarm/Zotero/storage/93BYFJK9/dehumanising-impenetrable-frustrating-the-grim-reality-of-job-hunting-in-the-age-of-ai.html:text/html},
}

@article{paton_neural_2018,
	title = {The neural basis of timing: distributed mechanisms for diverse functions},
	volume = {98},
	issn = {08966273},
	shorttitle = {The neural basis of timing},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627318302514},
	doi = {10.1016/j.neuron.2018.03.045},
	language = {en},
	number = {4},
	urldate = {2019-03-03},
	journal = {Neuron},
	author = {Paton, Joseph J. and Buonomano, Dean V.},
	month = may,
	year = {2018},
	keywords = {project.lstmMEG, readme, computational neuroscience, time},
	pages = {687--705},
}

@article{chung_empirical_2014,
	title = {Empirical evaluation of gated recurrent neural networks on sequence modeling},
	url = {http://arxiv.org/abs/1412.3555},
	abstract = {In this paper we compare different types of recurrent units in recurrent neural networks (RNNs). Especially, we focus on more sophisticated units that implement a gating mechanism, such as a long short-term memory (LSTM) unit and a recently proposed gated recurrent unit (GRU). We evaluate these recurrent units on the tasks of polyphonic music modeling and speech signal modeling. Our experiments revealed that these advanced recurrent units are indeed better than more traditional recurrent units such as tanh units. Also, we found GRU to be comparable to LSTM.},
	urldate = {2019-02-28},
	journal = {arXiv:1412.3555 [cs]},
	author = {Chung, Junyoung and Gulcehre, Caglar and Cho, KyungHyun and Bengio, Yoshua},
	month = dec,
	year = {2014},
	note = {arXiv: 1412.3555},
	keywords = {RNN, project.lstmMEG, readme, machine learning, GRU, computer science},
}

@article{fitz_neuronal_2019,
	title = {Neuronal memory for language processing},
	copyright = {© 2019, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
	url = {https://www.biorxiv.org/content/10.1101/546325v1},
	doi = {10.1101/546325},
	abstract = {{\textless}p{\textgreater}In language processing, an interpretation is computed incrementally within memory while utterances unfold in time. Here, we investigate the nature of this processing memory in a spiking network model of sentence comprehension. We show that the history dependence of neuronal responses endows circuits of biological neurons with adequate memory to assign semantic roles and resolve binding relations between words in a stream of language input. A neurobiological read-write memory is proposed where short-lived spiking activity encodes information into coupled dynamic variables that move at slower timescales. This state-dependent network does not rely on persistent activity, excitatory feedback, or synaptic plasticity for storage. Instead, information is maintained in adaptive neuronal conductances and can be accessed directly during comprehension without cued retrieval of previous input words. This work provides a step towards a computational neurobiology of language.{\textless}/p{\textgreater}},
	language = {en},
	urldate = {2019-02-27},
	journal = {bioRxiv},
	author = {Fitz, Hartmut and Uhlmann, Marvin and Broek, Dick van den and Duarte, Renato and Hagoort, Peter and Petersson, Karl Magnus},
	month = feb,
	year = {2019},
	keywords = {readme, project.lsnn, LIF, computational neuroscience},
	pages = {546325},
	file = {Fitz et al. - 2019 - Neuronal memory for language processing.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Fitz et al. - 2019 - Neuronal memory for language processing.pdf:application/pdf},
}

@article{yogatama_memory_2018-1,
	title = {Memory architectures in recurrent neural network language models},
	url = {https://openreview.net/forum?id=SkFqf0lAZ},
	abstract = {We compare and analyze sequential, random access, and stack memory architectures for recurrent neural network language models. Our experiments on the Penn Treebank and Wikitext-2 datasets show that...},
	urldate = {2019-02-20},
	author = {Yogatama, Dani and Miao, Yishu and Melis, Gabor and Ling, Wang and Kuncoro, Adhiguna and Dyer, Chris and Blunsom, Phil},
	month = feb,
	year = {2018},
	keywords = {language model, LM, project.lstmMEG, readme, memory networks},
	file = {Yogatama et al. - 2018 - Memory architectures in recurrent neural network l.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Yogatama et al. - 2018 - Memory architectures in recurrent neural network l.pdf:application/pdf},
}

@article{weston_memory_2014,
	title = {Memory networks},
	url = {http://arxiv.org/abs/1410.3916},
	abstract = {We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chaining multiple supporting sentences to answer questions that require understanding the intension of verbs.},
	urldate = {2019-02-20},
	journal = {arXiv:1410.3916 [cs, stat]},
	author = {Weston, Jason and Chopra, Sumit and Bordes, Antoine},
	month = oct,
	year = {2014},
	note = {arXiv: 1410.3916},
	keywords = {project.lstmMEG, machine learning, memory networks},
	file = {Weston et al. - 2014 - Memory networks.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Weston et al. - 2014 - Memory networks.pdf:application/pdf},
}

@article{kukushkin_memory_2017,
	title = {Memory takes time},
	volume = {95},
	issn = {08966273},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627317304671},
	doi = {10.1016/j.neuron.2017.05.029},
	language = {en},
	number = {2},
	urldate = {2019-01-30},
	journal = {Neuron},
	author = {Kukushkin, Nikolay Vadimovich and Carew, Thomas James},
	month = jul,
	year = {2017},
	keywords = {project.lstmMEG, readme, neuroscience, processing memory, memory},
	pages = {259--279},
}

@article{sukhbaatar_end--end_2015-1,
	title = {End-to-end memory networks},
	url = {http://arxiv.org/abs/1503.08895},
	abstract = {We introduce a neural network with a recurrent attention model over a possibly large external memory. The architecture is a form of Memory Network [23] but unlike the model in that work, it is trained end-to-end, and hence requires signiﬁcantly less supervision during training, making it more generally applicable in realistic settings. It can also be seen as an extension of RNNsearch [2] to the case where multiple computational steps (hops) are performed per output symbol. The ﬂexibility of the model allows us to apply it to tasks as diverse as (synthetic) question answering [22] and to language modeling. For the former our approach is competitive with Memory Networks, but with less supervision. For the latter, on the Penn TreeBank and Text8 datasets our approach demonstrates comparable performance to RNNs and LSTMs. In both cases we show that the key concept of multiple computational hops yields improved results.},
	language = {en},
	urldate = {2019-02-19},
	journal = {arXiv:1503.08895 [cs]},
	author = {Sukhbaatar, Sainbayar and Szlam, Arthur and Weston, Jason and Fergus, Rob},
	month = mar,
	year = {2015},
	note = {arXiv: 1503.08895},
	file = {Sukhbaatar et al. - 2015 - End-to-end memory networks.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Sukhbaatar et al. - 2015 - End-to-end memory networks.pdf:application/pdf},
}

@article{costa_cortical_2017,
	title = {Cortical microcircuits as gated-recurrent neural networks},
	url = {http://arxiv.org/abs/1711.02448},
	abstract = {Cortical circuits exhibit intricate recurrent architectures that are remarkably similar across different brain areas. Such stereotyped structure suggests the existence of common computational principles. However, such principles have remained largely elusive. Inspired by gated-memory networks, namely long short-term memory networks (LSTMs), we introduce a recurrent neural network in which information is gated through inhibitory cells that are subtractive (subLSTM). We propose a natural mapping of subLSTMs onto known canonical excitatory-inhibitory cortical microcircuits. Our empirical evaluation across sequential image classification and language modelling tasks shows that subLSTM units can achieve similar performance to LSTM units. These results suggest that cortical circuits can be optimised to solve complex contextual problems and proposes a novel view on their computational function. Overall our work provides a step towards unifying recurrent networks as used in machine learning with their biological counterparts.},
	urldate = {2019-02-18},
	journal = {arXiv:1711.02448 [cs, q-bio, stat]},
	author = {Costa, Rui Ponte and Assael, Yannis M. and Shillingford, Brendan and de Freitas, Nando and Vogels, Tim P.},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.02448},
	keywords = {deep learning, LSTM, project.lstmMEG, read},
	file = {Costa et al. - 2017 - Cortical microcircuits as gated-recurrent neural n.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Costa et al. - 2017 - Cortical microcircuits as gated-recurrent neural n.pdf:application/pdf},
}

@article{brysbaert_how_2016,
	title = {How many words do we know? {Practical} wstimates of vocabulary size dependent on word definition, the degree of language input and the participant’s age},
	volume = {7},
	issn = {1664-1078},
	shorttitle = {How {Many} {Words} {Do} {We} {Know}?},
	url = {http://journal.frontiersin.org/Article/10.3389/fpsyg.2016.01116/abstract},
	doi = {10.3389/fpsyg.2016.01116},
	urldate = {2019-02-17},
	journal = {Frontiers in Psychology},
	author = {Brysbaert, Marc and Stevens, Michaël and Mandera, Paweł and Keuleers, Emmanuel},
	month = jul,
	year = {2016},
	keywords = {mental lexicon, psycholinguistics, vocabulary},
}

@book{maley_unified_2018,
	title = {A unified mechanistic account of teleological functions for psychology and neuroscience},
	volume = {1},
	url = {http://www.oxfordscholarship.com/view/10.1093/oso/9780199685509.001.0001/oso-9780199685509-chapter-11},
	language = {en},
	urldate = {2019-02-14},
	publisher = {Oxford University Press},
	author = {Maley, Corey J. and Piccinini, Gualtiero},
	month = jan,
	year = {2018},
	doi = {10.1093/oso/9780199685509.003.0011},
	keywords = {explanation, philosophy of science, computation, philosohpy of neuroscience, function, teleology},
	file = {Maley and Piccinini - 2018 - A unified mechanistic account of teleological func.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Maley and Piccinini - 2018 - A unified mechanistic account of teleological func.pdf:application/pdf},
}

@article{whittington_theories_2019,
	title = {Theories of error back-propagation in the brain},
	volume = {23},
	issn = {13646613},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661319300129},
	doi = {10.1016/j.tics.2018.12.005},
	language = {en},
	number = {3},
	urldate = {2019-02-14},
	journal = {Trends in Cognitive Sciences},
	author = {Whittington, James C.R. and Bogacz, Rafal},
	month = mar,
	year = {2019},
	keywords = {deep learning, backprop, project.lstmMEG, read, machine learning, project.lsnn},
	pages = {235--250},
}

@inproceedings{kuncoro_lstms_2018,
	address = {Melbourne, Australia},
	title = {{LSTMs} can learn syntax-sensitive dependencies well, but modeling structure makes them better},
	url = {http://aclweb.org/anthology/P18-1132},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Kuncoro, Adhiguna and Dyer, Chris and Hale, John and Yogatama, Dani and Clark, Stephen and Blunsom, Phil},
	year = {2018},
	keywords = {readme},
	pages = {1426--1436},
}

@article{tavanaei_deep_2019,
	title = {Deep learning in spiking neural networks},
	volume = {111},
	issn = {0893-6080},
	url = {http://www.sciencedirect.com/science/article/pii/S0893608018303332},
	doi = {10.1016/j.neunet.2018.12.002},
	abstract = {In recent years, deep learning has revolutionized the field of machine learning, for computer vision in particular. In this approach, a deep (multilayer) artificial neural network (ANN) is trained, most often in a supervised manner using backpropagation. Vast amounts of labeled training examples are required, but the resulting classification accuracy is truly impressive, sometimes outperforming humans. Neurons in an ANN are characterized by a single, static, continuous-valued activation. Yet biological neurons use discrete spikes to compute and transmit information, and the spike times, in addition to the spike rates, matter. Spiking neural networks (SNNs) are thus more biologically realistic than ANNs, and are arguably the only viable option if one wants to understand how the brain computes at the neuronal description level. The spikes of biological neurons are sparse in time and space, and event-driven. Combined with bio-plausible local learning rules, this makes it easier to build low-power, neuromorphic hardware for SNNs. However, training deep SNNs remains a challenge. Spiking neurons’ transfer function is usually non-differentiable, which prevents using backpropagation. Here we review recent supervised and unsupervised methods to train deep SNNs, and compare them in terms of accuracy and computational cost. The emerging picture is that SNNs still lag behind ANNs in terms of accuracy, but the gap is decreasing, and can even vanish on some tasks, while SNNs typically require many fewer operations and are the better candidates to process spatio-temporal data.},
	urldate = {2019-02-05},
	journal = {Neural Networks},
	author = {Tavanaei, Amirhossein and Ghodrati, Masoud and Kheradpisheh, Saeed Reza and Masquelier, Timothée and Maida, Anthony},
	month = mar,
	year = {2019},
	keywords = {neural networks, machine learning, SNN},
	pages = {47--63},
}

@book{dayan_theoretical_2001,
	address = {Cambridge, Mass},
	series = {Computational neuroscience},
	title = {Theoretical neuroscience: computational and mathematical modeling of neural systems},
	isbn = {978-0-262-04199-7},
	shorttitle = {Theoretical neuroscience},
	language = {en},
	publisher = {Massachusetts Institute of Technology Press},
	author = {Dayan, Peter and Abbott, L. F.},
	year = {2001},
	file = {Dayan and Abbott - 2001 - Theoretical neuroscience computational and mathem.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Dayan and Abbott - 2001 - Theoretical neuroscience computational and mathem.pdf:application/pdf},
}

@article{tavanaei_deep_2018,
	title = {Deep learning in spiking neural networks},
	url = {http://arxiv.org/abs/1804.08150},
	abstract = {In recent years, deep learning has been a revolution in the field of machine learning, for computer vision in particular. In this approach, a deep (multilayer) artificial neural network (ANN) is trained in a supervised manner using backpropagation. Huge amounts of labeled examples are required, but the resulting classification accuracy is truly impressive, sometimes outperforming humans. Neurons in an ANN are characterized by a single, static, continuous-valued activation. Yet biological neurons use discrete spikes to compute and transmit information, and the spike times, in addition to the spike rates, matter. Spiking neural networks (SNNs) are thus more biologically realistic than ANNs, and arguably the only viable option if one wants to understand how the brain computes. SNNs are also more hardware friendly and energy-efficient than ANNs, and are thus appealing for technology, especially for portable devices. However, training deep SNNs remains a challenge. Spiking neurons' transfer function is usually non-differentiable, which prevents using backpropagation. Here we review recent supervised and unsupervised methods to train deep SNNs, and compare them in terms of accuracy, but also computational cost and hardware friendliness. The emerging picture is that SNNs still lag behind ANNs in terms of accuracy, but the gap is decreasing, and can even vanish on some tasks, while the SNNs typically require much fewer operations.},
	urldate = {2018-09-24},
	journal = {arXiv:1804.08150 [cs]},
	author = {Tavanaei, Amirhossein and Ghodrati, Masoud and Kheradpisheh, Saeed Reza and Masquelier, Timothee and Maida, Anthony S.},
	month = apr,
	year = {2018},
	note = {arXiv: 1804.08150},
	keywords = {readme, reading},
	file = {Tavanaei et al. - 2018 - Deep learning in spiking neural networks.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Tavanaei et al. - 2018 - Deep learning in spiking neural networks.pdf:application/pdf},
}

@article{yuan_speaker_2008,
	title = {Speaker identification on the {SCOTUS} corpus},
	volume = {123},
	issn = {0001-4966},
	url = {http://asa.scitation.org/doi/10.1121/1.2935783},
	doi = {10.1121/1.2935783},
	language = {en},
	number = {5},
	urldate = {2019-01-25},
	journal = {The Journal of the Acoustical Society of America},
	author = {Yuan, Jiahong and Liberman, Mark},
	month = may,
	year = {2008},
	keywords = {project.lstmMEG, readme, speech, acoustics, forced alignment, P2FA},
	pages = {3878--3878},
}

@article{willems_early_2008,
	title = {Early decreases in alpha and gamma band power distinguish linguistic from visual information during spoken sentence comprehension},
	volume = {1219},
	issn = {00068993},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0006899308009864},
	doi = {10.1016/j.brainres.2008.04.065},
	language = {en},
	urldate = {2019-01-22},
	journal = {Brain Research},
	author = {Willems, Roel M. and Oostenveld, Robert and Hagoort, Peter},
	month = jul,
	year = {2008},
	keywords = {project.streams, readme, neural oscillations, gamma, alpha},
	pages = {78--90},
}

@article{spitzer_beyond_2017,
	title = {Beyond the {Status} {Quo}: {A} {Role} for {Beta} {Oscillations} in {Endogenous} {Content} ({Re}){Activation}},
	volume = {4},
	issn = {2373-2822},
	shorttitle = {Beyond the {Status} {Quo}},
	url = {http://eneuro.org/cgi/doi/10.1523/ENEURO.0170-17.2017},
	doi = {10.1523/ENEURO.0170-17.2017},
	language = {en},
	number = {4},
	urldate = {2019-01-22},
	journal = {eneuro},
	author = {Spitzer, Bernhard and Haegens, Saskia},
	year = {2017},
	keywords = {project.streams, readme, prediction, beta, neural oscillations},
	pages = {ENEURO.0170--17.2017},
}

@article{bastiaansen_syntactic_2010,
	title = {Syntactic unification operations are reflected in oscillatory dynamics during on-line sentence comprehension},
	volume = {22},
	issn = {0898-929X, 1530-8898},
	doi = {10.1162/jocn.2009.21283},
	language = {en},
	number = {7},
	urldate = {2018-03-14},
	journal = {Journal of Cognitive Neuroscience},
	author = {Bastiaansen, Marcel C.M. and Magyari, Lilla and Hagoort, Peter},
	month = jul,
	year = {2010},
	keywords = {project.streams, read, language, oscillations},
	pages = {1333--1347},
}

@article{bastiaansen_frequency-based_2015,
	title = {Frequency-based segregation of syntactic and semantic unification during online sentence level language comprehension},
	volume = {27},
	issn = {0898-929X, 1530-8898},
	doi = {10.1162/jocn_a_00829},
	language = {en},
	number = {11},
	journal = {Journal of Cognitive Neuroscience},
	author = {Bastiaansen, Marcel C.M. and Hagoort, Peter},
	month = nov,
	year = {2015},
	keywords = {project.streams, read, language, oscillations},
	pages = {2095--2107},
}

@incollection{bastiaansen_oscillatory_2006,
	series = {Event-{Related} {Dynamics} of {Brain} {Oscillations}},
	title = {Oscillatory neuronal dynamics during language comprehension},
	volume = {159},
	isbn = {978-0-444-52183-5},
	abstract = {Language comprehension involves two basic operations: the retrieval of lexical information (such as phonologic, syntactic, and semantic information) from long-term memory, and the unification of this information into a coherent representation of the overall utterance. Neuroimaging studies using hemodynamic measures such as PET and fMRI have provided detailed information on which areas of the brain are involved in these language-related memory and unification operations. However, much less is known about the dynamics of the brain's language network. This chapter presents a literature review of the oscillatory neuronal dynamics of EEG and MEG data that can be observed during language comprehension tasks. From a detailed review of this (rapidly growing) literature the following picture emerges: memory retrieval operations are mostly accompanied by increased neuronal synchronization in the theta frequency range (4–7Hz). Unification operations, in contrast, induce high-frequency neuronal synchronization in the beta (12–30Hz) and gamma (above 30Hz) frequency bands. A desynchronization in the (upper) alpha frequency band is found for those studies that use secondary tasks, and seems to correspond with attentional processes, and with the behavioral consequences of the language comprehension process. We conclude that it is possible to capture the dynamics of the brain's language network by a careful analysis of the event-related changes in power and coherence of EEG and MEG data in a wide range of frequencies, in combination with subtle experimental manipulations in a range of language comprehension tasks. It appears then that neuronal synchrony is a mechanism by which the brain integrates the different types of information about language (such as phonological, orthographic, semantic, and syntactic information) represented in different brain areas.},
	urldate = {2018-01-10},
	booktitle = {Progress in {Brain} {Research}},
	publisher = {Elsevier},
	author = {Bastiaansen, Marcel C.M. and Hagoort, Peter},
	editor = {Neuper, Christa and Klimesch, Wolfgang},
	month = jan,
	year = {2006},
	doi = {10.1016/S0079-6123(06)59012-0},
	keywords = {EEG, project.streams, read, neural oscillations, language},
	pages = {179--196},
}

@article{lee_training_2016,
	title = {Training deep spiking neural networks using backpropagation},
	volume = {10},
	issn = {1662-453X},
	url = {http://journal.frontiersin.org/article/10.3389/fnins.2016.00508/full},
	doi = {10.3389/fnins.2016.00508},
	urldate = {2019-01-17},
	journal = {Frontiers in Neuroscience},
	author = {Lee, Jun Haeng and Delbruck, Tobi and Pfeiffer, Michael},
	month = nov,
	year = {2016},
	keywords = {deep learning, RNN, backprop, machine learning, ANN, LIF, reading, WTA},
	file = {Lee et al. - 2016 - Training Deep Spiking Neural Networks Using Backpr.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Lee et al. - 2016 - Training Deep Spiking Neural Networks Using Backpr.pdf:application/pdf},
}

@book{bailey_improvisation:_1993,
	address = {New York},
	title = {Improvisation: its nature and practice in music},
	isbn = {978-0-306-80528-8},
	shorttitle = {Improvisation},
	publisher = {Da Capo Press},
	author = {Bailey, Derek},
	year = {1993},
	keywords = {music, improvisation, musicology},
}

@article{pfeiffer_deep_2018,
	title = {Deep learning with spiking neurons: {Opportunities} and challenges},
	volume = {12},
	issn = {1662-453X},
	shorttitle = {Deep {Learning} {With} {Spiking} {Neurons}},
	url = {https://www.frontiersin.org/article/10.3389/fnins.2018.00774/full},
	doi = {10.3389/fnins.2018.00774},
	urldate = {2019-01-12},
	journal = {Frontiers in Neuroscience},
	author = {Pfeiffer, Michael and Pfeil, Thomas},
	month = oct,
	year = {2018},
	keywords = {deep learning, read, machine learning, project.lsnn, LIF, lsnn, SNN},
}

@article{kopell_gamma_2000,
	title = {Gamma rhythms and beta rhythms have different synchronization properties},
	volume = {97},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/cgi/doi/10.1073/pnas.97.4.1867},
	doi = {10.1073/pnas.97.4.1867},
	language = {en},
	number = {4},
	urldate = {2019-01-09},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Kopell, N. and Ermentrout, G. B. and Whittington, M. A. and Traub, R. D.},
	month = feb,
	year = {2000},
	keywords = {readme, beta, neural oscillations, gamma, computational neuroscience, oscillations},
	pages = {1867--1872},
}

@incollection{sacramento_dendritic_2018,
	title = {Dendritic cortical microcircuits approximate the backpropagation algorithm},
	url = {http://papers.nips.cc/paper/8089-dendritic-cortical-microcircuits-approximate-the-backpropagation-algorithm.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31},
	publisher = {Curran Associates, Inc.},
	author = {Sacramento, João and Ponte Costa, Rui and Bengio, Yoshua and Senn, Walter},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
	keywords = {machine learning, spiking neurons, reading, dendritic computation},
	pages = {8735--8746},
}

@book{pinker_sense_2015,
	address = {London},
	title = {The sense of style: the thinking person's guide to writing in the 21st century},
	isbn = {978-0-241-95771-4},
	shorttitle = {The sense of style},
	language = {eng},
	publisher = {Penguin Books},
	author = {Pinker, Steven},
	year = {2015},
	note = {OCLC: 935890801},
	keywords = {writing, academic writing, style},
}

@book{pieraccini_voice_2012,
	title = {The voice in the machine},
	isbn = {978-0-262-01685-8},
	url = {https://mitpress.mit.edu/books/voice-machine},
	abstract = {An examination of more than sixty years of successes and failures in developing technologies that allow computers to understand human spoken language.
                Stanley Kubrick's 1968 film 2001: A Space Odyssey famously featured HAL, a computer with the ability to hold lengthy conversations with his fellow space travelers. More than forty years later, we have advanced computer technology that Kubrick never imagined, but we do not have computers that talk and understand speech as HAL did. Is it a failure of our technology that we have not gotten much further than an automated voice that tells us to “say or press 1”? Or is there something fundamental in human language and speech that we do not yet understand deeply enough to be able to replicate in a computer? In The Voice in the Machine, Roberto Pieraccini examines six decades of work in science and technology to develop computers that can interact with humans using speech and the industry that has arisen around the quest for these technologies. He shows that although the computers today that understand speech may not have HAL's capacity for conversation, they have capabilities that make them usable in many applications today and are on a fast track of improvement and innovation.Pieraccini describes the evolution of speech recognition and speech understanding processes from waveform methods to artificial intelligence approaches to statistical learning and modeling of human speech based on a rigorous mathematical model—specifically, Hidden Markov Models (HMM). He details the development of dialog systems, the ability to produce speech, and the process of bringing talking machines to the market. Finally, he asks a question that only the future can answer: will we end up with HAL-like computers or something completely unexpected?},
	language = {en},
	urldate = {2018-12-17},
	publisher = {MIT Press},
	author = {Pieraccini, Roberto},
	year = {2012},
	keywords = {AI, machine learning, artificial intelligence, speec recognition},
	file = {Snapshot:/Users/kriarm/Zotero/storage/PYPWZG2F/voice-machine.html:text/html},
}

@book{hofstadter_go_1999,
	address = {New York},
	edition = {20th anniversary ed},
	title = {Gödel, {Escher}, {Bach}: an eternal golden braid},
	isbn = {978-0-394-75682-0 978-0-465-02656-2},
	shorttitle = {Gödel, {Escher}, {Bach}},
	publisher = {Basic Books},
	author = {Hofstadter, Douglas R.},
	year = {1999},
	keywords = {computation, neuroscience, artificial intelligence, improvisation, Bach, Turing test},
}

@book{patel_music_2010,
	address = {Oxford},
	edition = {first edition},
	title = {Music, language, and the brain},
	isbn = {978-0-19-975530-1},
	language = {eng},
	publisher = {Oxford Univ. Press},
	author = {Patel, Aniruddh D.},
	year = {2010},
	note = {OCLC: 845933649},
}

@article{beaty_neuroscience_2015,
	title = {The neuroscience of musical improvisation},
	volume = {51},
	issn = {01497634},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0149763415000068},
	doi = {10.1016/j.neubiorev.2015.01.004},
	language = {en},
	urldate = {2019-01-04},
	journal = {Neuroscience \& Biobehavioral Reviews},
	author = {Beaty, Roger E.},
	month = apr,
	year = {2015},
	keywords = {music, jazz, improvisation},
	pages = {108--117},
}

@book{nachmanovitch_free_2010,
	address = {New York},
	title = {Free play: improvisation in life and art},
	isbn = {978-0-87477-631-7},
	shorttitle = {Free play},
	language = {eng},
	publisher = {Tarcher/Putnam},
	author = {Nachmanovitch, Stephen},
	year = {2010},
	note = {OCLC: 837931560},
	keywords = {music, improvisation, improvisation theatre},
}

@misc{thompson_one_2013,
	title = {One on one},
	url = {https://www.youtube.com/watch?v=rWOC-SMC-Fc},
	urldate = {2019-01-04},
	publisher = {NBA TV},
	collaborator = {Thompson, Andy and Rashad, Ahmad and Jordan, Michael},
	year = {2013},
	file = {(18) Ahmad Rashad One On One Interview With Michael Jordan @ Age 50 - 2nd Three-Peat (1996-98) - YouTube:/Users/kriarm/Zotero/storage/MHLYLUPD/watch.html:text/html},
}

@book{werner_effortless_1996,
	address = {New Albany, Ind},
	title = {Effortless mastery: liberating the master musician within},
	isbn = {978-1-56224-003-5},
	shorttitle = {Effortless {Mastery}},
	language = {eng},
	publisher = {Jamey Aebersold Jazz},
	author = {Werner, Kenny},
	year = {1996},
	note = {OCLC: 885430669},
	keywords = {language, music, jazz, improvisation},
}

@misc{wooten_music_2013,
	title = {Music as a {Language}},
	url = {https://www.youtube.com/watch?v=2zvjW9arAZ0&t=190s},
	urldate = {2019-01-04},
	publisher = {TEDx Talks},
	author = {Wooten, Victor},
	month = may,
	year = {2013},
	keywords = {learning, language, music, jazz, improvisation, bass},
	file = {(18) Music as a Language\: Victor Wooten at TEDxGabriolaIsland - YouTube:/Users/kriarm/Zotero/storage/WXQ4GIWS/watch.html:text/html},
}

@book{steinel_building_1995,
	title = {Building a jazz vocabulary: {A} resource for learning jazz improvisation},
	shorttitle = {Building a {Jazz} {Vocabulary}},
	language = {English},
	publisher = {Hal Leonard},
	author = {Steinel, Mike},
	month = jan,
	year = {1995},
}

@incollection{woodward_explanation_2018,
	title = {Explanation in neurobiology. {An} interventionist perspective.},
	volume = {1},
	url = {http://www.oxfordscholarship.com/view/10.1093/oso/9780199685509.001.0001/oso-9780199685509-chapter-4},
	language = {en},
	urldate = {2018-12-26},
	booktitle = {Explanation and {Integration} in {Mind} and {Brain} {Science}},
	publisher = {Oxford University Press},
	author = {Woodward, James},
	editor = {Kaplan, David M.},
	month = jan,
	year = {2018},
	doi = {10.1093/oso/9780199685509.003.0004},
	pages = {70--100},
}

@book{sejnowski_deep_2018,
	address = {Cambridge, Massachusetts},
	title = {The deep learning revolution},
	isbn = {978-0-262-03803-4},
	publisher = {The MIT Press},
	author = {Sejnowski, Terrence J.},
	year = {2018},
	keywords = {deep learning, AI, artificial intelligence, big data},
}

@inproceedings{martens_word_2002,
	address = {Las Palmas de Gran Canaria, Spain},
	title = {Word segmentation in the {Spoken} {Dutch} corpus},
	url = {http://www.lrec-conf.org/proceedings/lrec2002/pdf/97.pdf},
	booktitle = {Proceedings of {LREC2002}},
	author = {Martens, Jean-Pierre and Binnenpoorte, Diana and Demuynck, Kris and Van Parys and Laureys, Tom and Wim, Goedertier and Duchateau, Jacques},
	year = {2002},
	keywords = {project.streams, forced alignment, Corpus Gesproken Nederlands},
	pages = {1432--1437},
}

@article{ghosh-dastidar_spiking_2009,
	title = {Spiking neural networks},
	volume = {19},
	issn = {0129-0657, 1793-6462},
	url = {http://www.worldscientific.com/doi/abs/10.1142/S0129065709002002},
	doi = {10.1142/S0129065709002002},
	language = {en},
	number = {04},
	urldate = {2018-12-18},
	journal = {International Journal of Neural Systems},
	author = {Ghosh-Dastidar, Samanwoy and Adeli, Hojjat},
	month = aug,
	year = {2009},
	keywords = {project.lsnn, LSNN},
	pages = {295--308},
}

@article{kutas_thirty_2011,
	title = {Thirty years and counting: finding meaning in the {N400} component of the event-related brain potential ({ERP})},
	volume = {62},
	issn = {0066-4308, 1545-2085},
	shorttitle = {Thirty {Years} and {Counting}},
	url = {http://www.annualreviews.org/doi/10.1146/annurev.psych.093008.131123},
	doi = {10.1146/annurev.psych.093008.131123},
	language = {en},
	number = {1},
	urldate = {2018-12-17},
	journal = {Annual Review of Psychology},
	author = {Kutas, Marta and Federmeier, Kara D.},
	month = jan,
	year = {2011},
	keywords = {N400, EEG, project.streams, readme, language comprehension, semantics, 1},
	pages = {621--647},
	file = {Accepted Version:/Users/kriarm/Zotero/storage/YU577P6R/Kutas and Federmeier - 2011 - Thirty Years and Counting Finding Meaning in the .pdf:application/pdf},
}

@article{knill_bayesian_2004,
	title = {The {Bayesian} brain: the role of uncertainty in neural coding and computation},
	volume = {27},
	issn = {01662236},
	shorttitle = {The {Bayesian} brain},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0166223604003352},
	doi = {10.1016/j.tins.2004.10.007},
	language = {en},
	number = {12},
	urldate = {2018-12-17},
	journal = {Trends in Neurosciences},
	author = {Knill, David C. and Pouget, Alexandre},
	month = dec,
	year = {2004},
	keywords = {project.streams, readme, probability theory, computational neuroscience, Bayesian brain, Bayes theorem},
	pages = {712--719},
}

@article{dubey_probabilistic_2013,
	title = {Probabilistic modeling of discourse-aware sentence processing},
	volume = {5},
	issn = {17568757},
	url = {http://doi.wiley.com/10.1111/tops.12023},
	doi = {10.1111/tops.12023},
	language = {en},
	number = {3},
	urldate = {2020-09-08},
	journal = {Topics in Cognitive Science},
	author = {Dubey, Amit and Keller, Frank and Sturt, Patrick},
	month = jul,
	year = {2013},
	pages = {425--451},
}

@article{hagoort_beyond_2007,
	title = {Beyond the sentence given},
	volume = {362},
	issn = {0962-8436, 1471-2970},
	url = {https://royalsocietypublishing.org/doi/10.1098/rstb.2007.2089},
	doi = {10.1098/rstb.2007.2089},
	language = {en},
	number = {1481},
	urldate = {2020-09-08},
	journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	author = {Hagoort, Peter and van Berkum, Jos},
	month = may,
	year = {2007},
	pages = {801--811},
}

@article{marcus_atoms_2014,
	title = {The atoms of neural computation},
	volume = {346},
	issn = {0036-8075, 1095-9203},
	url = {https://www.sciencemag.org/lookup/doi/10.1126/science.1261661},
	doi = {10.1126/science.1261661},
	language = {en},
	number = {6209},
	urldate = {2020-09-08},
	journal = {Science},
	author = {Marcus, G. and Marblestone, A. and Dean, T.},
	month = oct,
	year = {2014},
	pages = {551--552},
}

@inproceedings{vanschijndel_evidence_2015,
	address = {Denver, Colorado},
	title = {Evidence of syntactic working memory usage in {MEG} data},
	booktitle = {Proceedings of the 6th workshop on cognitive modeling and computational linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Van Schijndel, Marten and Murphy, Brian and Schuler, William},
	year = {2015},
	keywords = {MEG, syntax, thesis, thesis.lmrev},
	pages = {79--88},
}

@article{mazzoni_computing_2015,
	title = {Computing the local field potential ({LFP}) from integrate-and-fire network models},
	volume = {11},
	issn = {1553-734X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4682791/},
	doi = {10.1371/journal.pcbi.1004584},
	abstract = {Leaky integrate-and-fire (LIF) network models are commonly used to study how the spiking dynamics of neural networks changes with stimuli, tasks or dynamic network states. However, neurophysiological studies in vivo often rather measure the mass activity of neuronal microcircuits with the local field potential (LFP). Given that LFPs are generated by spatially separated currents across the neuronal membrane, they cannot be computed directly from quantities defined in models of point-like LIF neurons. Here, we explore the best approximation for predicting the LFP based on standard output from point-neuron LIF networks. To search for this best “LFP proxy”, we compared LFP predictions from candidate proxies based on LIF network output (e.g, firing rates, membrane potentials, synaptic currents) with “ground-truth” LFP obtained when the LIF network synaptic input currents were injected into an analogous three-dimensional (3D) network model of multi-compartmental neurons with realistic morphology, spatial distributions of somata and synapses. We found that a specific fixed linear combination of the LIF synaptic currents provided an accurate LFP proxy, accounting for most of the variance of the LFP time course observed in the 3D network for all recording locations. This proxy performed well over a broad set of conditions, including substantial variations of the neuronal morphologies. Our results provide a simple formula for estimating the time course of the LFP from LIF network simulations in cases where a single pyramidal population dominates the LFP generation, and thereby facilitate quantitative comparison between computational models and experimental LFP recordings in vivo., Leaky integrate-and-fire (LIF) networks are often used to model neural network activity. The spike trains they produce, however, cannot be directly compared to the local field potentials (LFPs) that are measured by low-pass filtering the potential recorded from extracellular electrodes. This is because LFPs are generated by neurons with spatial extensions, while LIF networks typically consist of point neurons. In order to still be able to approximately predict LFPs from LIF network simulations, we here explore simple proxies for computing LFPs based on standard output from LIF network simulations. Predictions from the various LFP proxies were compared with “ground-truth” LFPs computed by means of well-established volume conduction theory where synaptic currents corresponding to the LIF network simulation were injected into populations of multi-compartmental neurons with realistic morphologies. We found that a simple weighted sum of the LIF synaptic currents with a single universally applicable set of weights excellently capture the time course of the LFP signal when the LFP predominantly is generated by a single population of pyramidal cells. Our study therefore provides a simple formula by which the LFP signal can be estimated directly from the LIF network activity, providing a missing quantitative link between simple neural models and LFP measures in vivo.},
	number = {12},
	urldate = {2020-09-04},
	journal = {PLoS Computational Biology},
	author = {Mazzoni, Alberto and Lindén, Henrik and Cuntz, Hermann and Lansner, Anders and Panzeri, Stefano and Einevoll, Gaute T.},
	month = dec,
	year = {2015},
	pmid = {26657024},
	pmcid = {PMC4682791},
}

@article{toneva_interpreting_2019,
	title = {Interpreting and improving natural-language processing (in machines) with natural language-processing (in the brain)},
	url = {http://arxiv.org/abs/1905.11833},
	abstract = {Neural networks models for NLP are typically implemented without the explicit encoding of language rules and yet they are able to break one performance record after another. This has generated a lot of research interest in interpreting the representations learned by these networks. We propose here a novel interpretation approach that relies on the only processing system we have that does understand language: the human brain. We use brain imaging recordings of subjects reading complex natural text to interpret word and sequence embeddings from 4 recent NLP models - ELMo, USE, BERT and Transformer-XL. We study how their representations differ across layer depth, context length, and attention type. Our results reveal differences in the context-related representations across these models. Further, in the transformer models, we find an interaction between layer depth and context length, and between layer depth and attention type. We finally hypothesize that altering BERT to better align with brain recordings would enable it to also better understand language. Probing the altered BERT using syntactic NLP tasks reveals that the model with increased brain-alignment outperforms the original model. Cognitive neuroscientists have already begun using NLP networks to study the brain, and this work closes the loop to allow the interaction between NLP and cognitive neuroscience to be a true cross-pollination.},
	urldate = {2020-08-19},
	journal = {arXiv:1905.11833 [cs, q-bio]},
	author = {Toneva, Mariya and Wehbe, Leila},
	month = nov,
	year = {2019},
	note = {arXiv: 1905.11833},
}

@article{lake_word_2020,
	title = {Word meaning in minds and machines},
	url = {http://arxiv.org/abs/2008.01766},
	abstract = {Machines show an increasingly broad set of linguistic competencies, thanks to recent progress in Natural Language Processing (NLP). Many algorithms stem from past computational work in psychology, raising the question of whether they understand words as people do. In this paper, we compare how humans and machines represent the meaning of words. We argue that contemporary NLP systems are promising models of human word similarity, but they fall short in many other respects. Current models are too strongly linked to the text-based patterns in large corpora, and too weakly linked to the desires, goals, and beliefs that people use words in order to express. Word meanings must also be grounded in vision and action, and capable of flexible combinations, in ways that current systems are not. We pose concrete challenges for developing machines with a more human-like, conceptual basis for word meaning. We also discuss implications for cognitive science and NLP.},
	urldate = {2020-09-02},
	journal = {arXiv:2008.01766 [cs]},
	author = {Lake, Brenden M. and Murphy, Gregory L.},
	month = aug,
	year = {2020},
	note = {arXiv: 2008.01766},
}

@article{bisk_experience_2020,
	title = {Experience grounds language},
	url = {http://arxiv.org/abs/2004.10151},
	abstract = {Successful linguistic communication relies on a shared experience of the world, and it is this shared experience that makes utterances meaningful. Despite the incredible effectiveness of language processing models trained on text alone, today's best systems still make mistakes that arise from a failure to relate language to the physical world it describes and to the social interactions it facilitates. Natural Language Processing is a diverse field, and progress throughout its development has come from new representational theories, modeling techniques, data collection paradigms, and tasks. We posit that the present success of representation learning approaches trained on large text corpora can be deeply enriched from the parallel tradition of research on the contextual and social nature of language. In this article, we consider work on the contextual foundations of language: grounding, embodiment, and social interaction. We describe a brief history and possible progression of how contextual information can factor into our representations, with an eye towards how this integration can move the field forward and where it is currently being pioneered. We believe this framing will serve as a roadmap for truly contextual language understanding.},
	urldate = {2020-09-02},
	journal = {arXiv:2004.10151 [cs]},
	author = {Bisk, Yonatan and Holtzman, Ari and Thomason, Jesse and Andreas, Jacob and Bengio, Yoshua and Chai, Joyce and Lapata, Mirella and Lazaridou, Angeliki and May, Jonathan and Nisnevich, Aleksandr and Pinto, Nicolas and Turian, Joseph},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.10151},
}

@article{markram_reconstruction_2015,
	title = {Reconstruction and simulation of neocortical microcircuitry},
	volume = {163},
	issn = {0092-8674},
	url = {http://www.sciencedirect.com/science/article/pii/S0092867415011915},
	doi = {10.1016/j.cell.2015.09.029},
	abstract = {We present a first-draft digital reconstruction of the microcircuitry of somatosensory cortex of juvenile rat. The reconstruction uses cellular and synaptic organizing principles to algorithmically reconstruct detailed anatomy and physiology from sparse experimental data. An objective anatomical method defines a neocortical volume of 0.29 ± 0.01 mm3 containing ∼31,000 neurons, and patch-clamp studies identify 55 layer-specific morphological and 207 morpho-electrical neuron subtypes. When digitally reconstructed neurons are positioned in the volume and synapse formation is restricted to biological bouton densities and numbers of synapses per connection, their overlapping arbors form ∼8 million connections with ∼37 million synapses. Simulations reproduce an array of in vitro and in vivo experiments without parameter tuning. Additionally, we find a spectrum of network states with a sharp transition from synchronous to asynchronous activity, modulated by physiological mechanisms. The spectrum of network states, dynamically reconfigured around this transition, supports diverse information processing strategies.
PaperClip
Video Abstract},
	language = {en},
	number = {2},
	urldate = {2020-09-03},
	journal = {Cell},
	author = {Markram, Henry and Muller, Eilif and Ramaswamy, Srikanth and Reimann, Michael W. and Abdellah, Marwan and Sanchez, Carlos Aguado and Ailamaki, Anastasia and Alonso-Nanclares, Lidia and Antille, Nicolas and Arsever, Selim and Kahou, Guy Antoine Atenekeng and Berger, Thomas K. and Bilgili, Ahmet and Buncic, Nenad and Chalimourda, Athanassia and Chindemi, Giuseppe and Courcol, Jean-Denis and Delalondre, Fabien and Delattre, Vincent and Druckmann, Shaul and Dumusc, Raphael and Dynes, James and Eilemann, Stefan and Gal, Eyal and Gevaert, Michael Emiel and Ghobril, Jean-Pierre and Gidon, Albert and Graham, Joe W. and Gupta, Anirudh and Haenel, Valentin and Hay, Etay and Heinis, Thomas and Hernando, Juan B. and Hines, Michael and Kanari, Lida and Keller, Daniel and Kenyon, John and Khazen, Georges and Kim, Yihwa and King, James G. and Kisvarday, Zoltan and Kumbhar, Pramod and Lasserre, Sébastien and Le Bé, Jean-Vincent and Magalhães, Bruno R. C. and Merchán-Pérez, Angel and Meystre, Julie and Morrice, Benjamin Roy and Muller, Jeffrey and Muñoz-Céspedes, Alberto and Muralidhar, Shruti and Muthurasa, Keerthan and Nachbaur, Daniel and Newton, Taylor H. and Nolte, Max and Ovcharenko, Aleksandr and Palacios, Juan and Pastor, Luis and Perin, Rodrigo and Ranjan, Rajnish and Riachi, Imad and Rodríguez, José-Rodrigo and Riquelme, Juan Luis and Rössert, Christian and Sfyrakis, Konstantinos and Shi, Ying and Shillcock, Julian C. and Silberberg, Gilad and Silva, Ricardo and Tauheed, Farhan and Telefont, Martin and Toledo-Rodriguez, Maria and Tränkler, Thomas and Van Geit, Werner and Díaz, Jafet Villafranca and Walker, Richard and Wang, Yun and Zaninetta, Stefano M. and DeFelipe, Javier and Hill, Sean L. and Segev, Idan and Schürmann, Felix},
	month = oct,
	year = {2015},
	pages = {456--492},
}

@article{richardson_engagement_2020,
	title = {Engagement in video and audio narratives: contrasting self-report and physiological measures},
	volume = {10},
	copyright = {2020 The Author(s)},
	issn = {2045-2322},
	shorttitle = {Engagement in video and audio narratives},
	url = {http://www.nature.com/articles/s41598-020-68253-2},
	doi = {10.1038/s41598-020-68253-2},
	abstract = {Stories play a fundamental role in human culture. They provide a mechanism for sharing cultural identity, imparting knowledge, revealing beliefs, reinforcing social bonds and providing entertainment that is central to all human societies. Here we investigated the extent to which the delivery medium of a story (audio or visual) affected self-reported and physiologically measured engagement with the narrative. Although participants self-reported greater involvement for watching video relative to listening to auditory scenes, stronger physiological responses were recorded for auditory stories. Sensors placed at their wrists showed higher and more variable heart rates, greater electrodermal activity, and even higher body temperatures. We interpret these findings as evidence that the stories were more cognitively and emotionally engaging at a physiological level when presented in an auditory format. This may be because listening to a story, rather than watching a video, is a more active process of co-creation, and that this imaginative process in the listener’s mind is detectable on the skin at their wrist.},
	language = {en},
	number = {1},
	urldate = {2020-09-03},
	journal = {Scientific Reports},
	author = {Richardson, Daniel C. and Griffin, Nicole K. and Zaki, Lara and Stephenson, Auburn and Yan, Jiachen and Curry, Thomas and Noble, Richard and Hogan, John and Skipper, Jeremy I. and Devlin, Joseph T.},
	month = jul,
	year = {2020},
	note = {Number: 1
Publisher: Nature Publishing Group},
	pages = {11298},
}

@article{graesser_constructing_1994,
	title = {Constructing inferences during narrative text comprehension.},
	volume = {101},
	issn = {1939-1471, 0033-295X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-295X.101.3.371},
	doi = {10.1037/0033-295X.101.3.371},
	language = {en},
	number = {3},
	urldate = {2020-09-03},
	journal = {Psychological Review},
	author = {Graesser, Arthur C. and Singer, Murray and Trabasso, Tom},
	year = {1994},
	keywords = {narrative, inference},
	pages = {371--395},
}

@inproceedings{dunietz_test_2020,
	address = {Online},
	title = {To test machine comprehension, start by defining comprehension},
	url = {https://www.aclweb.org/anthology/2020.acl-main.701},
	doi = {10.18653/v1/2020.acl-main.701},
	abstract = {Many tasks aim to measure machine reading comprehension (MRC), often focusing on question types presumed to be difficult. Rarely, however, do task designers start by considering what systems should in fact comprehend. In this paper we make two key contributions. First, we argue that existing approaches do not adequately define comprehension; they are too unsystematic about what content is tested. Second, we present a detailed definition of comprehension—a “Template of Understanding”—for a widely useful class of texts, namely short narratives. We then conduct an experiment that strongly suggests existing systems are not up to the task of narrative understanding as we define it.},
	urldate = {2020-09-02},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Dunietz, Jesse and Burnham, Greg and Bharadwaj, Akash and Rambow, Owen and Chu-Carroll, Jennifer and Ferrucci, Dave},
	month = jul,
	year = {2020},
	pages = {7839--7859},
}

@article{lake_building_2017,
	title = {Building machines that learn and think like people},
	volume = {40},
	issn = {0140-525X, 1469-1825},
	url = {https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/building-machines-that-learn-and-think-like-people/A9535B1D745A0377E16C590E14B94993},
	doi = {10.1017/S0140525X16001837},
	abstract = {Abstract
Recent progress in artificial intelligence has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats that of humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn and how they learn it. Specifically, we argue that these machines should (1) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (2) ground learning in intuitive theories of physics and psychology to support and enrich the knowledge that is learned; and (3) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes toward these goals that can combine the strengths of recent neural network advances with more structured cognitive models.},
	urldate = {2017-12-14},
	journal = {Behavioral and Brain Sciences},
	author = {Lake, Brenden M. and Ullman, Tomer D. and Tenenbaum, Joshua B. and Gershman, Samuel J.},
	year = {2017},
	keywords = {AI, project.lstmMEG, read, project.lsnn, ANN, cognitive science, statistical learning, pattern recognition, Turing},
	file = {Lake idr. - 2017 - Building machines that learn and think like people.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Lake idr. - 2017 - Building machines that learn and think like people.pdf:application/pdf},
}

@article{kaestner_toward_2020,
	title = {Toward {A} database of intracranial electrophysiology during natural language presentation},
	volume = {35},
	issn = {2327-3798},
	url = {https://doi.org/10.1080/23273798.2018.1500262},
	doi = {10.1080/23273798.2018.1500262},
	abstract = {Intracranial electrophysiology (iEEG) studies using cognitive tasks contribute to the understanding of the neural basis of language. However, though iEEG is recorded continuously during clinical treatment, due to patient considerations task time is limited. To increase the usefulness of iEEG recordings for language study we provided patients with a tablet pre-loaded with media filled with natural language, wirelessly synchronised to clinical iEEG. This iEEG data collected and time-locked to natural language presentation is particularly applicable for studying the neural basis of combining words into larger contexts. We validate this approach with pilot analyses involving words heard during a movie, tagging syntactic properties and verb contextual probabilities. Event-related averages of high-frequency power (70–170 Hz) identified bilateral perisylvian electrodes with differential responses to syntactic class and a linear regression identified activity associated with contextual probabilities, demonstrating the usefulness of aligning media to iEEG. We imagine future multi-site collaborations building an “intracranial neurolinguistic corpus”.},
	number = {6},
	urldate = {2020-09-02},
	journal = {Language, Cognition and Neuroscience},
	author = {Kaestner, Erik and Morgan, Adam Milton and Snider, Joseph and Zhan, Meilin and Jiang, Xi and Levy, Roger and Ferreira, Victor S. and Thesen, Thomas and Halgren, Eric},
	month = jul,
	year = {2020},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/23273798.2018.1500262},
	keywords = {contextual probability, Intracranial electrophysiology, natural language},
	pages = {729--738},
}

@article{choi_decoding_nodate,
	title = {Decoding the {Real}-{Time} {Neurobiological} {Properties} of {Incremental} {Semantic} {Interpretation}},
	url = {http://academic.oup.com/cercor/advance-article/doi/10.1093/cercor/bhaa222/5899709},
	doi = {10.1093/cercor/bhaa222},
	abstract = {Abstract.  Communication through spoken language is a central human capacity, involving a wide range of complex computations that incrementally interpret each w},
	language = {en},
	urldate = {2020-09-02},
	journal = {Cerebral Cortex},
	author = {Choi, Hun S. and Marslen-Wilson, William D. and Lyu, Bingjiang and Randall, Billi and Tyler, Lorraine K.},
	file = {Snapshot:/Users/kriarm/Zotero/storage/GQILATI5/5899709.html:text/html},
}

@misc{humphries_why_2019,
	title = {Why model the brain?},
	url = {https://medium.com/the-spike/why-model-the-brain-c7a8e160e566},
	abstract = {An Actual Scientific Case for Brain Simulations},
	language = {en},
	urldate = {2019-08-03},
	journal = {Medium},
	author = {Humphries, Mark},
	month = aug,
	year = {2019},
	keywords = {explanation, computation, simulation neuroscience},
	file = {Snapshot:/Users/kriarm/Zotero/storage/XYDWTYNL/why-model-the-brain-c7a8e160e566.html:text/html},
}

@article{russakovsky_imagenet_2015,
	title = {{ImageNet} large scale visual recognition challenge},
	volume = {115},
	issn = {0920-5691, 1573-1405},
	url = {http://link.springer.com/10.1007/s11263-015-0816-y},
	doi = {10.1007/s11263-015-0816-y},
	language = {en},
	number = {3},
	urldate = {2019-01-07},
	journal = {International Journal of Computer Vision},
	author = {Russakovsky, Olga and Deng, Jia and Su, Hao and Krause, Jonathan and Satheesh, Sanjeev and Ma, Sean and Huang, Zhiheng and Karpathy, Andrej and Khosla, Aditya and Bernstein, Michael and Berg, Alexander C. and Fei-Fei, Li},
	month = dec,
	year = {2015},
	pages = {211--252},
}

@article{van_essen_wu-minn_2013,
	series = {Mapping the {Connectome}},
	title = {The {WU}-{Minn} {Human} {Connectome} {Project}: {An} overview},
	volume = {80},
	issn = {1053-8119},
	shorttitle = {The {WU}-{Minn} {Human} {Connectome} {Project}},
	url = {http://www.sciencedirect.com/science/article/pii/S1053811913005351},
	doi = {10.1016/j.neuroimage.2013.05.041},
	abstract = {The Human Connectome Project consortium led by Washington University, University of Minnesota, and Oxford University is undertaking a systematic effort to map macroscopic human brain circuits and their relationship to behavior in a large population of healthy adults. This overview article focuses on progress made during the first half of the 5-year project in refining the methods for data acquisition and analysis. Preliminary analyses based on a finalized set of acquisition and preprocessing protocols demonstrate the exceptionally high quality of the data from each modality. The first quarterly release of imaging and behavioral data via the ConnectomeDB database demonstrates the commitment to making HCP datasets freely accessible. Altogether, the progress to date provides grounds for optimism that the HCP datasets and associated methods and software will become increasingly valuable resources for characterizing human brain connectivity and function, their relationship to behavior, and their heritability and genetic underpinnings.},
	language = {en},
	urldate = {2020-08-29},
	journal = {NeuroImage},
	author = {Van Essen, David C. and Smith, Stephen M. and Barch, Deanna M. and Behrens, Timothy E. J. and Yacoub, Essa and Ugurbil, Kamil},
	month = oct,
	year = {2013},
	pages = {62--79},
}

@article{poldrack_computational_2019,
	title = {Computational and informatic advances for reproducible data analysis in neuroimaging},
	volume = {2},
	url = {https://doi.org/10.1146/annurev-biodatasci-072018-021237},
	doi = {10.1146/annurev-biodatasci-072018-021237},
	abstract = {The reproducibility of scientific research has become a point of critical concern. We argue that openness and transparency are critical for reproducibility, and we outline an ecosystem for open and transparent science that has emerged within the human neuroimaging community. We discuss the range of open data-sharing resources that have been developed for neuroimaging data, as well as the role of data standards (particularly the brain imaging data structure) in enabling the automated sharing, processing, and reuse of large neuroimaging data sets. We outline how the open source Python language has provided the basis for a data science platform that enables reproducible data analysis and visualization. We also discuss how new advances in software engineering, such as containerization, provide the basis for greater reproducibility in data analysis. The emergence of this new ecosystem provides an example for many areas of science that are currently struggling with reproducibility.},
	number = {1},
	urldate = {2020-06-02},
	journal = {Annual Review of Biomedical Data Science},
	author = {Poldrack, Russell A. and Gorgolewski, Krzysztof J. and Varoquaux, Gaël},
	year = {2019},
	note = {\_eprint: https://doi.org/10.1146/annurev-biodatasci-072018-021237},
	keywords = {open science, reproducibility, data sharing},
	pages = {119--138},
}

@article{poldrack_making_2014,
	title = {Making big data open: data sharing in neuroimaging},
	volume = {17},
	issn = {1097-6256, 1546-1726},
	shorttitle = {Making big data open},
	url = {http://www.nature.com/articles/nn.3818},
	doi = {10.1038/nn.3818},
	language = {en},
	number = {11},
	urldate = {2020-05-23},
	journal = {Nature Neuroscience},
	author = {Poldrack, Russell A and Gorgolewski, Krzysztof J},
	month = nov,
	year = {2014},
	keywords = {open science, data sharing},
	pages = {1510--1517},
}

@article{schrimpf_brain-score_2020,
	title = {Brain-{Score}: {Which} artificial neural network for object recognition is most brain-like?},
	copyright = {© 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	shorttitle = {Brain-{Score}},
	url = {https://www.biorxiv.org/content/10.1101/407007v2},
	doi = {10.1101/407007},
	abstract = {The internal representations of early deep artificial neural networks (ANNs) were found to be remarkably similar to the internal neural representations measured experimentally in the primate brain. Here we ask, as deep ANNs have continued to evolve, are they becoming more or less brain-like? ANNs that are most functionally similar to the brain will contain mechanisms that are most like those used by the brain. We therefore developed Brain-Score – a composite of multiple neural and behavioral benchmarks that score any ANN on how similar it is to the brain’s mechanisms for core object recognition – and we deployed it to evaluate a wide range of state-of-the-art deep ANNs. Using this scoring system, we here report that: (1) DenseNet-169, CORnet-S and ResNet-101 are the most brain-like ANNs. (2) There remains considerable variability in neural and behavioral responses that is not predicted by any ANN, suggesting that no ANN model has yet captured all the relevant mechanisms. (3) Extending prior work, we found that gains in ANN ImageNet performance led to gains on Brain-Score. However, correlation weakened at ≥ 70\% top-1 ImageNet performance, suggesting that additional guidance from neuroscience is needed to make further advances in capturing brain mechanisms. (4) We uncovered smaller (i.e. less complex) ANNs that are more brain-like than many of the best-performing ImageNet models, which suggests the opportunity to simplify ANNs to better understand the ventral stream. The scoring system used here is far from complete. However, we propose that evaluating and tracking model-benchmark correspondences through a Brain-Score that is regularly updated with new brain data is an exciting opportunity: experimental benchmarks can be used to guide machine network evolution, and machine networks are mechanistic hypotheses of the brain’s network and thus drive next experiments. To facilitate both of these, we release [Brain-Score.org][1]: a platform that hosts the neural and behavioral benchmarks, where ANNs for visual processing can be submitted to receive a Brain-Score and their rank relative to other models, and where new experimental data can be naturally incorporated.

 [1]: http://Brain-Score.org},
	language = {en},
	urldate = {2020-08-26},
	journal = {bioRxiv},
	author = {Schrimpf, Martin and Kubilius, Jonas and Hong, Ha and Majaj, Najib J. and Rajalingham, Rishi and Issa, Elias B. and Kar, Kohitij and Bashivan, Pouya and Prescott-Roy, Jonathan and Geiger, Franziska and Schmidt, Kailyn and Yamins, Daniel L. K. and DiCarlo, James J.},
	month = jan,
	year = {2020},
	note = {Publisher: Cold Spring Harbor Laboratory
Section: New Results},
	pages = {407007},
	file = {Snapshot:/Users/kriarm/Zotero/storage/KA36IG4S/407007v2.html:text/html},
}

@article{carlson_ghosts_2017,
	title = {Ghosts in machine learning for cognitive neuroscience: {Moving} from data to theory},
	issn = {1053-8119},
	shorttitle = {Ghosts in machine learning for cognitive neuroscience},
	url = {http://www.sciencedirect.com/science/article/pii/S1053811917306663},
	doi = {10.1016/j.neuroimage.2017.08.019},
	abstract = {The application of machine learning methods to neuroimaging data has fundamentally altered the field of cognitive neuroscience. Future progress in understanding brain function using these methods will require addressing a number of key methodological and interpretive challenges. Because these challenges often remain unseen and metaphorically “haunt” our efforts to use these methods to understand the brain, we refer to them as “ghosts”. In this paper, we describe three such ghosts, situate them within a more general framework from philosophy of science, and then describe steps to address them. The first ghost arises from difficulties in determining what information machine learning classifiers use for decoding. The second ghost arises from the interplay of experimental design and the structure of information in the brain – that is, our methods embody implicit assumptions about information processing in the brain, and it is often difficult to determine if those assumptions are satisfied. The third ghost emerges from our limited ability to distinguish information that is merely decodable from the brain from information that is represented and used by the brain. Each of the three ghosts place limits on the interpretability of decoding research in cognitive neuroscience. There are no easy solutions, but facing these issues squarely will provide a clearer path to understanding the nature of representation and computation in the human brain.},
	urldate = {2017-12-02},
	journal = {NeuroImage},
	author = {Carlson, Thomas and Goddard, Erin and Kaplan, David M. and Klein, Colin and Ritchie, J. Brendan},
	month = aug,
	year = {2017},
	keywords = {explanation, fixme, philosophy of neuroscience, read, epistemology, machine learning},
	file = {Carlson idr. - 2017 - Ghosts in machine learning for cognitive neuroscie.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Carlson idr. - 2017 - Ghosts in machine learning for cognitive neuroscie.pdf:application/pdf},
}

@inproceedings{athanasiou_neural_2018,
	address = {Santa Fe, New Mexico, USA},
	title = {Neural activation semantic models: {Computational} lexical semantic models of localized neural activations},
	shorttitle = {Neural {Activation} {Semantic} {Models}},
	url = {https://www.aclweb.org/anthology/C18-1243},
	abstract = {Neural activation models have been proposed in the literature that use a set of example words for which fMRI measurements are available in order to find a mapping between word semantics and localized neural activations. Successful mappings let us expand to the full lexicon of concrete nouns using the assumption that similarity of meaning implies similar neural activation patterns. In this paper, we propose a computational model that estimates semantic similarity in the neural activation space and investigates the relative performance of this model for various natural language processing tasks. Despite the simplicity of the proposed model and the very small number of example words used to bootstrap it, the neural activation semantic model performs surprisingly well compared to state-of-the-art word embeddings. Specifically, the neural activation semantic model performs better than the state-of-the-art for the task of semantic similarity estimation between very similar or very dissimilar words, while performing well on other tasks such as entailment and word categorization. These are strong indications that neural activation semantic models can not only shed some light into human cognition but also contribute to computation models for certain tasks.},
	urldate = {2020-08-25},
	booktitle = {Proceedings of the 27th {International} {Conference} on {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Athanasiou, Nikos and Iosif, Elias and Potamianos, Alexandros},
	month = aug,
	year = {2018},
	keywords = {fMRI, semantic similarity},
	pages = {2867--2878},
}

@book{puchner_written_2017,
	address = {New York},
	edition = {First edition},
	title = {The written world: the power of stories to shape people, history, civilization},
	isbn = {978-0-8129-9893-1 978-0-8129-8827-7},
	shorttitle = {The written world},
	publisher = {Random House},
	author = {Puchner, Martin},
	year = {2017},
	keywords = {narrative, history, comparative literature, literature},
}

@article{fitz_neuronal_2020,
	title = {Neuronal spike-rate adaptation supports working memory in language processing},
	copyright = {© 2020 . https://www-pnas-org.ru.idm.oclc.org/site/aboutpnas/licenses.xhtmlPublished under the PNAS license.},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/content/early/2020/08/07/2000222117},
	doi = {10.1073/pnas.2000222117},
	abstract = {Language processing involves the ability to store and integrate pieces of information in working memory over short periods of time. According to the dominant view, information is maintained through sustained, elevated neural activity. Other work has argued that short-term synaptic facilitation can serve as a substrate of memory. Here we propose an account where memory is supported by intrinsic plasticity that downregulates neuronal firing rates. Single neuron responses are dependent on experience, and we show through simulations that these adaptive changes in excitability provide memory on timescales ranging from milliseconds to seconds. On this account, spiking activity writes information into coupled dynamic variables that control adaptation and move at slower timescales than the membrane potential. From these variables, information is continuously read back into the active membrane state for processing. This neuronal memory mechanism does not rely on persistent activity, excitatory feedback, or synaptic plasticity for storage. Instead, information is maintained in adaptive conductances that reduce firing rates and can be accessed directly without cued retrieval. Memory span is systematically related to both the time constant of adaptation and baseline levels of neuronal excitability. Interference effects within memory arise when adaptation is long lasting. We demonstrate that this mechanism is sensitive to context and serial order which makes it suitable for temporal integration in sequence processing within the language domain. We also show that it enables the binding of linguistic features over time within dynamic memory registers. This work provides a step toward a computational neurobiology of language.},
	language = {en},
	urldate = {2020-08-23},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Fitz, Hartmut and Uhlmann, Marvin and Broek, Dick van den and Duarte, Renato and Hagoort, Peter and Petersson, Karl Magnus},
	month = aug,
	year = {2020},
	pmid = {32788365},
	note = {Publisher: National Academy of Sciences
Section: Biological Sciences},
	keywords = {working memory, processing memory, spike-rate adaptation, SNN},
}

@article{rybinski_clinical_2020,
	title = {Clinical trial search: {Using} biomedical language understanding models for re-ranking},
	issn = {1532-0464},
	shorttitle = {Clinical {Trial} {Search}},
	url = {http://www.sciencedirect.com/science/article/pii/S1532046420301581},
	doi = {10.1016/j.jbi.2020.103530},
	abstract = {Bidirectional Encoder Representations from Transformers (BERT) have achieved state-of-the-art effectiveness in some of the biomedical information processing applications. We investigate the effectiveness of these techniques for clinical trial search systems. In precision medicine, matching patients to relevant experimental evidence or prospective treatments is a complex task which requires both clinical and biological knowledge. To assist in this complex decision making, we investigate the effectiveness of different ranking models based on the BERT models under the same retrieval platform to ensure fair comparisons. An evaluation on the TREC Precision Medicine benchmarks indicates that our approach using the BERT model pre-trained on scientific abstracts and clinical notes achieves state-of-the-art results, on par with highly specialised, manually optimised heuristic models. We also report the best results to date on the TREC Precision Medicine 2017 ad hoc retrieval task for clinical trial search.},
	language = {en},
	urldate = {2020-08-21},
	journal = {Journal of Biomedical Informatics},
	author = {Rybinski, Maciej and Xu, Jerry and Karimi, Sarvnaz},
	month = aug,
	year = {2020},
	keywords = {transformer, clinical NLP, clinical trial},
	pages = {103530},
}

@article{merkx_comparing_2020,
	title = {Comparing transformers and {RNNs} on predicting human sentence processing data},
	url = {http://arxiv.org/abs/2005.09471},
	abstract = {Recurrent neural networks (RNNs) have long been an architecture of interest for computational models of human sentence processing. The more recently introduced Transformer architecture has been shown to outperform recurrent neural networks on many natural language processing tasks but little is known about their ability to model human language processing. It has long been thought that human sentence reading involves something akin to recurrence and so RNNs may still have an advantage over the Transformer as a cognitive model. In this paper we train both Transformer and RNN based language models and compare their performance as a model of human sentence processing. We use the trained language models to compute surprisal values for the stimuli used in several reading experiments and use mixed linear modelling to measure how well the surprisal explains measures of human reading effort. Our analysis shows that the Transformers outperform the RNNs as cognitive models in explaining self-paced reading times and N400 strength but not gaze durations from an eye-tracking experiment.},
	urldate = {2020-08-21},
	journal = {arXiv:2005.09471 [cs]},
	author = {Merkx, Danny and Frank, Stefan L.},
	month = may,
	year = {2020},
	note = {arXiv: 2005.09471},
	keywords = {LM, N400, transformer, reading comprehension, RSVP, self-paced reading},
}

@article{raffel_exploring_2020,
	title = {Exploring the limits of transfer learning with a unified text-to-text transformer},
	url = {http://arxiv.org/abs/1910.10683},
	abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new ``Colossal Clean Crawled Corpus'', we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
	urldate = {2020-08-19},
	journal = {arXiv:1910.10683 [cs, stat]},
	author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
	month = jul,
	year = {2020},
	note = {arXiv: 1910.10683},
}

@article{koelewijn_pupil_2014,
	title = {The pupil response is sensitive to divided attention during speech processing},
	volume = {312},
	issn = {0378-5955},
	url = {http://www.sciencedirect.com/science/article/pii/S0378595514000422},
	doi = {10.1016/j.heares.2014.03.010},
	abstract = {Dividing attention over two streams of speech strongly decreases performance compared to focusing on only one. How divided attention affects cognitive processing load as indexed with pupillometry during speech recognition has so far not been investigated. In 12 young adults the pupil response was recorded while they focused on either one or both of two sentences that were presented dichotically and masked by fluctuating noise across a range of signal-to-noise ratios. In line with previous studies, the performance decreases when processing two target sentences instead of one. Additionally, dividing attention to process two sentences caused larger pupil dilation and later peak pupil latency than processing only one. This suggests an effect of attention on cognitive processing load (pupil dilation) during speech processing in noise.},
	language = {en},
	urldate = {2020-08-20},
	journal = {Hearing Research},
	author = {Koelewijn, Thomas and Shinn-Cunningham, Barbara G. and Zekveld, Adriana A. and Kramer, Sophia E.},
	month = jun,
	year = {2014},
	pages = {114--120},
}

@article{white_value_1980-1,
	title = {The value of narrativity in the representation of reality},
	doi = {10.1086/448086},
	abstract = {To raise the question of the nature of narrative is to invite reflection on the very nature of culture and, possibly, even on the nature of humanity itself. So natural is the impulse to narrate, so inevitable is the form of narrative for any report of the way things really happened, that narrativity could appear problematical only in a culture in which it was absent-absent or, as in some domains of contemporary Western intellectual and artistic culture, programmatically refused. As a panglobal fact of culture, narrative and narration are less problems than simply data. As the late (and already profoundly missed) Roland Barthes remarked, narrative "is simply there like life itself. . international, transhistorical, transcultural."' Far from being a problem, then, narrative might well be considered a solution to a problem of general human concern, namely, the problem of how to translate knowing into telling,2 the problem of fashioning human experience into a form assimilable to structures of meaning that are generally human rather than culture-specific. We may not be able fully to comprehend specific thought patterns of another culture, but we have relatively less difficulty understanding a story coming from another culture, however exotic that},
	author = {White, H.},
	year = {1980},
	keywords = {narrative, narratology, relativism},
}

@article{bruner_narrative_1991,
	title = {The narrative construction of reality},
	volume = {18},
	issn = {0093-1896},
	url = {https://www.jstor.org/stable/1343711},
	number = {1},
	urldate = {2020-08-15},
	journal = {Critical Inquiry},
	author = {Bruner, Jerome},
	year = {1991},
	note = {Publisher: The University of Chicago Press},
	keywords = {narrative, empiricism, rationalism},
	pages = {1--21},
}

@misc{noauthor_field_nodate,
	title = {The field of natural language processing is chasing the wrong goal},
	url = {https://www.technologyreview.com/2020/07/31/1005876/natural-language-processing-evaluation-ai-opinion/},
	abstract = {At a typical annual meeting of the Association for Computational Linguistics (ACL), the program is a parade of titles like “A Structured Variational Autoencoder for Contextual Morphological Inflection.” The same technical flavor permeates the papers, the research talks, and many hallway chats. At this year’s conference in July, though, something felt different—and it wasn’t just…},
	language = {en},
	urldate = {2020-08-15},
	journal = {MIT Technology Review},
}

@article{alishahi_analyzing_2019,
	title = {Analyzing and interpreting neural networks for {NLP}: {A} report on the first {BlackboxNLP} workshop},
	volume = {25},
	issn = {1351-3249, 1469-8110},
	shorttitle = {Analyzing and interpreting neural networks for {NLP}},
	url = {https://www.cambridge.org/core/product/identifier/S135132491900024X/type/journal_article},
	doi = {10.1017/S135132491900024X},
	language = {en},
	number = {4},
	urldate = {2020-08-14},
	journal = {Natural Language Engineering},
	author = {Alishahi, Afra and Chrupała, Grzegorz and Linzen, Tal},
	month = jul,
	year = {2019},
	keywords = {NLP, interpretability},
	pages = {543--557},
}

@article{linzen_how_2020,
	title = {How can we accelerate progress towards human-like linguistic generalization?},
	url = {http://arxiv.org/abs/2005.00955},
	abstract = {This position paper describes and critiques the Pretraining-Agnostic Identically Distributed (PAID) evaluation paradigm, which has become a central tool for measuring progress in natural language understanding. This paradigm consists of three stages: (1) pre-training of a word prediction model on a corpus of arbitrary size; (2) fine-tuning (transfer learning) on a training set representing a classification task; (3) evaluation on a test set drawn from the same distribution as that training set. This paradigm favors simple, low-bias architectures, which, first, can be scaled to process vast amounts of data, and second, can capture the fine-grained statistical properties of a particular data set, regardless of whether those properties are likely to generalize to examples of the task outside the data set. This contrasts with humans, who learn language from several orders of magnitude less data than the systems favored by this evaluation paradigm, and generalize to new tasks in a consistent way. We advocate for supplementing or replacing PAID with paradigms that reward architectures that generalize as quickly and robustly as humans.},
	urldate = {2020-08-10},
	journal = {arXiv:2005.00955 [cs]},
	author = {Linzen, Tal},
	month = may,
	year = {2020},
	note = {arXiv: 2005.00955},
	keywords = {NLP, BERT, computational linguistics, ACL, generalization, pretraining},
	file = {arXiv.org Snapshot:/Users/kriarm/Zotero/storage/G2JGNTY3/2005.html:text/html},
}

@article{diliberto_low-frequency_2015,
	title = {Low-frequency cortical entrainment to speech reflects phoneme-level processing},
	volume = {25},
	issn = {09609822},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0960982215010015},
	doi = {10.1016/j.cub.2015.08.030},
	language = {en},
	number = {19},
	urldate = {2020-08-14},
	journal = {Current Biology},
	author = {Di Liberto, Giovanni M. and O’Sullivan, James A. and Lalor, Edmund C.},
	month = oct,
	year = {2015},
	pages = {2457--2465},
}

@incollection{mccloskey_catastrophic_1989,
	title = {Catastrophic interference in connectionist networks: {The} sequential learning problem},
	volume = {24},
	shorttitle = {Catastrophic {Interference} in {Connectionist} {Networks}},
	url = {http://www.sciencedirect.com/science/article/pii/S0079742108605368},
	abstract = {Connectionist networks in which information is stored in weights on connections among simple processing units have attracted considerable interest in cognitive science. Much of the interest centers around two characteristics of these networks. First, the weights on connections between units need not be prewired by the model builder but rather may be established through training in which items to be learned are presented repeatedly to the network and the connection weights are adjusted in small increments according to a learning algorithm. Second, the networks may represent information in a distributed fashion. This chapter discusses the catastrophic interference in connectionist networks. Distributed representations established through the application of learning algorithms have several properties that are claimed to be desirable from the standpoint of modeling human cognition. These properties include content-addressable memory and so-called automatic generalization in which a network trained on a set of items responds correctly to other untrained items within the same domain. New learning may interfere catastrophically with old learning when networks are trained sequentially. The analysis of the causes of interference implies that at least some interference will occur whenever new learning may alter weights involved in representing old learning, and the simulation results demonstrate only that interference is catastrophic in some specific networks.},
	language = {en},
	urldate = {2020-08-12},
	booktitle = {Psychology of {Learning} and {Motivation}},
	publisher = {Academic Press},
	author = {McCloskey, Michael and Cohen, Neal J.},
	editor = {Bower, Gordon H.},
	month = jan,
	year = {1989},
	doi = {10.1016/S0079-7421(08)60536-8},
	keywords = {connectionism, ANN, catastrophic forgetting},
	pages = {109--165},
	file = {ScienceDirect Full Text PDF:/Users/kriarm/Zotero/storage/J3U6J3TH/McCloskey and Cohen - 1989 - Catastrophic Interference in Connectionist Network.pdf:application/pdf},
}

@article{yogatama_learning_2019,
	title = {Learning and evaluating general linguistic intelligence},
	url = {http://arxiv.org/abs/1901.11373},
	abstract = {We define general linguistic intelligence as the ability to reuse previously acquired knowledge about a language's lexicon, syntax, semantics, and pragmatic conventions to adapt to new tasks quickly. Using this definition, we analyze state-of-the-art natural language understanding models and conduct an extensive empirical investigation to evaluate them against these criteria through a series of experiments that assess the task-independence of the knowledge being acquired by the learning process. In addition to task performance, we propose a new evaluation metric based on an online encoding of the test data that quantifies how quickly an existing agent (model) learns a new task. Our results show that while the field has made impressive progress in terms of model architectures that generalize to many tasks, these models still require a lot of in-domain training examples (e.g., for fine tuning, training task-specific modules), and are prone to catastrophic forgetting. Moreover, we find that far from solving general tasks (e.g., document question answering), our models are overfitting to the quirks of particular datasets (e.g., SQuAD). We discuss missing components and conjecture on how to make progress toward general linguistic intelligence.},
	urldate = {2020-08-10},
	journal = {arXiv:1901.11373 [cs, stat]},
	author = {Yogatama, Dani and d'Autume, Cyprien de Masson and Connor, Jerome and Kocisky, Tomas and Chrzanowski, Mike and Kong, Lingpeng and Lazaridou, Angeliki and Ling, Wang and Yu, Lei and Dyer, Chris and Blunsom, Phil},
	month = jan,
	year = {2019},
	note = {arXiv: 1901.11373},
	file = {arXiv.org Snapshot:/Users/kriarm/Zotero/storage/BVCWZ95D/1901.html:text/html},
}

@article{poirazi_illuminating_2020,
	title = {Illuminating dendritic function with computational models},
	volume = {21},
	copyright = {2020 Springer Nature Limited},
	issn = {1471-0048},
	url = {https://www.nature.com/articles/s41583-020-0301-7},
	doi = {10.1038/s41583-020-0301-7},
	abstract = {Dendrites have always fascinated researchers: from the artistic drawings by Ramon y Cajal to the beautiful recordings of today, neuroscientists have been striving to unravel the mysteries of these structures. Theoretical work in the 1960s predicted important dendritic effects on neuronal processing, establishing computational modelling as a powerful technique for their investigation. Since then, modelling of dendrites has been instrumental in driving neuroscience research in a targeted manner, providing experimentally testable predictions that range from the subcellular level to the systems level, and their relevance extends to fields beyond neuroscience, such as machine learning and artificial intelligence. Validation of modelling predictions often requires — and drives — new technological advances, thus closing the loop with theory-driven experimentation that moves the field forward. This Review features the most important, to our understanding, contributions of modelling of dendritic computations, including those pending experimental verification, and highlights studies of successful interactions between the modelling and experimental neuroscience communities.},
	language = {en},
	number = {6},
	urldate = {2020-08-10},
	journal = {Nature Reviews Neuroscience},
	author = {Poirazi, Panayiota and Papoutsi, Athanasia},
	month = jun,
	year = {2020},
	note = {Number: 6
Publisher: Nature Publishing Group},
	keywords = {computational neuroscience, dendrites, dendritic computation},
	pages = {303--321},
	file = {Snapshot:/Users/kriarm/Zotero/storage/GYM6AJFZ/s41583-020-0301-7.html:text/html},
}

@book{buonomano_your_2018,
	title = {Your brain is a time machine: the neuroscience and physics of time},
	isbn = {978-0-393-35560-4},
	shorttitle = {Your brain is a time machine},
	abstract = {A leading neuroscientist presents a groundbreaking exploration of how time works inside the mind, arguing that the human brain's complex system constructs our sense of chronological flow in ways that are essential to evolution and everyday survival.},
	language = {English},
	author = {Buonomano, Dean},
	year = {2018},
	note = {OCLC: 1178807576},
	keywords = {time, perception},
}

@inproceedings{pennington_glove_2014,
	address = {Doha, Qatar},
	title = {Glove: {Global} vectors for word representation},
	shorttitle = {Glove},
	url = {http://aclweb.org/anthology/D14-1162},
	doi = {10.3115/v1/D14-1162},
	language = {en},
	urldate = {2020-08-08},
	booktitle = {Proceedings of the 2014 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} ({EMNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Pennington, Jeffrey and Socher, Richard and Manning, Christopher},
	year = {2014},
	pages = {1532--1543},
}

@article{kahn_breakthroughs_2020,
	title = {A.{I}. breakthroughs in natural-language processing are big for business},
	url = {https://fortune.com/2020/01/20/natural-language-processing-business/},
	abstract = {A series of breakthroughs in a branch of A.I. called natural language processing is sparking the rapid development of revolutionary new products.},
	language = {en},
	urldate = {2020-08-07},
	journal = {Fortune},
	author = {Kahn, Jeremy},
	month = jan,
	year = {2020},
	file = {Snapshot:/Users/kriarm/Zotero/storage/PV3CCBAN/natural-language-processing-business.html:text/html},
}

@misc{noauthor_introduction_nodate,
	title = {Introduction},
	url = {http://naturalistic-data.org/intro#How-can-we-study-the-dynamics-of-brain-activity?},
	urldate = {2020-08-07},
	file = {Introduction:/Users/kriarm/Zotero/storage/TQ8EXSUS/intro.html:text/html},
}

@article{noauthor_mechanisms_2017,
	title = {The mechanisms and functions of synaptic facilitation},
	volume = {94},
	issn = {0896-6273},
	url = {http://www.sciencedirect.com/science/article/pii/S0896627317301551},
	doi = {10.1016/j.neuron.2017.02.047},
	abstract = {The ability of the brain to store and process information relies on changing the strength of connections between neurons. Synaptic facilitation is a f…},
	language = {en},
	number = {3},
	urldate = {2020-08-04},
	journal = {Neuron},
	month = may,
	year = {2017},
	note = {Publisher: Cell Press},
	pages = {447--464},
}

@article{goldman-rakic_cellular_1995,
	title = {Cellular basis of working memory},
	volume = {14},
	issn = {08966273},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0896627395903046},
	doi = {10.1016/0896-6273(95)90304-6},
	language = {en},
	number = {3},
	urldate = {2020-08-04},
	journal = {Neuron},
	author = {Goldman-Rakic, P.S},
	month = mar,
	year = {1995},
	keywords = {working memory, PFC, persistent activity},
	pages = {477--485},
}

@article{federmeier_thinking_2007,
	title = {Thinking ahead: {The} role and roots of prediction in language comprehension},
	volume = {44},
	issn = {1469-8986},
	shorttitle = {Thinking ahead},
	url = {http://onlinelibrary.wiley.com/doi/abs/10.1111/j.1469-8986.2007.00531.x},
	doi = {10.1111/j.1469-8986.2007.00531.x},
	abstract = {Reviewed are studies using event-related potentials to examine when and how sentence context information is used during language comprehension. Results suggest that, when it can, the brain uses context to predict features of likely upcoming items. However, although prediction seems important for comprehension, it also appears susceptible to age-related deterioration and can be associated with processing costs. The brain may address this trade-off by employing multiple processing strategies, distributed across the two cerebral hemispheres. In particular, left hemisphere language processing seems to be oriented toward prediction and the use of top-down cues, whereas right hemisphere comprehension is more bottom-up, biased toward the veridical maintenance of information. Such asymmetries may arise, in turn, because language comprehension mechanisms are integrated with language production mechanisms only in the left hemisphere (the PARLO framework).},
	language = {en},
	number = {4},
	urldate = {2020-05-24},
	journal = {Psychophysiology},
	author = {Federmeier, Kara D.},
	year = {2007},
	note = {\_eprint: https://onlinelibrary.wiley.com/doi/pdf/10.1111/j.1469-8986.2007.00531.x},
	keywords = {N400, sentence comprehension},
	pages = {491--505},
}

@article{zajzon_passing_2019,
	title = {Passing the message: {Representation} transfer in modular balanced networks},
	volume = {13},
	issn = {1662-5188},
	shorttitle = {Passing the {Message}},
	url = {https://www.frontiersin.org/articles/10.3389/fncom.2019.00079/full#B35},
	doi = {10.3389/fncom.2019.00079},
	abstract = {Neurobiological systems rely on hierarchical and modular architectures to carry out intricate computations using minimal resources. A prerequisite for such systems to operate adequately is the capability to reliably and efficiently transfer information across multiple modules. Here, we study the features enabling a robust transfer of stimulus representations in modular networks of spiking neurons, tuned to operate in a balanced regime. To capitalize on the complex, transient dynamics that such networks exhibit during active processing, we apply reservoir computing principles and probe the systems' computational efficacy with specific tasks. Focusing on the comparison of random feed-forward connectivity and biologically inspired topographic maps, we find that, in a sequential set-up, structured projections between the modules are strictly necessary for information to propagate accurately to deeper modules. Such mappings not only improve computational performance and efficiency, they also reduce response variability, increase robustness against interference effects, and boost memory capacity. We further investigate how information from two separate input streams is integrated and demonstrate that it is more advantageous to perform non-linear computations on the input locally, within a given module, and subsequently transfer the result downstream, rather than transferring intermediate information and performing the computation downstream. Depending on how information is integrated early on in the system, the networks achieve similar task-performance using different strategies, indicating that the dimensionality of the neural responses does not necessarily correlate with nonlinear integration, as predicted by previous studies. These findings highlight a key role of topographic maps in supporting fast, robust and accurate neural communication over longer distances. Given the prevalence of such structural feature, particularly in the sensory systems, elucidating their functional purpose remains an important challenge towards which this work provides relevant, new insights. At the same time, these results shed new light on important requirements for designing functional hierarchical spiking networks.},
	language = {English},
	urldate = {2020-08-04},
	journal = {Frontiers in Computational Neuroscience},
	author = {Zajzon, Barna and Mahmoudian, Sepehr and Morrison, Abigail and Duarte, Renato},
	year = {2019},
	note = {Publisher: Frontiers},
	keywords = {spiking neural networks, SNN, reservoir computing},
}

@article{durstewitz_neurocomputational_2000,
	title = {Neurocomputational models of working memory},
	volume = {3},
	copyright = {2000 Nature America Inc.},
	issn = {1546-1726},
	url = {http://www.nature.com/articles/nn1100_1184},
	doi = {10.1038/81460},
	abstract = {During working memory tasks, the firing rates of single neurons recorded in behaving monkeys remain elevated without external cues. Modeling studies have explored different mechanisms that could underlie this selective persistent activity, including recurrent excitation within cell assemblies, synfire chains and single-cell bistability. The models show how sustained activity can be stable in the presence of noise and distractors, how different synaptic and voltage-gated conductances contribute to persistent activity, how neuromodulation could influence its robustness, how completely novel items could be maintained, and how continuous attractor states might be achieved. More work is needed to address the full repertoire of neural dynamics observed during working memory tasks.},
	language = {en},
	number = {11},
	urldate = {2020-08-04},
	journal = {Nature Neuroscience},
	author = {Durstewitz, Daniel and Seamans, Jeremy K. and Sejnowski, Terrence J.},
	month = nov,
	year = {2000},
	note = {Number: 11
Publisher: Nature Publishing Group},
	pages = {1184--1191},
}

@article{zucker_short-term_2002,
	title = {Short-term synaptic plasticity},
	volume = {64},
	url = {https://doi.org/10.1146/annurev.physiol.64.092501.114547},
	doi = {10.1146/annurev.physiol.64.092501.114547},
	abstract = {Synaptic transmission is a dynamic process. Postsynaptic responses wax and wane as presynaptic activity evolves. This prominent characteristic of chemical synaptic transmission is a crucial determinant of the response properties of synapses and, in turn, of the stimulus properties selected by neural networks and of the patterns of activity generated by those networks. This review focuses on synaptic changes that result from prior activity in the synapse under study, and is restricted to short-term effects that last for at most a few minutes. Forms of synaptic enhancement, such as facilitation, augmentation, and post-tetanic potentiation, are usually attributed to effects of a residual elevation in presynaptic [Ca2+]i, acting on one or more molecular targets that appear to be distinct from the secretory trigger responsible for fast exocytosis and phasic release of transmitter to single action potentials. We discuss the evidence for this hypothesis, and the origins of the different kinetic phases of synaptic enhancement, as well as the interpretation of statistical changes in transmitter release and roles played by other factors such as alterations in presynaptic Ca2+ influx or postsynaptic levels of [Ca2+]i. Synaptic depression dominates enhancement at many synapses. Depression is usually attributed to depletion of some pool of readily releasable vesicles, and various forms of the depletion model are discussed. Depression can also arise from feedback activation of presynaptic receptors and from postsynaptic processes such as receptor desensitization. In addition, glial-neuronal interactions can contribute to short-term synaptic plasticity. Finally, we summarize the recent literature on putative molecular players in synaptic plasticity and the effects of genetic manipulations and other modulatory influences.},
	number = {1},
	urldate = {2020-08-04},
	journal = {Annual Review of Physiology},
	author = {Zucker, Robert S. and Regehr, Wade G.},
	year = {2002},
	pmid = {11826273},
	note = {\_eprint: https://doi.org/10.1146/annurev.physiol.64.092501.114547},
	pages = {355--405},
}

@article{feldman_spike-timing_2012,
	title = {The spike-timing dependence of plasticity},
	volume = {75},
	issn = {08966273},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627312007039},
	doi = {10.1016/j.neuron.2012.08.001},
	language = {en},
	number = {4},
	urldate = {2019-07-04},
	journal = {Neuron},
	author = {Feldman, Daniel E.},
	month = aug,
	year = {2012},
	keywords = {readme, STDP, Hebbian learning},
	pages = {556--571},
}

@article{magee_synaptic_2020,
	title = {Synaptic plasticity forms and functions},
	volume = {43},
	issn = {0147-006X},
	url = {https://www.annualreviews.org/doi/10.1146/annurev-neuro-090919-022842},
	doi = {10.1146/annurev-neuro-090919-022842},
	abstract = {Synaptic plasticity, the activity-dependent change in neuronal connection strength, has long been considered an important component of learning and memory. Computational and engineering work corroborate the power of learning through the directed adjustment of connection weights. Here we review the fundamental elements of four broadly categorized forms of synaptic plasticity and discuss their functional capabilities and limitations. Although standard, correlation-based, Hebbian synaptic plasticity has been the primary focus of neuroscientists for decades, it is inherently limited. Three-factor plasticity rules supplement Hebbian forms with neuromodulation and eligibility traces, while true supervised types go even further by adding objectives and instructive signals. Finally, a recently discovered hippocampal form of synaptic plasticity combines the above elements, while leaving behind the primary Hebbian requirement. We suggest that the effort to determine the neural basis of adaptive behavior could benefit from renewed experimental and theoretical investigation of more powerful directed types of synaptic plasticity.},
	number = {1},
	urldate = {2020-08-04},
	journal = {Annual Review of Neuroscience},
	author = {Magee, Jeffrey C. and Grienberger, Christine},
	month = jul,
	year = {2020},
	note = {Publisher: Annual Reviews},
	keywords = {synaptic plasticity},
	pages = {95--117},
	file = {Snapshot:/Users/kriarm/Zotero/storage/2WTVLXYD/annurev-neuro-090919-022842.html:text/html},
}

@article{citri_synaptic_2008,
	title = {Synaptic plasticity: multiple forms, functions, and mechanisms},
	volume = {33},
	copyright = {2008 American College of Neuropsychopharmacology},
	issn = {1740-634X},
	shorttitle = {Synaptic {Plasticity}},
	url = {http://www.nature.com/articles/1301559},
	doi = {10.1038/sj.npp.1301559},
	abstract = {Experiences, whether they be learning in a classroom, a stressful event, or ingestion of a psychoactive substance, impact the brain by modifying the activity and organization of specific neural circuitry. A major mechanism by which the neural activity generated by an experience modifies brain function is via modifications of synaptic transmission; that is, synaptic plasticity. Here, we review current understanding of the mechanisms of the major forms of synaptic plasticity at excitatory synapses in the mammalian brain. We also provide examples of the possible developmental and behavioral functions of synaptic plasticity and how maladaptive synaptic plasticity may contribute to neuropsychiatric disorders.},
	language = {en},
	number = {1},
	urldate = {2020-08-04},
	journal = {Neuropsychopharmacology},
	author = {Citri, Ami and Malenka, Robert C.},
	month = jan,
	year = {2008},
	note = {Number: 1
Publisher: Nature Publishing Group},
	pages = {18--41},
}

@article{bellec_solution_2020,
	title = {A solution to the learning dilemma for recurrent networks of spiking neurons},
	volume = {11},
	copyright = {2020 The Author(s)},
	issn = {2041-1723},
	url = {http://www.nature.com/articles/s41467-020-17236-y},
	doi = {10.1038/s41467-020-17236-y},
	abstract = {Recurrently connected networks of spiking neurons underlie the astounding information processing capabilities of the brain. Yet in spite of extensive research, how they can learn through synaptic plasticity to carry out complex network computations remains unclear. We argue that two pieces of this puzzle were provided by experimental data from neuroscience. A mathematical result tells us how these pieces need to be combined to enable biologically plausible online network learning through gradient descent, in particular deep reinforcement learning. This learning method–called e-prop–approaches the performance of backpropagation through time (BPTT), the best-known method for training recurrent neural networks in machine learning. In addition, it suggests a method for powerful on-chip learning in energy-efficient spike-based hardware for artificial intelligence.},
	language = {en},
	number = {1},
	urldate = {2020-08-03},
	journal = {Nature Communications},
	author = {Bellec, Guillaume and Scherr, Franz and Subramoney, Anand and Hajek, Elias and Salaj, Darjan and Legenstein, Robert and Maass, Wolfgang},
	month = jul,
	year = {2020},
	note = {Number: 1
Publisher: Nature Publishing Group},
	pages = {3625},
}

@article{salaj_spike-frequency_2020,
	title = {Spike-frequency adaptation provides a long short-term memory to networks of spiking neurons},
	copyright = {© 2020, Posted by Cold Spring Harbor Laboratory. The copyright holder for this pre-print is the author. All rights reserved. The material may not be redistributed, re-used or adapted without the author's permission.},
	url = {https://www.biorxiv.org/content/10.1101/2020.05.11.081513v1},
	doi = {10.1101/2020.05.11.081513},
	abstract = {{\textless}h3{\textgreater}Abstract{\textless}/h3{\textgreater} {\textless}p{\textgreater}Brains are able to integrate memory from the recent past into their current computations, seemingly without effort. This ability is critical for cognitive tasks such as speech understanding or working with sequences of symbols according to dynamically changing rules. But it has remained unknown how networks of spiking neurons in the brain can achieve that. We show that the presence of neurons with spike frequency adaptation makes a significant difference: Their inclusion in a network moves its performance for such computing tasks from a very low level close to the level of human performance. While artificial neural networks with special long short-term memory (LSTM) units had already reached such high performance levels, they lack biological plausibility. We find that neurons with spike-frequency adaptation, which occur especially frequently in higher cortical areas of the human brain, provide to brains a functional equivalent to LSTM units.{\textless}/p{\textgreater}},
	language = {en},
	urldate = {2020-08-01},
	journal = {bioRxiv},
	author = {Salaj, Darjan and Subramoney, Anand and Kraišniković, Ceca and Bellec, Guillaume and Legenstein, Robert and Maass, Wolfgang},
	month = may,
	year = {2020},
	note = {Publisher: Cold Spring Harbor Laboratory
Section: New Results},
	keywords = {LSTM, LSNN, e-prop, SFA},
	pages = {2020.05.11.081513},
}

@article{krakauer_neuroscience_2017,
	title = {Neuroscience needs behavior: {Correcting} a reductionist bias},
	volume = {93},
	issn = {08966273},
	shorttitle = {Neuroscience {Needs} {Behavior}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627316310406},
	doi = {10.1016/j.neuron.2016.12.041},
	language = {en},
	number = {3},
	urldate = {2020-07-30},
	journal = {Neuron},
	author = {Krakauer, John W. and Ghazanfar, Asif A. and Gomez-Marin, Alex and MacIver, Malcolm A. and Poeppel, David},
	month = feb,
	year = {2017},
	pages = {480--490},
}

@article{pulvermuller_thinking_2014,
	title = {Thinking in circuits: toward neurobiological explanation in cognitive neuroscience},
	volume = {108},
	issn = {0340-1200, 1432-0770},
	shorttitle = {Thinking in circuits},
	url = {http://link.springer.com/10.1007/s00422-014-0603-9},
	doi = {10.1007/s00422-014-0603-9},
	language = {en},
	number = {5},
	urldate = {2020-07-30},
	journal = {Biological Cybernetics},
	author = {Pulvermüller, Friedemann and Garagnani, Max and Wennekers, Thomas},
	month = oct,
	year = {2014},
	pages = {573--593},
}

@book{craver_explaining_2009,
	address = {Oxford},
	edition = {1. publ. in paperback},
	title = {Explaining the brain: mechanisms and the mosaic unity of neuroscience},
	isbn = {978-0-19-956822-2 978-0-19-929931-7},
	shorttitle = {Explaining the brain},
	language = {eng},
	publisher = {Oxford Univ. Press},
	author = {Craver, Carl F.},
	year = {2009},
	note = {OCLC: 729671866},
	keywords = {explanation, computation, mechanism},
}

@article{yarkoni_neural_2008,
	title = {Neural substrates of narrative comprehension and memory},
	volume = {41},
	issn = {10538119},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1053811908003017},
	doi = {10.1016/j.neuroimage.2008.03.062},
	language = {en},
	number = {4},
	urldate = {2020-07-30},
	journal = {NeuroImage},
	author = {Yarkoni, Tal and Speer, Nicole K. and Zacks, Jeffrey M.},
	month = jul,
	year = {2008},
	pages = {1408--1425},
}

@article{zador_critique_2019,
	title = {A critique of pure learning and what artificial neural networks can learn from animal brains},
	volume = {10},
	copyright = {2019 The Author(s)},
	issn = {2041-1723},
	url = {http://www.nature.com/articles/s41467-019-11786-6},
	doi = {10.1038/s41467-019-11786-6},
	abstract = {Artificial neural networks (ANNs) have undergone a revolution, catalyzed by better supervised learning algorithms. However, in stark contrast to young animals (including humans), training such networks requires enormous numbers of labeled examples, leading to the belief that animals must rely instead mainly on unsupervised learning. Here we argue that most animal behavior is not the result of clever learning algorithms—supervised or unsupervised—but is encoded in the genome. Specifically, animals are born with highly structured brain connectivity, which enables them to learn very rapidly. Because the wiring diagram is far too complex to be specified explicitly in the genome, it must be compressed through a “genomic bottleneck”. The genomic bottleneck suggests a path toward ANNs capable of rapid learning.},
	language = {en},
	number = {1},
	urldate = {2020-07-30},
	journal = {Nature Communications},
	author = {Zador, Anthony M.},
	month = aug,
	year = {2019},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {deep learning, ANN},
	pages = {3770},
}

@article{london_dendritic_2005,
	title = {Dendritic computation},
	volume = {28},
	issn = {0147-006X, 1545-4126},
	url = {http://www.annualreviews.org/doi/10.1146/annurev.neuro.28.061604.135703},
	doi = {10.1146/annurev.neuro.28.061604.135703},
	language = {en},
	number = {1},
	urldate = {2020-07-29},
	journal = {Annual Review of Neuroscience},
	author = {London, Michael and Häusser, Michael},
	month = jul,
	year = {2005},
	keywords = {dendrites, dendritic computation},
	pages = {503--532},
}

@article{gaier_weight_2019,
	title = {Weight agnostic neural networks},
	url = {http://arxiv.org/abs/1906.04358},
	abstract = {Not all neural network architectures are created equal, some perform much better than others for certain tasks. But how important are the weight parameters of a neural network compared to its architecture? In this work, we question to what extent neural network architectures alone, without learning any weight parameters, can encode solutions for a given task. We propose a search method for neural network architectures that can already perform a task without any explicit weight training. To evaluate these networks, we populate the connections with a single shared weight parameter sampled from a uniform random distribution, and measure the expected performance. We demonstrate that our method can find minimal neural network architectures that can perform several reinforcement learning tasks without weight training. On a supervised learning domain, we find network architectures that achieve much higher than chance accuracy on MNIST using random weights. Interactive version of this paper at https://weightagnostic.github.io/},
	urldate = {2020-07-28},
	journal = {arXiv:1906.04358 [cs, stat]},
	author = {Gaier, Adam and Ha, David},
	month = sep,
	year = {2019},
	note = {arXiv: 1906.04358},
	keywords = {gradient descent, CNN, MNIST},
}

@inproceedings{wang_glue_2018,
	title = {{GLUE}: {A} multi-task benchmark and analysis platform for natural language understanding},
	shorttitle = {{GLUE}},
	url = {https://openreview.net/forum?id=rJ4km2R5t7},
	abstract = {For natural language understanding (NLU) technology to be maximally useful, it must be able to process language in a way that is not exclusive to a single task, genre, or dataset. In pursuit of...},
	urldate = {2020-07-27},
	author = {Wang, Alex and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
	month = sep,
	year = {2018},
	keywords = {benchmark, GLUE, NLP},
}

@article{caruana_multitask_1997,
	title = {Multitask learning},
	volume = {28},
	issn = {1573-0565},
	url = {https://doi.org/10.1023/A:1007379606734},
	doi = {10.1023/A:1007379606734},
	abstract = {Multitask Learning is an approach to inductive transfer that improves generalization by using the domain information contained in the training signals of related tasks as an inductive bias. It does this by learning tasks in parallel while using a shared representation; what is learned for each task can help other tasks be learned better. This paper reviews prior work on MTL, presents new evidence that MTL in backprop nets discovers task relatedness without the need of supervisory signals, and presents new results for MTL with k-nearest neighbor and kernel regression. In this paper we demonstrate multitask learning in three domains. We explain how multitask learning works, and show that there are many opportunities for multitask learning in real domains. We present an algorithm and results for multitask learning with case-based methods like k-nearest neighbor and kernel regression, and sketch an algorithm for multitask learning in decision trees. Because multitask learning works, can be applied to many different kinds of domains, and can be used with different learning algorithms, we conjecture there will be many opportunities for its use on real-world problems.},
	language = {en},
	number = {1},
	urldate = {2020-07-27},
	journal = {Machine Learning},
	author = {Caruana, Rich},
	month = jul,
	year = {1997},
	keywords = {ML, generalization, multi-task learning},
	pages = {41--75},
}

@article{charles_toward_2020,
	title = {Toward community-driven big open brain science: {Open} big data and tools for structure, function, and genetics},
	volume = {43},
	shorttitle = {Toward {Community}-{Driven} {Big} {Open} {Brain} {Science}},
	url = {https://doi.org/10.1146/annurev-neuro-100119-110036},
	doi = {10.1146/annurev-neuro-100119-110036},
	abstract = {As acquiring bigger data becomes easier in experimental brain science, computational and statistical brain science must achieve similar advances to fully capitalize on these data. Tackling these problems will benefit from a more explicit and concerted effort to work together. Specifically, brain science can be further democratized by harnessing the power of community-driven tools, which both are built by and benefit from many different people with different backgrounds and expertise. This perspective can be applied across modalities and scales and enables collaborations across previously siloed communities. Expected final online publication date for the Annual Review of Neuroscience, Volume 43 is July 8, 2020. Please see http://www.annualreviews.org/page/journal/pubdates for revised estimates.},
	number = {1},
	urldate = {2020-07-02},
	journal = {Annual Review of Neuroscience},
	author = {Charles, Adam S. and Falk, Benjamin and Turner, Nicholas and Pereira, Talmo D. and Tward, Daniel and Pedigo, Benjamin D. and Chung, Jaewon and Burns, Randal and Ghosh, Satrajit S. and Kebschull, Justus M. and Silversmith, William and Vogelstein, Joshua T.},
	year = {2020},
	pmid = {32283996},
	note = {\_eprint: https://doi.org/10.1146/annurev-neuro-100119-110036},
	pages = {null},
}

@article{manning_emergent_2020,
	title = {Emergent linguistic structure in artificial neural networks trained by self-supervision},
	copyright = {© 2020 . https://www-pnas-org.ru.idm.oclc.org/site/aboutpnas/licenses.xhtmlPublished under the PNAS license.},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/content/early/2020/06/02/1907367117},
	doi = {10.1073/pnas.1907367117},
	abstract = {This paper explores the knowledge of linguistic structure learned by large artificial neural networks, trained via self-supervision, whereby the model simply tries to predict a masked word in a given context. Human language communication is via sequences of words, but language understanding requires constructing rich hierarchical structures that are never observed explicitly. The mechanisms for this have been a prime mystery of human language acquisition, while engineering work has mainly proceeded by supervised learning on treebanks of sentences hand labeled for this latent structure. However, we demonstrate that modern deep contextual language models learn major aspects of this structure, without any explicit supervision. We develop methods for identifying linguistic hierarchical structure emergent in artificial neural networks and demonstrate that components in these models focus on syntactic grammatical relationships and anaphoric coreference. Indeed, we show that a linear transformation of learned embeddings in these models captures parse tree distances to a surprising degree, allowing approximate reconstruction of the sentence tree structures normally assumed by linguists. These results help explain why these models have brought such large improvements across many language-understanding tasks.},
	language = {en},
	urldate = {2020-07-24},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Manning, Christopher D. and Clark, Kevin and Hewitt, John and Khandelwal, Urvashi and Levy, Omer},
	month = jun,
	year = {2020},
	pmid = {32493748},
	note = {Publisher: National Academy of Sciences
Section: Physical Sciences},
	keywords = {syntax, RNN, BERT, transformer},
}

@article{berezutskaya_modeling_2017,
	title = {Modeling brain responses to perceived speech with {LSTM} networks},
	url = {https://repository.ubn.ru.nl/handle/2066/180448},
	abstract = {We used recurrent neural networks with longshort term memory units (LSTM) to model the brain responses to speech based on the speech audio features. We compared the performance of the LSTM models to the performance of the linear ridge regression model and found the LSTM models to be more robust for predicting brain responses across different feature sets.},
	language = {English (eng)},
	urldate = {2020-07-24},
	journal = {https://pure.tue.nl/ws/files/72619856/benelearn\_2017.pdf\#page=150},
	author = {Berezutskaya, Y. and Freudenburg, Z. V. and Ramsey, N. F. and Güçlü, U. and Gerven, M. A. J. van},
	year = {2017},
	note = {Accepted: 2017-12-18T21:43:09Z
Publisher: [S.l. : s.n.]},
	keywords = {encoding models, ECoG},
}

@techreport{schrimpf_artificial_2020,
	type = {preprint},
	title = {Artificial neural networks accurately predict language processing in the brain},
	url = {http://biorxiv.org/lookup/doi/10.1101/2020.06.26.174482},
	abstract = {The ability to share ideas through language is our species’ signature cognitive skill, but how this feat is achieved by the brain remains unknown. Inspired by the success of artificial neural networks (ANNs) in explaining neural responses in perceptual tasks (Kell et al., 2018; Khaligh-Razavi \& Kriegeskorte, 2014; Schrimpf et al., 2018; Yamins et al., 2014; Zhuang et al., 2017), we here investigated whether state-of-the-art ANN language models (e.g. Devlin et al., 2018; Pennington et al., 2014; Radford et al., 2019) capture human brain activity elicited during language comprehension. We tested 43 language models spanning major current model classes on three neural datasets (including neuroimaging and intracranial recordings) and found that the most powerful generative transformer models (Radford et al., 2019) accurately predict neural responses, in some cases achieving near-perfect predictivity relative to the noise ceiling. In contrast, simpler word-based embedding models (e.g. Pennington et al., 2014) only poorly predict neural responses ({\textless}10\% predictivity). Models’ predictivities are consistent across neural datasets, and also correlate with their success on a next-word-prediction task (but not other language tasks) and ability to explain human comprehension difficulty in an independent behavioral dataset. Intriguingly, model architecture alone drives a large portion of brain predictivity, with each model’s untrained score predictive of its trained score. These results support the hypothesis that a drive to predict future inputs may shape human language processing, and perhaps the way knowledge of language is learned and organized in the brain. In addition, the finding of strong correspondences between ANNs and human representations opens the door to using the growing suite of tools for neural network interpretation to test hypotheses about the human mind.},
	language = {en},
	urldate = {2020-07-10},
	institution = {Neuroscience},
	author = {Schrimpf, Martin and Blank, Idan and Tuckute, Greta and Kauf, Carina and Hosseini, Eghbal A. and Kanwisher, Nancy and Tenenbaum, Joshua and Fedorenko, Evelina},
	month = jun,
	year = {2020},
	doi = {10.1101/2020.06.26.174482},
}

@article{kuperberg_neural_2007,
	title = {Neural mechanisms of language comprehension: {Challenges} to syntax},
	volume = {1146},
	issn = {00068993},
	shorttitle = {Neural mechanisms of language comprehension},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0006899306036821},
	doi = {10.1016/j.brainres.2006.12.063},
	language = {en},
	urldate = {2020-07-20},
	journal = {Brain Research},
	author = {Kuperberg, Gina R.},
	month = may,
	year = {2007},
	pages = {23--49},
}

@article{friederici_mapping_2007,
	title = {Mapping sentence form onto meaning: {The} syntax–semantic interface},
	volume = {1146},
	issn = {00068993},
	shorttitle = {Mapping sentence form onto meaning},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0006899306024334},
	doi = {10.1016/j.brainres.2006.08.038},
	language = {en},
	urldate = {2020-07-20},
	journal = {Brain Research},
	author = {Friederici, Angela D. and Weissenborn, Jürgen},
	month = may,
	year = {2007},
	pages = {50--58},
}

@article{lakretz_exploring_2020,
	title = {Exploring processing of nested dependencies in neural-network language models and humans},
	url = {http://arxiv.org/abs/2006.11098},
	abstract = {Recursive processing in sentence comprehension is considered a hallmark of human linguistic abilities. However, its underlying neural mechanisms remain largely unknown. We studied whether a recurrent neural network with Long Short-Term Memory units can mimic a central aspect of human sentence processing, namely the handling of long-distance agreement dependencies. Although the network was solely trained to predict the next word in a large corpus, analysis showed the emergence of a small set of specialized units that successfully handled local and long-distance syntactic agreement for grammatical number. However, simulations showed that this mechanism does not support full recursion and fails with some long-range embedded dependencies. We tested the model's predictions in a behavioral experiment where humans detected violations in number agreement in sentences with systematic variations in the singular/plural status of multiple nouns, with or without embedding. Human and model error patterns were remarkably similar, showing that the model echoes various effects observed in human data. However, a key difference was that, with embedded long-range dependencies, humans remained above chance level, while the model's systematic errors brought it below chance. Overall, our study shows that exploring the ways in which modern artificial neural networks process sentences leads to precise and testable hypotheses about human linguistic performance.},
	urldate = {2020-07-18},
	journal = {arXiv:2006.11098 [cs]},
	author = {Lakretz, Yair and Hupkes, Dieuwke and Vergallito, Alessandra and Marelli, Marco and Baroni, Marco and Dehaene, Stanislas},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.11098},
	keywords = {NLP, RNN, NA-task},
}

@article{tanenhaus_integration_1995,
	title = {Integration of visual and linguistic information in spoken language comprehension},
	volume = {268},
	issn = {0036-8075, 1095-9203},
	url = {https://www.sciencemag.org/lookup/doi/10.1126/science.7777863},
	doi = {10.1126/science.7777863},
	language = {en},
	number = {5217},
	urldate = {2020-06-29},
	journal = {Science},
	author = {Tanenhaus, M. and Spivey-Knowlton, M. and Eberhard, K. and Sedivy, J.},
	month = jun,
	year = {1995},
	pages = {1632--1634},
}

@inproceedings{gauthier_linking_2019,
	address = {Hong Kong, China},
	title = {Linking artificial and human neural representations of language},
	url = {https://www.aclweb.org/anthology/D19-1050},
	doi = {10.18653/v1/D19-1050},
	language = {en},
	urldate = {2020-06-29},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Gauthier, Jon and Levy, Roger},
	year = {2019},
	keywords = {fMRI, NLP},
	pages = {529--539},
}

@article{widmann_digital_2015,
	series = {Cutting-edge {EEG} {Methods}},
	title = {Digital filter design for electrophysiological data – a practical approach},
	volume = {250},
	issn = {0165-0270},
	url = {http://www.sciencedirect.com/science/article/pii/S0165027014002866},
	doi = {10.1016/j.jneumeth.2014.08.002},
	abstract = {Background
Filtering is a ubiquitous step in the preprocessing of electroencephalographic (EEG) and magnetoencephalographic (MEG) data. Besides the intended effect of the attenuation of signal components considered as noise, filtering can also result in various unintended adverse filter effects (distortions such as smoothing) and filter artifacts.
Method
We give some practical guidelines for the evaluation of filter responses (impulse and frequency response) and the selection of filter types (high-pass/low-pass/band-pass/band-stop; finite/infinite impulse response, FIR/IIR) and filter parameters (cutoff frequencies, filter order and roll-off, ripple, delay and causality) to optimize signal-to-noise ratio and avoid or reduce signal distortions for selected electrophysiological applications.
Results
Various filter implementations in common electrophysiology software packages are introduced and discussed. Resulting filter responses are compared and evaluated.
Conclusion
We present strategies for recognizing common adverse filter effects and filter artifacts and demonstrate them in practical examples. Best practices and recommendations for the selection and reporting of filter parameters, limitations, and alternatives to filtering are discussed.},
	language = {en},
	urldate = {2020-06-29},
	journal = {Journal of Neuroscience Methods},
	author = {Widmann, Andreas and Schröger, Erich and Maess, Burkhard},
	month = jul,
	year = {2015},
	keywords = {MEG, EEG, methods, filters, DSP},
	pages = {34--46},
}

@article{lakretz_what_2020,
	title = {What limits our capacity to process nested long-range dependencies in sentence comprehension?},
	volume = {22},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1099-4300/22/4/446},
	doi = {10.3390/e22040446},
	abstract = {Sentence comprehension requires inferring, from a sequence of words, the structure of syntactic relationships that bind these words into a semantic representation. Our limited ability to build some specific syntactic structures, such as nested center-embedded clauses (e.g., \&ldquo;The dog that the cat that the mouse bit chased ran away\&rdquo;), suggests a striking capacity limitation of sentence processing, and thus offers a window to understand how the human brain processes sentences. Here, we review the main hypotheses proposed in psycholinguistics to explain such capacity limitation. We then introduce an alternative approach, derived from our recent work on artificial neural networks optimized for language modeling, and predict that capacity limitation derives from the emergence of sparse and feature-specific syntactic units. Unlike psycholinguistic theories, our neural network-based framework provides precise capacity-limit predictions without making any a priori assumptions about the form of the grammar or parser. Finally, we discuss how our framework may clarify the mechanistic underpinning of language processing and its limitations in the human brain.},
	language = {en},
	number = {4},
	urldate = {2020-06-26},
	journal = {Entropy},
	author = {Lakretz, Yair and Dehaene, Stanislas and King, Jean-Rémi},
	month = apr,
	year = {2020},
	note = {Number: 4
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {syntax, LSTM, RNN, surprisal, ANN, ACT-R, DLT},
	pages = {446},
}

@article{jeffery_transitions_2020,
	title = {Transitions in brain evolution: {Space}, time and entropy},
	volume = {43},
	issn = {0166-2236, 1878-108X},
	shorttitle = {Transitions in {Brain} {Evolution}},
	url = {https://www.cell.com/trends/neurosciences/abstract/S0166-2236(20)30099-0},
	doi = {10.1016/j.tins.2020.04.008},
	language = {English},
	number = {7},
	urldate = {2020-06-26},
	journal = {Trends in Neurosciences},
	author = {Jeffery, Kate J. and Rovelli, Carlo},
	month = jul,
	year = {2020},
	pmid = {32414530},
	note = {Publisher: Elsevier},
	keywords = {evolution, entropy, time, complexity, free energy},
	pages = {467--474},
}

@article{freedman_issues_1995,
	title = {Some issues in the foundation of statistics},
	volume = {1},
	issn = {1572-8471},
	url = {https://doi.org/10.1007/BF00208723},
	doi = {10.1007/BF00208723},
	abstract = {After sketching the conflict between objectivists and subjectivists on the foundations of statistics, this paper discusses an issue facing statisticians of both schools, namely, model validation. Statistical models originate in the study of games of chance, and have been successfully applied in the physical and life sciences. However, there are basic problems in applying the models to social phenomena; some of the difficulties will be pointed out. Hooke's law will be contrasted with regression models for salary discrimination, the latter being a fairly typical application in the social sciences.},
	language = {en},
	number = {1},
	urldate = {2020-06-23},
	journal = {Foundations of Science},
	author = {Freedman, David},
	month = mar,
	year = {1995},
	keywords = {statistics, bayesian models, Bayes, Kolmogorov theorem, probability},
	pages = {19--39},
}

@article{breiman_statistical_2001,
	title = {Statistical modeling: {The} two cultures},
	volume = {16},
	issn = {0883-4237},
	shorttitle = {Statistical {Modeling}},
	url = {https://www.jstor.org/stable/2676681},
	abstract = {There are two cultures in the use of statistical modeling to reach conclusions from data. One assumes that the data are generated by a given stochastic data model. The other uses algorithmic models and treats the data mechanism as unknown. The statistical community has been committed to the almost exclusive use of data models. This commitment has led to irrelevant theory, questionable conclusions, and has kept statisticians from working on a large range of interesting current problems. Algorithmic modeling, both in theory and practice, has developed rapidly in fields outside statistics. It can be used both on large complex data sets and as a more accurate and informative alternative to data modeling on smaller data sets. If our goal as a field is to use data to solve problems, then we need to move away from exclusive dependence on data models and adopt a more diverse set of tools.},
	number = {3},
	urldate = {2020-06-23},
	journal = {Statistical Science},
	author = {Breiman, Leo},
	year = {2001},
	note = {Publisher: Institute of Mathematical Statistics},
	keywords = {statistics, neural networks},
	pages = {199--215},
}

@article{pickering_integrated_2013,
	title = {An integrated theory of language production and comprehension},
	volume = {36},
	issn = {0140-525X, 1469-1825},
	url = {https://www.cambridge.org/core/product/identifier/S0140525X12001495/type/journal_article},
	doi = {10.1017/S0140525X12001495},
	language = {en},
	number = {4},
	urldate = {2020-06-22},
	journal = {Behavioral and Brain Sciences},
	author = {Pickering, Martin J. and Garrod, Simon},
	month = aug,
	year = {2013},
	keywords = {language comprehension, language production},
	pages = {329--347},
}

@article{levinson_turn-taking_2016,
	title = {Turn-taking in human communication – origins and implications for language processing},
	volume = {20},
	issn = {1364-6613},
	url = {http://www.sciencedirect.com/science/article/pii/S1364661315002764},
	doi = {10.1016/j.tics.2015.10.010},
	abstract = {Most language usage is interactive, involving rapid turn-taking. The turn-taking system has a number of striking properties: turns are short and responses are remarkably rapid, but turns are of varying length and often of very complex construction such that the underlying cognitive processing is highly compressed. Although neglected in cognitive science, the system has deep implications for language processing and acquisition that are only now becoming clear. Appearing earlier in ontogeny than linguistic competence, it is also found across all the major primate clades. This suggests a possible phylogenetic continuity, which may provide key insights into language evolution.},
	language = {en},
	number = {1},
	urldate = {2020-06-22},
	journal = {Trends in Cognitive Sciences},
	author = {Levinson, Stephen C.},
	month = jan,
	year = {2016},
	keywords = {language comprehension, speech, dialogue, language acquisition, predictive processing, turn taking},
	pages = {6--14},
}

@article{liu_spike-frequency_2001,
	title = {Spike-frequency adaptation of a generalized leaky integrate-and-fire model neuron},
	volume = {10},
	issn = {0929-5313},
	abstract = {Although spike-frequency adaptation is a commonly observed property of neurons, its functional implications are still poorly understood. In this work, using a leaky integrate-and-fire neural model that includes a Ca2+-activated K+ current (IAHP), we develop a quantitative theory of adaptation temporal dynamics and compare our results with recent in vivo intracellular recordings from pyramidal cells in the cat visual cortex. Experimentally testable relations between the degree and the time constant of spike-frequency adaptation are predicted. We also contrast the IAHP model with an alternative adaptation model based on a dynamical firing threshold. Possible roles of adaptation in temporal computation are explored, as a a time-delayed neuronal self-inhibition mechanism. Our results include the following: (1) given the same firing rate, the variability of interspike intervals (ISIs) is either reduced or enhanced by adaptation, depending on whether the IAHP dynamics is fast or slow compared with the mean ISI in the output spike train; (2) when the inputs are Poisson-distributed (uncorrelated), adaptation generates temporal anticorrelation between ISIs, we suggest that measurement of this negative correlation provides a probe to assess the strength of IAHP in vivo; (3) the forward masking effect produced by the slow dynamics of IAHP is nonlinear and effective at selecting the strongest input among competing sources of input signals.},
	language = {eng},
	number = {1},
	journal = {Journal of Computational Neuroscience},
	author = {Liu, Y. H. and Wang, X. J.},
	year = {2001},
	pmid = {11316338},
	keywords = {readme, project.lsnn, LIF, computational neuroscience, spike-rate adaptation},
	pages = {25--45},
}

@article{chafee_matching_1998,
	title = {Matching patterns of activity in primate prefrontal area 8a and parietal area 7ip neurons during a spatial working memory task},
	volume = {79},
	issn = {0022-3077},
	url = {https://journals.physiology.org/doi/full/10.1152/jn.1998.79.6.2919},
	doi = {10.1152/jn.1998.79.6.2919},
	abstract = {Chafee, Matthew V. and Patricia S. Goldman-Rakic. Matching patterns of activity in primate prefrontal area 8a and parietal area 7ip neurons during a spatial working memory task. J. Neurophysiol. 79: 2919–2940, 1998. Single-unit recording studies of posterior parietal neurons have indicated a similarity of neuronal activation to that observed in the dorsolateral prefrontal cortex in relation to performance of delayed saccade tasks. A key issue addressed in the present study is whether the different classes of neuronal activity observed in these tasks are encountered more frequently in one or the other area or otherwise exhibit region-specific properties. The present study is the first to directly compare these patterns of neuronal activity by alternately recording from parietal area 7ip and prefrontal area 8a, under the identical behavioral conditions, within the same hemisphere of two monkeys performing an oculomotor delayed response task. The firing rate of 222 posterior parietal and 235 prefrontal neurons significantly changed during the cue, delay, and/or saccade periods of the task. Neuronal responses in the two areas could be distinguished only by subtle differences in their incidence and timing. Thus neurons responding to the cue appeared earliest and were more frequent among the task-related neurons within parietal cortex, whereas neurons exhibiting delay-period activity accounted for a larger proportion of task-related neurons in prefrontal cortex. Otherwise, the task-related neuronal activities were remarkably similar. Cue period activity in prefrontal and parietal cortex exhibited comparable spatial tuning and temporal duration characteristics, taking the form of phasic, tonic, or combined phasic/tonic excitation in both cortical populations. Neurons in both cortical areas exhibited sustained activity during the delay period with nearly identical spatial tuning. The various patterns of delay-period activity—tonic, increasing or decreasing, alone or in combination with greater activation during cue and/or saccade periods—likewise were distributed to both cortical areas. Finally, similarities in the two populations extended to the proportion and spatial tuning of presaccadic and postsaccadic neuronal activity occurring in relation to the memory-guided saccade. The present findings support and extend evidence for a faithful duplication of receptive field properties and virtually every other dimension of task-related activity observed when parietal and prefrontal cortex are recruited to a common task. This striking similarity attests to the principal that information shared by a prefrontal region and a sensory association area with which it is connected is domain specific and not subject to hierarchical elaboration, as is evident at earlier stages of visuospatial processing.},
	number = {6},
	urldate = {2020-06-22},
	journal = {Journal of Neurophysiology},
	author = {Chafee, Matthew V. and Goldman-Rakic, Patricia S.},
	month = jun,
	year = {1998},
	note = {Publisher: American Physiological Society},
	keywords = {PFC, persistent activity, parietal cortex},
	pages = {2919--2940},
}

@article{funahashi_mnemonic_1989,
	title = {Mnemonic coding of visual space in the monkey's dorsolateral prefrontal cortex},
	volume = {61},
	issn = {0022-3077},
	url = {https://journals.physiology.org/doi/abs/10.1152/jn.1989.61.2.331},
	doi = {10.1152/jn.1989.61.2.331},
	abstract = {1. An oculomotor delayed-response task was used to examine the spatial memory functions of neurons in primate prefrontal cortex. Monkeys were trained to fixate a central spot during a brief presentation (0.5 s) of a peripheral cue and throughout a subsequent delay period (1-6 s), and then, upon the extinction of the fixation target, to make a saccadic eye movement to where the cue had been presented. Cues were usually presented in one of eight different locations separated by 45 degrees. This task thus requires monkeys to direct their gaze to the location of a remembered visual cue, controls the retinal coordinates of the visual cues, controls the monkey's oculomotor behavior during the delay period, and also allows precise measurement of the timing and direction of the relevant behavioral responses. 2. Recordings were obtained from 288 neurons in the prefrontal cortex within and surrounding the principal sulcus (PS) while monkeys performed this task. An additional 31 neurons in the frontal eye fields (FEF) region within and near the anterior bank of the arcuate sulcus were also studied. 3. Of the 288 PS neurons, 170 exhibited task-related activity during at least one phase of this task and, of these, 87 showed significant excitation or inhibition of activity during the delay period relative to activity during the intertrial interval. 4. Delay period activity was classified as directional for 79\% of these 87 neurons in that significant responses only occurred following cues located over a certain range of visual field directions and were weak or absent for other cue directions. The remaining 21\% were omnidirectional, i.e., showed comparable delay period activity for all visual field locations tested. Directional preferences, or lack thereof, were maintained across different delay intervals (1-6 s). 5. For 50 of the 87 PS neurons, activity during the delay period was significantly elevated above the neuron's spontaneous rate for at least one cue location; for the remaining 37 neurons only inhibitory delay period activity was seen. Nearly all (92\%) neurons with excitatory delay period activity were directional and few (8\%) were omnidirectional. Most (62\%) neurons with purely inhibitory delay period activity were directional, but a substantial minority (38\%) was omnidirectional. 6. Fifteen of the neurons with excitatory directional delay period activity also had significant inhibitory delay period activity for other cue directions. These inhibitory responses were usually strongest for, or centered about, cue directions roughly opposite those optimal for excitatory responses.(ABSTRACT TRUNCATED AT 400 WORDS)},
	number = {2},
	urldate = {2020-06-22},
	journal = {Journal of Neurophysiology},
	author = {Funahashi, S. and Bruce, C. J. and Goldman-Rakic, P. S.},
	month = feb,
	year = {1989},
	note = {Publisher: American Physiological Society},
	keywords = {PFC, persistent activity},
	pages = {331--349},
}

@article{masse_reevaluating_2020,
	title = {Reevaluating the role of persistent neural activity in short-term memory},
	volume = {24},
	issn = {1364-6613},
	url = {http://www.sciencedirect.com/science/article/pii/S1364661319303201},
	doi = {10.1016/j.tics.2019.12.014},
	abstract = {A traditional view of short-term working memory (STM) is that task-relevant information is maintained ‘online’ in persistent spiking activity. However, recent experimental and modeling studies have begun to question this long-held belief. In this review, we discuss new evidence demonstrating that information can be ‘silently’ maintained via short-term synaptic plasticity (STSP) without the need for persistent activity. We discuss how the neural mechanisms underlying STM are inextricably linked with the cognitive demands of the task, such that the passive maintenance and the active manipulation of information are subserved differently in the brain. Together, these recent findings point towards a more nuanced view of STM in which multiple substrates work in concert to support our ability to temporarily maintain and manipulate information.},
	language = {en},
	number = {3},
	urldate = {2020-06-22},
	journal = {Trends in Cognitive Sciences},
	author = {Masse, Nicolas Y. and Rosen, Matthew C. and Freedman, David J.},
	month = mar,
	year = {2020},
	keywords = {working memory, persistent activity},
	pages = {242--258},
}

@article{dvornek_learning_2018,
	title = {Learning generalizable recurrent neural networks from small task-{fMRI} datasets},
	volume = {11072},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6411297/},
	doi = {10.1007/978-3-030-00931-1_38},
	abstract = {Deep learning has become the new state-of-the-art for many problems in image analysis. However, large datasets are often required for such deep networks to learn effectively. This poses a difficult challenge for many medical image analysis problems in which only a small number of subjects are available, e.g., patients undergoing a new treatment. In this work, we propose a number of approaches for learning generalizable recurrent neural networks from smaller task-fMRI datasets: 1) a resampling method for ROI-based fMRI analysis to create augmented data; 2) inclusion of a small number of non-imaging variables to provide subject-specific initialization of the recurrent neural network; and 3) selection of the most generalizable model from multiple reinitialized training runs using criteria based on only training loss. Using cross-validation to assess model performance, we demonstrate the effectiveness of the proposed methods to train recurrent neural networks from small datasets to predict treatment outcome for children with autism spectrum disorder (N = 21) and classify autistic vs. typical control subjects (N = 40) from task-fMRI scans.},
	urldate = {2020-06-22},
	journal = {Medical image computing and computer-assisted intervention : MICCAI ... International Conference on Medical Image Computing and Computer-Assisted Intervention},
	author = {Dvornek, Nicha C. and Yang, Daniel and Ventola, Pamela and Duncan, James S.},
	month = sep,
	year = {2018},
	pmid = {30873514},
	pmcid = {PMC6411297},
	keywords = {open science, open data},
	pages = {329--337},
}

@article{thompson_dataset_2020,
	title = {Dataset decay and the problem of sequential analyses on open datasets},
	volume = {9},
	issn = {2050-084X},
	url = {https://doi.org/10.7554/eLife.53498},
	doi = {10.7554/eLife.53498},
	abstract = {Open data allows researchers to explore pre-existing datasets in new ways. However, if many researchers reuse the same dataset, multiple statistical testing may increase false positives. Here we demonstrate that sequential hypothesis testing on the same dataset by multiple researchers can inflate error rates. We go on to discuss a number of correction procedures that can reduce the number of false positives, and the challenges associated with these correction procedures.},
	urldate = {2020-06-22},
	journal = {eLife},
	author = {Thompson, William Hedley and Wright, Jessey and Bissett, Patrick G and Poldrack, Russell A},
	editor = {Rodgers, Peter and Baker, Chris I and Holmes, Nick and Baker, Chris I and Rousselet, Guillaume A},
	month = may,
	year = {2020},
	note = {Publisher: eLife Sciences Publications, Ltd},
	keywords = {open science, data sharing},
	pages = {e53498},
}

@article{gorgolewski_impact_2015,
	title = {The impact of shared data in neuroimaging: the case of {OpenfMRI}.org},
	volume = {4},
	shorttitle = {{\textless}p{\textgreater}{The} impact of shared data in neuroimaging},
	url = {https://f1000research.com/posters/4-299},
	doi = {10.7490/f1000research.1110040.1},
	abstract = {Read this work by Gorgolewski KJ, at F1000Research.},
	urldate = {2020-06-21},
	journal = {F1000Research},
	author = {Gorgolewski, Krzysztof J. and Wheeler, Kelsey and Halchenko, Yaroslav O. and Poline, Jean-Babtiste and Poldrack, Russel A.},
	month = jul,
	year = {2015},
}

@article{ritchie_decoding_2019,
	title = {Decoding the brain: {Neural} representation and the limits of multivariate pattern analysis in cognitive neuroscience},
	volume = {70},
	issn = {0007-0882, 1464-3537},
	shorttitle = {Decoding the {Brain}},
	url = {https://academic.oup.com/bjps/article/70/2/581/4105010},
	doi = {10.1093/bjps/axx023},
	language = {en},
	number = {2},
	urldate = {2020-06-21},
	journal = {The British Journal for the Philosophy of Science},
	author = {Ritchie, J Brendan and Kaplan, David Michael and Klein, Colin},
	month = jun,
	year = {2019},
	pages = {581--607},
}

@article{logar_possible_2011,
	title = {A possible use of {EEG} signals in a brain-computer interface},
	volume = {80},
	copyright = {Copyright (c)},
	issn = {1581-0224},
	url = {https://vestnik.szd.si/index.php/ZdravVest/article/view/111},
	abstract = {Povzetek
					Background: Newest insights in the field of brain-information coding suggest that the information is transferred between the active regions of the brain as a phase-coded content. Considering the informational richness of the electroencephalographic (EEG) signals, we can assume that by using appropriate methods of signal processing it is possible to decode some of this information. The authors would like to show that using a phase-demodulation approach it is possible to successfully decode the information about the wrist movements of a complex dynamic visuo-motor task (dVM). Since the causality of the methodology is assured, it is also usable for the development of a brain-computer interface (BCI).
Methods: In this study we measured the EEG signals from four subjects while performing a dynamic visuo-motor task. For decoding the information, which is supposedly carried by the EEG signals we used brain-rhythm filtering, phase demodulation and principal component analysis approach. As a prediction model for wrist movements, fuzzy inference model was used.
Results: The presented results show that the EEG signals measured during the performance of dVM tasks carry enough information about the current action for satisfactory decoding and prediction of the wrist movements. Successful estimation of the motor action is proved also by obtaining reasonably high values of the correlation coefficients.
Conclusions: The study has shown that using the proposed methodology it is possible to decode the EEG information of the wrist movements during dVM tasks. The study has also shown that these relatively simple methods of signal processing and a fuzzy model are applicable to the development of a closed-loop, non-invasive BCI.},
	language = {en},
	number = {2},
	urldate = {2020-06-18},
	journal = {Slovenian Medical Journal},
	author = {Logar, Vito and Belič, Aleš},
	month = feb,
	year = {2011},
	note = {Number: 2},
	keywords = {EEG, beta, BCI},
}

@article{koskinen_brain_2020,
	title = {Brain activity reflects the predictability of word sequences in listened continuous speech},
	volume = {219},
	issn = {1053-8119},
	url = {http://www.sciencedirect.com/science/article/pii/S1053811920304225},
	doi = {10.1016/j.neuroimage.2020.116936},
	abstract = {Natural speech builds on contextual relations that can prompt predictions of upcoming utterances. To study the neural underpinnings of such predictive processing we asked 10 healthy adults to listen to a 1-h-long audiobook while their magnetoencephalographic (MEG) brain activity was recorded. We correlated the MEG signals with acoustic speech envelope, as well as with estimates of Bayesian word probability with and without the contextual word sequence (N-gram and Unigram, respectively), with a focus on time-lags. The MEG signals of auditory and sensorimotor cortices were strongly coupled to the speech envelope at the rates of syllables (4–8 ​Hz) and of prosody and intonation (0.5–2 ​Hz). The probability structure of word sequences, independently of the acoustical features, affected the ≤ 2-Hz signals extensively in auditory and rolandic regions, in precuneus, occipital cortices, and lateral and medial frontal regions. Fine-grained temporal progression patterns occurred across brain regions 100–1000 ​ms after word onsets. Although the acoustic effects were observed in both hemispheres, the contextual influences were statistically significantly lateralized to the left hemisphere. These results serve as a brain signature of the predictability of word sequences in listened continuous speech, confirming and extending previous results to demonstrate that deeply-learned knowledge and recent contextual information are employed dynamically and in a left-hemisphere-dominant manner in predicting the forthcoming words in natural speech.},
	language = {en},
	urldate = {2020-06-17},
	journal = {NeuroImage},
	author = {Koskinen, Miika and Kurimo, Mikko and Gross, Joachim and Hyvärinen, Aapo and Hari, Riitta},
	month = oct,
	year = {2020},
	pages = {116936},
}

@article{donhauser_two_2020,
	title = {Two distinct neural timescales for predictive speech processing},
	volume = {105},
	issn = {08966273},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627319308931},
	doi = {10.1016/j.neuron.2019.10.019},
	language = {en},
	number = {2},
	urldate = {2020-06-17},
	journal = {Neuron},
	author = {Donhauser, Peter W. and Baillet, Sylvain},
	month = jan,
	year = {2020},
	pages = {385--393.e9},
}

@article{berger_uber_1929,
	title = {Über das elektrenkephalogramm des menschen},
	volume = {87},
	issn = {0003-9373, 1433-8491},
	url = {http://link.springer.com/10.1007/BF01797193},
	doi = {10.1007/BF01797193},
	language = {de},
	number = {1},
	urldate = {2020-06-16},
	journal = {Archiv für Psychiatrie und Nervenkrankheiten},
	author = {Berger, Hans},
	month = dec,
	year = {1929},
	keywords = {neuronal oscillations, alpha},
	pages = {527--570},
}

@article{nieuwland_largescale_2018,
	title = {Large-scale replication study reveals a limit on probabilistic prediction in language comprehension},
	volume = {7},
	issn = {2050-084X},
	url = {https://elifesciences.org/articles/33468},
	doi = {10.7554/eLife.33468},
	language = {en},
	urldate = {2019-01-23},
	journal = {eLife},
	author = {Nieuwland, Mante S and Politzer-Ahles, Stephen and Heyselaar, Evelien and Segaert, Katrien and Darley, Emily and Kazanina, Nina and Von Grebmer Zu Wolfsthurn, Sarah and Bartolozzi, Federica and Kogan, Vita and Ito, Aine and Mézière, Diane and Barr, Dale J and Rousselet, Guillaume A and Ferguson, Heather J and Busch-Moreno, Simon and Fu, Xiao and Tuomainen, Jyrki and Kulakova, Eugenia and Husband, E Matthew and Donaldson, David I and Kohút, Zdenko and Rueschemeyer, Shirley-Ann and Huettig, Falk},
	month = apr,
	year = {2018},
	keywords = {EEG, project.streams, predictive coding, replication},
}

@article{vanrullen_perceptual_2016,
	title = {Perceptual cycles},
	volume = {20},
	issn = {13646613},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661316301048},
	doi = {10.1016/j.tics.2016.07.006},
	language = {en},
	number = {10},
	urldate = {2020-06-15},
	journal = {Trends in Cognitive Sciences},
	author = {VanRullen, Rufin},
	month = oct,
	year = {2016},
	keywords = {perception, neuronal oscillations},
	pages = {723--735},
}

@article{cole_brain_2017,
	title = {Brain oscillations and the importance of waveform shape},
	volume = {21},
	issn = {13646613},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661316302182},
	doi = {10.1016/j.tics.2016.12.008},
	language = {en},
	number = {2},
	urldate = {2020-06-15},
	journal = {Trends in Cognitive Sciences},
	author = {Cole, Scott R. and Voytek, Bradley},
	month = feb,
	year = {2017},
	keywords = {neuronal oscillations},
	pages = {137--149},
}

@techreport{gao_neuronal_2020-1,
	type = {preprint},
	title = {Neuronal timescales are functionally dynamic and shaped by cortical microarchitecture},
	url = {http://biorxiv.org/lookup/doi/10.1101/2020.05.25.115378},
	abstract = {Abstract
          
            Complex cognitive functions such as working memory and decision-making require the maintenance of information over many timescales, from transient sensory stimuli to long-term contextual cues
            1
            . However, while theoretical accounts predict that a corresponding hierarchy of neuronal timescales likely emerges as a result of graded variations in recurrent synaptic excitation
            2–4
            , direct evidence in the human cortex is lacking. This limits our ability to study how other cytoarchitectural and cell-intrinsic features shape the temporal patterns of cortical activity
            5–7
            , and whether neuronal timescales are dynamic and relevant for human cognition. Here, we use a novel computational approach to infer neuronal timescales from intracranial recordings and construct a continuous gradient across the human cortex. We find that timescales increase along the principal sensorimotor-to-association axis
            7–9
            , where higher-order association areas have longer neuronal timescales. These measurements reflect transmembrane current fluctuations and scale with single-unit spiking timescales across the macaque cortex
            10
            . Cortexwide transcriptomic analysis
            11–13
            in humans confirms direct alignment between timescales and expression of excitation- and inhibition-related genes, but further identifies genes specifically related to voltage-gated transmembrane ion transporters. Finally, neuronal timescales are functionally dynamic: prefrontal cortex timescales expand during working memory maintenance and predict individual performance, while cortex-wide timescales compress with aging. Thus, neuronal timescales follow cytoarchitectonic gradients across the human cortex, and are relevant for cognition in both short- and long-terms, bridging microcircuit physiology with macroscale dynamics and behavior.
          
          
            Data availability
            
              All data analyzed in this manuscript are from open data sources. All code used for all analyses and plots are publicly available on GitHub at
              https://github.com/rdqao/field-echos
              and
              https://github.com/rudyvdbrink/surface\_projection
              . See Extended Data Table 1 and 2.},
	language = {en},
	urldate = {2020-06-15},
	institution = {Neuroscience},
	author = {Gao, Richard and van den Brink, Ruud L. and Pfeffer, Thomas and Voytek, Bradley},
	month = may,
	year = {2020},
	doi = {10.1101/2020.05.25.115378},
	keywords = {ECoG, timescales},
}

@article{kiebel_hierarchy_2008,
	title = {A hierarchy of time-scales and the brain},
	volume = {4},
	issn = {1553-7358},
	url = {https://dx.plos.org/10.1371/journal.pcbi.1000209},
	doi = {10.1371/journal.pcbi.1000209},
	language = {en},
	number = {11},
	urldate = {2020-06-15},
	journal = {PLoS Computational Biology},
	author = {Kiebel, Stefan J. and Daunizeau, Jean and Friston, Karl J.},
	editor = {Sporns, Olaf},
	month = nov,
	year = {2008},
	pages = {e1000209},
}

@article{petersson_neurobiology_2012-1,
	title = {The neurobiology of syntax: beyond string sets},
	volume = {367},
	issn = {0962-8436, 1471-2970},
	shorttitle = {The neurobiology of syntax},
	url = {https://royalsocietypublishing.org/doi/10.1098/rstb.2012.0101},
	doi = {10.1098/rstb.2012.0101},
	language = {en},
	number = {1598},
	urldate = {2020-06-13},
	journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	author = {Petersson, Karl Magnus and Hagoort, Peter},
	month = jul,
	year = {2012},
	keywords = {computational neuroscience, review},
	pages = {1971--1983},
}

@misc{rooij_psychological_2019,
	title = {Psychological science needs theory development before preregistration},
	url = {https://featuredcontent.psychonomic.org/psychological-science-needs-theory-development-before-preregistration/},
	abstract = {"(…) a substantial proportion of research effort in experimental psychology isn't expended directly in the explanation business; it is expended in the busines},
	language = {en-US},
	urldate = {2020-06-09},
	journal = {Psychonomic Society Featured Content},
	author = {Rooij, Iris van},
	month = jan,
	year = {2019},
	note = {Library Catalog: featuredcontent.psychonomic.org},
	keywords = {explanation, computation, cognitive science},
}

@article{donoho_50_2017,
	title = {50 {Years} of data science},
	volume = {26},
	issn = {1061-8600},
	url = {https://doi.org/10.1080/10618600.2017.1384734},
	doi = {10.1080/10618600.2017.1384734},
	abstract = {More than 50 years ago, John Tukey called for a reformation of academic statistics. In “The Future of Data Analysis,” he pointed to the existence of an as-yet unrecognized science, whose subject of interest was learning from data, or “data analysis.” Ten to 20 years ago, John Chambers, Jeff Wu, Bill Cleveland, and Leo Breiman independently once again urged academic statistics to expand its boundaries beyond the classical domain of theoretical statistics; Chambers called for more emphasis on data preparation and presentation rather than statistical modeling; and Breiman called for emphasis on prediction rather than inference. Cleveland and Wu even suggested the catchy name “data science” for this envisioned field. A recent and growing phenomenon has been the emergence of “data science” programs at major universities, including UC Berkeley, NYU, MIT, and most prominently, the University of Michigan, which in September 2015 announced a \$100M “Data Science Initiative” that aims to hire 35 new faculty. Teaching in these new programs has significant overlap in curricular subject matter with traditional statistics courses; yet many academic statisticians perceive the new programs as “cultural appropriation.” This article reviews some ingredients of the current “data science moment,” including recent commentary about data science in the popular media, and about how/whether data science is really different from statistics. The now-contemplated field of data science amounts to a superset of the fields of statistics and machine learning, which adds some technology for “scaling up” to “big data.” This chosen superset is motivated by commercial rather than intellectual developments. Choosing in this way is likely to miss out on the really important intellectual event of the next 50 years. Because all of science itself will soon become data that can be mined, the imminent revolution in data science is not about mere “scaling up,” but instead the emergence of scientific studies of data analysis science-wide. In the future, we will be able to predict how a proposal to change data analysis workflows would impact the validity of data analysis across all of science, even predicting the impacts field-by-field. Drawing on work by Tukey, Cleveland, Chambers, and Breiman, I present a vision of data science based on the activities of people who are “learning from data,” and I describe an academic field dedicated to improving that activity in an evidence-based manner. This new field is a better academic enlargement of statistics and machine learning than today’s data science initiatives, while being able to accommodate the same short-term goals. Based on a presentation at the Tukey Centennial Workshop, Princeton, NJ, September 18, 2015.},
	number = {4},
	urldate = {2020-06-07},
	journal = {Journal of Computational and Graphical Statistics},
	author = {Donoho, David},
	month = oct,
	year = {2017},
	note = {Publisher: Taylor \& Francis
\_eprint: https://doi.org/10.1080/10618600.2017.1384734},
	keywords = {review, data science},
	pages = {745--766},
}

@article{kayser_extending_2017,
	title = {Extending the knowledge base of foresight: {The} contribution of text mining},
	volume = {116},
	issn = {0040-1625},
	shorttitle = {Extending the knowledge base of foresight},
	url = {http://www.sciencedirect.com/science/article/pii/S0040162516304784},
	doi = {10.1016/j.techfore.2016.10.017},
	abstract = {While the volume of data from heterogeneous sources grows considerably, foresight and its methods rarely benefit from such available data. This work concentrates on textual data and considers its use in foresight to address new research questions and integrate other stakeholders. This textual data can be accessed and systematically examined through text mining which structures and aggregates data in a largely automated manner. By exploiting new data sources (e.g. Twitter, web mining), more actors and views are integrated, and more emphasis is laid on the analysis of social changes. The objective of this article is to explore the potential of text mining for foresight by considering different data sources, text mining approaches, and foresight methods. After clarifying the potential of combining text mining and foresight, examples are outlined for roadmapping and scenario development. As the results show, text mining facilitates the detection and examination of emerging topics and technologies by extending the knowledge base of foresight. Hence, new foresight applications can be designed. In particular, text mining provides a solid base for reflecting on possible futures.},
	language = {en},
	urldate = {2020-06-07},
	journal = {Technological Forecasting and Social Change},
	author = {Kayser, Victoria and Blind, Knut},
	month = mar,
	year = {2017},
	keywords = {NLP, data science, forecasting, text mining},
	pages = {208--215},
	file = {ScienceDirect Snapshot:/Users/kriarm/Zotero/storage/N8KICICZ/S0040162516304784.html:text/html},
}

@techreport{ibm_data_2020,
	address = {Armonk, NY},
	title = {The data science skills competency model: {A} blueprint for the growing  data scientist profession},
	institution = {IBM Corporation 2020},
	author = {IBM},
	year = {2020},
	pages = {11},
}

@incollection{shagrir_marrs_2017-1,
	title = {Marr's computational-level theories and delineating phenomena},
	isbn = {978-0-19-968550-9},
	abstract = {A key component of scientific inquiry, especially inquiry devoted to developing mechanistic explanations, is delineating the phenomenon to be explained. The task of delineating phenomena, however, has not been sufficiently analyzed, even by the new mechanistic philosophers of science. We contend that Marr’s characterization of what he called the computational level (CL) provides a valuable resource for understanding what is involved in delineating phenomena. Unfortunately, the distinctive feature of Marr’s computational level, his dual emphasis on both what is computed and why it is computed, has not been appreciated in philosophical discussions of Marr. Accordingly we offer a distinctive account of CL. This then allows us to develop two important points about delineating phenomena. First, the accounts of phenomena that figure in explanatory practice are typically not qualitative but precise, formal or mathematical, representations. Second, delineating phenomena requires consideration of the demands the environment places on the mechanism—identifying, as Marr put it, the basis of the computed function in the world. As valuable as Marr’s account of CL is in characterizing phenomena, we contend that ultimately he did not go far enough. Determining the relevant demands of the environment on the mechanism often requires detailed empirical investigation. Moreover, often phenomena are reconstituted in the course of inquiry on the mechanism itself.},
	booktitle = {Integrating {Psychology} and {Neuroscience}: {Prospects} and {Problems}},
	publisher = {Oxford University Press},
	author = {Shagrir, Oron and Bechtel, William},
	editor = {Kaplan, David M.},
	year = {2017},
	pages = {190--214},
}

@article{small_neurobiological_2004,
	title = {On the neurobiological investigation of language understanding in context},
	volume = {89},
	issn = {0093934X},
	doi = {10.1016/S0093-934X(03)00344-4},
	abstract = {There are two significant problems in using functional neuroimaging methods to study language. Improving the state of functional brain imaging will depend on understanding how the dependent measure of brain imaging differs from behavioral dependent measures (the "dependent measure problem") and how the activation of the motor system may be confounded with non-motor aspects of processing in certain experimental designs (the "motor output problem"). To address these problems, it may be necessary to shift the focus of language research from the study of linguistic competence to the understanding of language use. This will require investigations of language processing in full multi-modal and environmental context, monitoring of natural behaviors, novel experimental design, and network-based analysis. Such a combined naturalistic approach could lead to tremendous new insights into language and the brain. ?? 2004 Elsevier Inc. All rights reserved.},
	number = {2},
	journal = {Brain and Language},
	author = {Small, Steven L. and Nusbaum, Howard C.},
	year = {2004},
	pmid = {15068912},
	note = {tex.isbn: 0093-934X
tex.mendeley-tags: PCA,cognitive neuroscience,fMRI,language},
	keywords = {cognitive neuroscience, fMRI, PCA, language},
	pages = {300--311},
}

@book{raymond_cathedral_2001,
	edition = {1 edition},
	title = {The {Cathedral} \& the {Bazaar}: {Musings} on {Linux} and {Open} {Source} by an {Accidental} {Revolutionary}},
	shorttitle = {The {Cathedral} \& the {Bazaar}},
	abstract = {Open source provides the competitive advantage in the Internet Age.  According to the August Forrester Report, 56 percent of IT managers  interviewed at Global 2,500 companies are already using some type of  open source software in their infrastructure and another 6 percent will  install it in the next two years. This revolutionary model for  collaborative software development is being embraced and studied by  many of the biggest players in the high-tech industry, from Sun  Microsystems to IBM to Intel.The Cathedral \& the Bazaar is a must for anyone who cares  about the future of the computer industry or the dynamics of the  information economy. Already, billions of dollars have been made and  lost based on the ideas in this book. Its conclusions will be studied,  debated, and implemented for years to come. According to Bob Young,  "This is Eric Raymond's great contribution to the success of the open  source revolution, to the adoption of Linux-based operating systems,  and to the success of open source users and the companies that  supply them."The interest in open source software development has grown  enormously in the past year. This revised and expanded paperback  edition includes new material on open source developments in 1999 and  2000. Raymond's clear and effective writing style accurately describing  the benefits of open source software has been key to its  success. With major vendors creating acceptance for open source  within companies, independent vendors will become the open source  story in 2001.},
	language = {English},
	publisher = {O'Reilly Media},
	author = {Raymond, Eric S.},
	month = feb,
	year = {2001},
}

@article{meyer_synchronous_2019,
	title = {Synchronous, but not entrained: exogenous and endogenous cortical rhythms of speech and language processing},
	volume = {0},
	issn = {2327-3798},
	shorttitle = {Synchronous, but not entrained},
	url = {https://doi.org/10.1080/23273798.2019.1693050},
	doi = {10.1080/23273798.2019.1693050},
	abstract = {Research on speech processing is often focused on a phenomenon termed “entrainment”, whereby the cortex shadows rhythmic acoustic information with oscillatory activity. Entrainment has been observed to a range of rhythms present in speech; in addition, synchronicity with abstract information (e.g. syntactic structures) has been observed. Entrainment accounts face two challenges: First, speech is not exactly rhythmic; second, synchronicity with representations that lack a clear acoustic counterpart has been described. We propose that apparent entrainment does not always result from acoustic information. Rather, internal rhythms may have functionalities in the generation of abstract representations and predictions. While acoustics may often provide punctate opportunities for entrainment, internal rhythms may also live a life of their own to infer and predict information, leading to intrinsic synchronicity – not to be counted as entrainment. This possibility may open up new research avenues in the psycho– and neurolinguistic study of language processing and language development.},
	number = {0},
	urldate = {2020-05-25},
	journal = {Language, Cognition and Neuroscience},
	author = {Meyer, Lars and Sun, Yue and Martin, Andrea E.},
	month = dec,
	year = {2019},
	note = {Publisher: Routledge
\_eprint: https://doi.org/10.1080/23273798.2019.1693050},
	keywords = {neuronal oscillations, neuronal entrainment, speech processing},
	pages = {1--11},
}

@article{giraud_oscillations_2020,
	title = {Oscillations for all: {A} commentary on {Meyer}, {Sun} \& {Martin} (2020)},
	issn = {2327-3798, 2327-3801},
	url = {https://www.tandfonline.com/doi/full/10.1080/23273798.2020.1764990},
	doi = {10.1080/23273798.2020.1764990},
	language = {en},
	urldate = {2020-05-25},
	journal = {Language, Cognition and Neuroscience},
	author = {Giraud, Anne-Lise},
	month = may,
	year = {2020},
	keywords = {neuronal oscillations, speech processing},
	pages = {1--8},
}

@incollection{cohen_neurophysiological_2017,
	address = {Chichester, UK},
	title = {Neurophysiological oscillations and action monitoring},
	isbn = {978-1-118-92049-7 978-1-118-92054-1},
	url = {http://doi.wiley.com/10.1002/9781118920497.ch14},
	language = {en},
	urldate = {2020-05-25},
	booktitle = {The {Wiley} {Handbook} of {Cognitive} {Control}},
	publisher = {John Wiley \& Sons, Ltd},
	author = {Cohen, Michael X.},
	editor = {Egner, Tobias},
	month = feb,
	year = {2017},
	doi = {10.1002/9781118920497.ch14},
	keywords = {cognitive control, action monitoring, neuonal oscillations},
	pages = {242--258},
}

@article{chomsky_review_1959,
	title = {A review of {B}. {F}. {Skinner}'s verbal behavior},
	volume = {35},
	issn = {0097-8507},
	abstract = {I had intended this review not specifically as a criticism of Skinner's speculations regarding language, but rather as a more general critique of behaviorist (I would now prefer to say "empiricist") speculation as to the nature of higher mental processes. My reason for discussing Skinner's book in such detail was that it was the most careful and thoroughgoing presentation of such speculations, an evaluation that I feel is still accurate. Therefore, if the conclusions I attempted to substantiate in the review are correct, as I believe they are, then Skinner's work can be regarded as, in effect, a reductio ad absurdum of behaviorist assumptions. My personal view is that it is a definite merit, not a defect, of Skinner's work that it can be used for this purpose, and it was for this reason that I tried to deal with it fairly exhaustively. I do not see how his proposals can be improved upon, aside from occasional details and oversights, within the framework of the general assumptions that he accepts. I do not, in other words, see any way in which his proposals can be substantially improved within the general framework of behaviorist or neobehaviorist, or, more generally, empiricist ideas that has dominated much of modern linguistics, psychology, and philosophy. The conclusion that I hoped to establish in the review, by discussing these speculations in their most explicit and detailed form, was that the general point of view was largely mythology, and that its widespread acceptance is not the result of empirical support, persuasive reasoning, or the absence of a plausible alternative.},
	number = {1},
	journal = {Language},
	author = {Chomsky, Noam},
	year = {1959},
	note = {tex.isbn: 00978507
tex.mendeley-tags: behaviorism,cognitive science,linguistics},
	keywords = {behaviourism},
	pages = {26--58},
}

@book{chomsky_minimalist_1995,
	address = {Cambridge, MA},
	title = {Minimalist program},
	publisher = {MIT Press},
	author = {Chomsky, Noam},
	year = {1995},
	keywords = {syntax, universal grammar},
}

@book{bastiaansen_beyond_2011,
	title = {Beyond {ERPs}:},
	shorttitle = {Beyond {ERPs}},
	url = {http://oxfordhandbooks.com/view/10.1093/oxfordhb/9780195374148.001.0001/oxfordhb-9780195374148-e-002},
	urldate = {2020-05-25},
	publisher = {Oxford University Press},
	author = {Bastiaansen, Marcel and Mazaheri, Ali and Jensen, Ole},
	month = dec,
	year = {2011},
	doi = {10.1093/oxfordhb/9780195374148.013.0024},
	keywords = {neuronal oscillations, neuronal synchrony},
}

@article{singer_synchronization_1993,
	title = {Synchronization of cortical activity and its putative role in information processing and learning},
	volume = {55},
	issn = {0066-4278, 1545-1585},
	url = {http://www.annualreviews.org/doi/10.1146/annurev.ph.55.030193.002025},
	doi = {10.1146/annurev.ph.55.030193.002025},
	language = {en},
	number = {1},
	urldate = {2020-05-25},
	journal = {Annual Review of Physiology},
	author = {Singer, W},
	month = oct,
	year = {1993},
	pages = {349--374},
}

@article{van_berkum_anticipating_2005,
	title = {Anticipating upcoming words in discourse: {Evidence} from {ERPs} and reading times.},
	volume = {31},
	issn = {1939-1285, 0278-7393},
	shorttitle = {Anticipating {Upcoming} {Words} in {Discourse}},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0278-7393.31.3.443},
	doi = {10.1037/0278-7393.31.3.443},
	language = {en},
	number = {3},
	urldate = {2020-05-25},
	journal = {Journal of Experimental Psychology: Learning, Memory, and Cognition},
	author = {Van Berkum, Jos J. A. and Brown, Colin M. and Zwitserlood, Pienie and Kooijman, Valesca and Hagoort, Peter},
	year = {2005},
	pages = {443--467},
}

@article{van_petten_prediction_2012,
	title = {Prediction during language comprehension: {Benefits}, costs, and {ERP} components},
	volume = {83},
	issn = {01678760},
	shorttitle = {Prediction during language comprehension},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0167876011002819},
	doi = {10.1016/j.ijpsycho.2011.09.015},
	language = {en},
	number = {2},
	urldate = {2020-05-25},
	journal = {International Journal of Psychophysiology},
	author = {Van Petten, Cyma and Luka, Barbara J.},
	month = feb,
	year = {2012},
	keywords = {ERP, predictive processing},
	pages = {176--190},
}

@article{saffran_statistical_2003,
	title = {Statistical language learning: mechanisms and donstraints},
	volume = {12},
	issn = {0963-7214, 1467-8721},
	shorttitle = {Statistical {Language} {Learning}},
	url = {http://journals.sagepub.com/doi/10.1111/1467-8721.01243},
	doi = {10.1111/1467-8721.01243},
	language = {en},
	number = {4},
	urldate = {2020-05-23},
	journal = {Current Directions in Psychological Science},
	author = {Saffran, Jenny R.},
	month = aug,
	year = {2003},
	pages = {110--114},
}

@article{fetsch_importance_2016,
	title = {The importance of task design and behavioral control for understanding the neural basis of cognitive functions},
	volume = {37},
	issn = {09594388},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0959438815001804},
	doi = {10.1016/j.conb.2015.12.002},
	language = {en},
	urldate = {2020-05-23},
	journal = {Current Opinion in Neurobiology},
	author = {Fetsch, Christopher R},
	month = apr,
	year = {2016},
	pages = {16--22},
}

@article{bechtel_dynamic_2010,
	title = {Dynamic mechanistic explanation: computational modeling of circadian rhythms as an exemplar for cognitive science},
	volume = {41},
	issn = {00393681},
	shorttitle = {Dynamic mechanistic explanation},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S003936811000035X},
	doi = {10.1016/j.shpsa.2010.07.003},
	language = {en},
	number = {3},
	urldate = {2020-05-23},
	journal = {Studies in History and Philosophy of Science Part A},
	author = {Bechtel, William and Abrahamsen, Adele},
	month = sep,
	year = {2010},
	pages = {321--333},
}

@article{pallier_cortical_2011,
	title = {Cortical representation of the constituent structure of sentences},
	volume = {108},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/cgi/doi/10.1073/pnas.1018711108},
	doi = {10.1073/pnas.1018711108},
	language = {en},
	number = {6},
	urldate = {2020-05-23},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Pallier, C. and Devauchelle, A.-D. and Dehaene, S.},
	month = feb,
	year = {2011},
	pages = {2522--2527},
}

@article{huettig_using_2011,
	title = {Using the visual world paradigm to study language processing: {A} review and critical evaluation},
	volume = {137},
	issn = {00016918},
	shorttitle = {Using the visual world paradigm to study language processing},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0001691810002180},
	doi = {10.1016/j.actpsy.2010.11.003},
	language = {en},
	number = {2},
	urldate = {2020-05-23},
	journal = {Acta Psychologica},
	author = {Huettig, Falk and Rommers, Joost and Meyer, Antje S.},
	month = jun,
	year = {2011},
	keywords = {eye-tracking},
	pages = {151--171},
}

@article{rayner_eye_1998,
	title = {Eye movements in reading and information processing: 20 years of research.},
	volume = {124},
	issn = {1939-1455, 0033-2909},
	shorttitle = {Eye movements in reading and information processing},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-2909.124.3.372},
	doi = {10.1037/0033-2909.124.3.372},
	language = {en},
	number = {3},
	urldate = {2020-05-23},
	journal = {Psychological Bulletin},
	author = {Rayner, Keith},
	year = {1998},
	keywords = {eye-tracking, reading comprehension},
	pages = {372--422},
}

@article{mars_model-based_2012,
	title = {Model-based analyses: {Promises}, pitfalls, and example applications to the study of cognitive control},
	volume = {65},
	issn = {1747-0218, 1747-0226},
	shorttitle = {Model-based analyses},
	url = {http://journals.sagepub.com/doi/10.1080/17470211003668272},
	doi = {10.1080/17470211003668272},
	language = {en},
	number = {2},
	urldate = {2020-05-23},
	journal = {Quarterly Journal of Experimental Psychology},
	author = {Mars, Rogier B. and Shea, Nicholas J. and Kolling, Nils and Rushworth, Matthew F. S.},
	month = feb,
	year = {2012},
	pages = {252--267},
}

@article{norris_merging_2000,
	title = {Merging information in speech recognition: {Feedback} is never necessary},
	volume = {23},
	issn = {0140-525X, 1469-1825},
	shorttitle = {Merging information in speech recognition},
	url = {https://www.cambridge.org/core/product/identifier/S0140525X00003241/type/journal_article},
	doi = {10.1017/S0140525X00003241},
	language = {en},
	number = {3},
	urldate = {2020-05-23},
	journal = {Behavioral and Brain Sciences},
	author = {Norris, Dennis and McQueen, James M. and Cutler, Anne},
	month = jun,
	year = {2000},
	keywords = {speech perception},
	pages = {299--325},
}

@article{kaan_brain_2002,
	title = {The brain circuitry of syntactic comprehension},
	volume = {6},
	issn = {13646613},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661302019472},
	doi = {10.1016/S1364-6613(02)01947-2},
	language = {en},
	number = {8},
	urldate = {2020-05-23},
	journal = {Trends in Cognitive Sciences},
	author = {Kaan, Edith and Swaab, Tamara Y.},
	month = aug,
	year = {2002},
	keywords = {syntax},
	pages = {350--356},
}

@article{tallon-baudry_oscillatory_1999,
	title = {Oscillatory gamma activity in humans and its role in object representation},
	volume = {3},
	issn = {13646613},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661399012991},
	doi = {10.1016/S1364-6613(99)01299-1},
	number = {4},
	urldate = {2020-05-22},
	journal = {Trends in Cognitive Sciences},
	author = {Tallon-Baudry, C},
	month = apr,
	year = {1999},
	keywords = {ner},
	pages = {151--162},
}

@article{rayner_language_2009,
	title = {Language processing in reading and speech perception is fast and incremental: {Implications} for event-related potential research},
	volume = {80},
	issn = {03010511},
	shorttitle = {Language processing in reading and speech perception is fast and incremental},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0301051108001245},
	doi = {10.1016/j.biopsycho.2008.05.002},
	language = {en},
	number = {1},
	urldate = {2020-05-22},
	journal = {Biological Psychology},
	author = {Rayner, Keith and Clifton, Charles},
	month = jan,
	year = {2009},
	keywords = {language comprehension},
	pages = {4--9},
}

@article{stokes_activity-silent_2015,
	title = {‘{Activity}-silent’ working memory in prefrontal cortex: a dynamic coding framework},
	volume = {19},
	issn = {1364-6613},
	shorttitle = {‘{Activity}-silent’ working memory in prefrontal cortex},
	url = {http://www.sciencedirect.com/science/article/pii/S1364661315001023},
	doi = {10.1016/j.tics.2015.05.004},
	abstract = {Working memory (WM) provides the functional backbone to high-level cognition. Maintenance in WM is often assumed to depend on the stationary persistence of neural activity patterns that represent memory content. However, accumulating evidence suggests that persistent delay activity does not always accompany WM maintenance but instead seems to wax and wane as a function of the current task relevance of memoranda. Furthermore, new methods for measuring and analysing population-level patterns show that activity states are highly dynamic. At first glance, these dynamics seem at odds with the very nature of WM. How can we keep a stable thought in mind while brain activity is constantly changing? This review considers how neural dynamics might be functionally important for WM maintenance.},
	language = {en},
	number = {7},
	urldate = {2020-05-21},
	journal = {Trends in Cognitive Sciences},
	author = {Stokes, Mark G.},
	month = jul,
	year = {2015},
	pages = {394--405},
}

@book{clark_surfing_2016,
	address = {Oxford, New York},
	title = {Surfing uncertainty: {Prediction}, action, and the embodied mind},
	isbn = {978-0-19-021701-3},
	shorttitle = {Surfing {Uncertainty}},
	abstract = {In this ground-breaking work, philosopher and cognitive scientist Andy Clark turns a common view of the human mind upside down. In stark opposition to familiar models of human cognition, Surfing Uncertainty explores exciting new theories in neuroscience, psychology, and artificial intelligence that reveal minds like ours to be prediction machines—devices that have evolved to anticipate the incoming streams of sensory stimulation before they arrive. This keeps minds like ours a few steps ahead of the game, poised to respond rapidly and apparently effortlessly to threats and opportunities as (and sometimes even before) they arise. Creatures thus equipped are more than simple response machines. They are knowing agents deep in the business of understanding their worlds. Such agents cope with changing and uncertain worlds by combining sensory evidence with informed prediction. Remarkably, the learning that makes neural prediction possible can itself be accomplished by the ceaseless effort to make better and better predictions. A single fundamental trick (the trick of trying to predict your own sensory inputs) thus enables learning, empowers moment-by-moment perception, and installs a rich understanding of the surrounding world. Action itself now appears in a new and revealing light. For action is not so much a 'response to an input' as a neat and efficient way of selecting the next 'input'. As mobile embodied agents we are forever intervening, actively bringing about the very streams of sensory information that our brains are simultaneously trying to predict. This binds perception and action in a delicate dance, a virtuous circle in which neural circuits animate, and are animated by, the movements of our own bodies. Some of our actions, in turn, structure the physical, social, and technological worlds around us. This moves the goalposts by altering the very things we need to engage and predict. Surfing Uncertainty brings work on the predictive brain into full and satisfying contact with work on the embodied and culturally situated mind. What emerges is a bold new vision of what brains do that places circular causal flows and the active structuring of the environment, center-stage. In place of cognitive couch potatoes idly awaiting the next sensory inputs, Clark's journey reveals us as proactive predictavores, skilfully surfing the waves of sensory stimulation.},
	publisher = {Oxford University Press},
	author = {Clark, Andy},
	month = jan,
	year = {2016},
}

@book{rosen_anticipatory_2012,
	address = {New York, NY},
	series = {{IFSR} {International} {Series} on {Systems} {Science} and {Engineering}},
	title = {Anticipatory systems},
	volume = {1},
	isbn = {978-1-4614-1268-7 978-1-4614-1269-4},
	url = {http://link.springer.com/10.1007/978-1-4614-1269-4},
	urldate = {2020-05-21},
	publisher = {Springer New York},
	author = {Rosen, Robert},
	year = {2012},
	doi = {10.1007/978-1-4614-1269-4},
}

@book{hohwy_predictive_2013,
	address = {Oxford, New York},
	title = {The predictive mind},
	isbn = {978-0-19-968673-5},
	abstract = {A new theory is taking hold in neuroscience. It is the theory that the brain is essentially a hypothesis-testing mechanism, one that attempts to minimise the error of its predictions about the sensory input it receives from the world. It is an attractive theory because powerful theoretical arguments support it, and yet it is at heart stunningly simple. Jakob Hohwy explains and explores this theory from the perspective of cognitive science and philosophy. The key argument throughout The Predictive Mind is that the mechanism explains the rich, deep, and multifaceted character of our conscious perception. It also gives a unified account of how perception is sculpted by attention, and how it depends on action. The mind is revealed as having a fragile and indirect relation to the world. Though we are deeply in tune with the world we are also strangely distanced from it.The first part of the book sets out how the theory enables rich, layered perception. The theory's probabilistic and statistical foundations are explained using examples from empirical research and analogies to different forms of inference. The second part uses the simple mechanism in an explanation of problematic cases of how we manage to represent, and sometimes misrepresent, the world in health as well as in mental illness. The third part looks into the mind, and shows how the theory accounts for attention, conscious unity, introspection, self and the privacy of our mental world.},
	publisher = {Oxford University Press},
	author = {Hohwy, Jakob},
	month = nov,
	year = {2013},
}

@article{hamalainen_meg_1993,
	title = {Magnetoencephalography—theory, instrumentation, and applications to noninvasive studies of the working human brain},
	volume = {65},
	issn = {0034-6861, 1539-0756},
	url = {https://link.aps.org/doi/10.1103/RevModPhys.65.413},
	doi = {10.1103/RevModPhys.65.413},
	language = {en},
	number = {2},
	urldate = {2020-05-02},
	journal = {Reviews of Modern Physics},
	author = {Hämäläinen, Matti and Hari, Riitta and Ilmoniemi, Risto J. and Knuutila, Jukka and Lounasmaa, Olli V.},
	month = apr,
	year = {1993},
	keywords = {MEG, methods},
	pages = {413--497},
}

@article{baillet_magnetoencephalography_2017,
	title = {Magnetoencephalography for brain electrophysiology and imaging},
	volume = {20},
	copyright = {2017 Nature Publishing Group, a division of Macmillan Publishers Limited. All Rights Reserved.},
	issn = {1546-1726},
	url = {http://www.nature.com/articles/nn.4504},
	doi = {10.1038/nn.4504},
	abstract = {Magnetoencephalography (MEG) tracks the millisecond electrical activity of the brain noninvasively. This review emphasizes MEG's unique assets, especially in terms of imaging and resolving the mechanisms underlying the apparent complexity of polyrhythmic brain dynamics. It also identifies practical challenges and clarifies misconceptions about the technique.},
	language = {en},
	number = {3},
	urldate = {2020-05-19},
	journal = {Nature Neuroscience},
	author = {Baillet, Sylvain},
	month = mar,
	year = {2017},
	note = {Number: 3
Publisher: Nature Publishing Group},
	keywords = {MEG, methods, thesis.introduction, review},
	pages = {327--339},
}

@article{baillet_electromagnetic_2001,
	title = {Electromagnetic brain mapping},
	volume = {18},
	number = {6},
	journal = {IEEE Signal Processing Magazine},
	author = {Baillet, S. and Mosher, J. C. and Leahy, R. M.},
	year = {2001},
	keywords = {MEG, methods},
	pages = {14--30},
}

@book{pierce_introduction_1980,
	address = {New York},
	edition = {2nd, rev. ed},
	title = {An introduction to information theory: symbols, signals \& noise},
	isbn = {978-0-486-24061-9},
	shorttitle = {An introduction to information theory},
	publisher = {Dover Publications},
	author = {Pierce, John R.},
	year = {1980},
	keywords = {entropy, information theory, thesis.introduction, signal processing, Shannon},
}

@book{manning_foundations_1999,
	address = {Cambridge, Mass},
	title = {Foundations of statistical natural language processing},
	isbn = {978-0-262-13360-9},
	publisher = {MIT Press},
	author = {Manning, Christopher D. and Schütze, Hinrich},
	year = {1999},
	keywords = {NLP, thesis.introduction, textbook, language modelling},
}

@incollection{firth_synopsis_1957,
	series = {Special volume of the {Philological} {Society}},
	title = {A synopsis of linguistic theory, 1930-1955},
	booktitle = {Studies in linguistic analysis},
	publisher = {Oxford: Blackwell},
	editor = {Firth, J. R.},
	year = {1957},
	note = {tex.publisher: Basil Blackwell},
	pages = {1--32},
}

@article{engel_dynamic_2001,
	title = {Dynamic predictions: {Oscillations} and synchrony in top–down processing},
	volume = {2},
	issn = {1471-003X, 1471-0048},
	shorttitle = {Dynamic predictions},
	url = {http://www.nature.com/articles/35094565},
	doi = {10.1038/35094565},
	language = {en},
	number = {10},
	urldate = {2020-05-17},
	journal = {Nature Reviews Neuroscience},
	author = {Engel, Andreas K. and Fries, Pascal and Singer, Wolf},
	month = oct,
	year = {2001},
	keywords = {neural oscillations, oscillations, predictive processing},
	pages = {704--716},
}

@article{bauer_varieties_1979,
	title = {Varieties of the locked-in syndrome},
	volume = {221},
	issn = {0340-5354, 1432-1459},
	url = {http://link.springer.com/10.1007/BF00313105},
	doi = {10.1007/BF00313105},
	language = {en},
	number = {2},
	urldate = {2020-05-03},
	journal = {Journal of Neurology},
	author = {Bauer, G. and Gerstenbrand, F. and Rumpl, E.},
	month = aug,
	year = {1979},
	keywords = {LIS},
	pages = {77--91},
}

@article{churchland_perspectives_1988,
	title = {Perspectives on cognitive neuroscience},
	volume = {242},
	issn = {0036-8075, 1095-9203},
	url = {https://www.sciencemag.org/lookup/doi/10.1126/science.3055294},
	doi = {10.1126/science.3055294},
	language = {en},
	number = {4879},
	urldate = {2020-05-02},
	journal = {Science},
	author = {Churchland, P. and Sejnowski, T.},
	month = nov,
	year = {1988},
	keywords = {thesis.introduction, levels, review},
	pages = {741--745},
}

@article{tshitoyan_unsupervised_2019,
	title = {Unsupervised word embeddings capture latent knowledge from materials science literature},
	volume = {571},
	copyright = {2019 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {1476-4687},
	url = {http://www.nature.com/articles/s41586-019-1335-8},
	doi = {10.1038/s41586-019-1335-8},
	abstract = {Natural language processing algorithms applied to three million materials science abstracts uncover relationships between words, material compositions and properties, and predict potential new thermoelectric materials.},
	language = {en},
	number = {7763},
	urldate = {2020-05-02},
	journal = {Nature},
	author = {Tshitoyan, Vahe and Dagdelen, John and Weston, Leigh and Dunn, Alexander and Rong, Ziqin and Kononova, Olga and Persson, Kristin A. and Ceder, Gerbrand and Jain, Anubhav},
	month = jul,
	year = {2019},
	note = {Number: 7763
Publisher: Nature Publishing Group},
	keywords = {word2vec, word embedding, chemistry, materials science, thermoelectric},
	pages = {95--98},
}

@article{vanDerMaaten2008,
	title = {Visualizing data using t-{SNE}},
	volume = {9},
	url = {http://www.jmlr.org/papers/v9/vandermaaten08a.html},
	journal = {Journal of Machine Learning Research},
	author = {van der Maaten, Laurens and Hinton, Geoffrey},
	year = {2008},
	note = {tex.biburl: https://www.bibsonomy.org/bibtex/28b9aebb404ad4a4c6a436ea413550b30/lopuszₖdd
tex.interhash: 370ba8b9e1909b61880a6f47c93bcd49
tex.intrahash: 8b9aebb404ad4a4c6a436ea413550b30},
	keywords = {methods, readme, machine learning, embeddings, t-SNE},
	pages = {2579--2605},
}

@article{burgelman_open_2019,
	title = {Open science, open data, and open scholarship: european policies to make science fit for the twenty-first century},
	volume = {2},
	issn = {2624-909X},
	shorttitle = {Open {Science}, {Open} {Data}, and {Open} {Scholarship}},
	url = {https://www.frontiersin.org/article/10.3389/fdata.2019.00043/full},
	doi = {10.3389/fdata.2019.00043},
	urldate = {2019-12-10},
	journal = {Frontiers in Big Data},
	author = {Burgelman, J.-C. and Pascu, C. and Szkuta, K. and Von Schomberg, R. and Karalopoulos, A. and Repanas, K. and Schouppe, M.},
	month = dec,
	year = {2019},
	pages = {43},
}

@article{pernet_eeg-bids_2019,
	title = {{EEG}-{BIDS}, an extension to the brain imaging data structure for electroencephalography},
	volume = {6},
	copyright = {2019 The Author(s)},
	issn = {2052-4463},
	url = {http://www.nature.com/articles/s41597-019-0104-8},
	doi = {10.1038/s41597-019-0104-8},
	abstract = {The Brain Imaging Data Structure (BIDS) project is a rapidly evolving effort in the human brain imaging research community to create standards allowing researchers to readily organize and share study data within and between laboratories. Here we present an extension to BIDS for electroencephalography (EEG) data, EEG-BIDS, along with tools and references to a series of public EEG datasets organized using this new standard.},
	language = {en},
	number = {1},
	urldate = {2020-05-01},
	journal = {Scientific Data},
	author = {Pernet, Cyril R. and Appelhoff, Stefan and Gorgolewski, Krzysztof J. and Flandin, Guillaume and Phillips, Christophe and Delorme, Arnaud and Oostenveld, Robert},
	month = jun,
	year = {2019},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {BIDS, open science, thesis.introduction, thesis, open data},
	pages = {1--5},
}

@article{kingma_adam_2017,
	title = {Adam: {A} method for stochastic optimization},
	shorttitle = {Adam},
	url = {http://arxiv.org/abs/1412.6980},
	abstract = {We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm.},
	urldate = {2020-04-21},
	journal = {arXiv:1412.6980 [cs]},
	author = {Kingma, Diederik P. and Ba, Jimmy},
	month = jan,
	year = {2017},
	note = {arXiv: 1412.6980},
	keywords = {backprop, machine learning, thesis, Adam, thesis.rnn},
}

@article{gorgolewski_brain_2016,
	title = {The brain imaging data structure, a format for organizing and describing outputs of neuroimaging experiments},
	volume = {3},
	copyright = {2016 The Author(s)},
	issn = {2052-4463},
	url = {http://www.nature.com/articles/sdata201644},
	doi = {10.1038/sdata.2016.44},
	abstract = {The development of magnetic resonance imaging (MRI) techniques has defined modern neuroimaging. Since its inception, tens of thousands of studies using techniques such as functional MRI and diffusion weighted imaging have allowed for the non-invasive study of the brain. Despite the fact that MRI is routinely used to obtain data for neuroscience research, there has been no widely adopted standard for organizing and describing the data collected in an imaging experiment. This renders sharing and reusing data (within or between labs) difficult if not impossible and unnecessarily complicates the application of automatic pipelines and quality assurance protocols. To solve this problem, we have developed the Brain Imaging Data Structure (BIDS), a standard for organizing and describing MRI datasets. The BIDS standard uses file formats compatible with existing software, unifies the majority of practices already common in the field, and captures the metadata necessary for most common data processing operations.},
	language = {en},
	number = {1},
	urldate = {2020-05-01},
	journal = {Scientific Data},
	author = {Gorgolewski, Krzysztof J. and Auer, Tibor and Calhoun, Vince D. and Craddock, R. Cameron and Das, Samir and Duff, Eugene P. and Flandin, Guillaume and Ghosh, Satrajit S. and Glatard, Tristan and Halchenko, Yaroslav O. and Handwerker, Daniel A. and Hanke, Michael and Keator, David and Li, Xiangrui and Michael, Zachary and Maumet, Camille and Nichols, B. Nolan and Nichols, Thomas E. and Pellman, John and Poline, Jean-Baptiste and Rokem, Ariel and Schaefer, Gunnar and Sochat, Vanessa and Triplett, William and Turner, Jessica A. and Varoquaux, Gaël and Poldrack, Russell A.},
	month = jun,
	year = {2016},
	note = {Number: 1
Publisher: Nature Publishing Group},
	keywords = {open science, open data},
	pages = {1--9},
}

@article{willems_narratives_2020,
	title = {Narratives for neuroscience},
	volume = {43},
	issn = {0166-2236},
	url = {http://www.sciencedirect.com/science/article/pii/S0166223620300497},
	doi = {10.1016/j.tins.2020.03.003},
	abstract = {People organize and convey their thoughts according to narratives. However, neuroscientists are often reluctant to incorporate narrative stimuli into their experiments. We argue that narratives deserve wider adoption in human neuroscience because they tap into the brain’s native machinery for representing the world and provide rich variability for testing hypotheses.},
	language = {en},
	number = {5},
	urldate = {2020-05-01},
	journal = {Trends in Neurosciences},
	author = {Willems, Roel M. and Nastase, Samuel A. and Milivojevic, Branka},
	month = may,
	year = {2020},
	keywords = {open science, narrative, open data},
	pages = {271--273},
}

@article{martin_decoding_2014,
	title = {Decoding spectrotemporal features of overt and covert speech from the human cortex},
	volume = {7},
	issn = {1662-6443},
	url = {https://www.frontiersin.org/articles/10.3389/fneng.2014.00014/full},
	doi = {10.3389/fneng.2014.00014},
	abstract = {Auditory perception and auditory imagery have been shown to activate overlapping brain regions. We hypothesized that these phenomena also share a common underlying neural representation. To assess this, we used intracranial recordings from epileptic patients performing an out loud or a silent reading task. In these tasks, short stories scrolled across a video screen in two conditions: subjects read the same stories both aloud (overt) and silently (covert). In a control condition the subject remained in a resting state. We first built a high gamma (70-150 Hz) neural decoding model to reconstruct spectrotemporal auditory features of self-generated overt speech. We then evaluated whether this same model could reconstruct auditory speech features in the covert speech condition. Two speech models were tested: a spectrogram and a modulation-based feature space. For the overt condition, reconstruction accuracy was evaluated as the correlation between original and predicted speech features, and was significant in each subject. For the covert speech condition, dynamic time warping was first used to realign the covert speech reconstruction with the corresponding original speech from the overt condition. Covert reconstruction accuracy was then compared to the accuracy obtained from reconstructions in the baseline control condition. Across the group of subjects, reconstruction accuracy for the covert condition was significantly better than for the control condition. Electrodes in the superior temporal gyrus, pre- and post-central gyrus were the strongest predictors of decoding accuracy. These results provide evidence that auditory representations of covert speech can be reconstructed from models that are built from an overt speech data set, supporting a partially shared neural substrate.},
	language = {English},
	urldate = {2020-05-01},
	journal = {Frontiers in Neuroengineering},
	author = {Martin, Stéphanie and Brunner, Peter and Holdgraf, Chris and Heinze, Hans-Jochen and Crone, Nathan E. and Rieger, Jochem and Schalk, Gerwin and Knight, Robert T. and Pasley, Brian N.},
	year = {2014},
	note = {Publisher: Frontiers},
	keywords = {decoding models, ECoG, covert speech},
}

@incollection{vaughan_brain-computer_2020,
	title = {Brain-computer interfaces for people with amyotrophic lateral sclerosis},
	volume = {168},
	isbn = {978-0-444-63934-9},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B9780444639349000044},
	language = {en},
	urldate = {2020-05-01},
	booktitle = {Handbook of {Clinical} {Neurology}},
	publisher = {Elsevier},
	author = {Vaughan, Theresa M.},
	year = {2020},
	doi = {10.1016/B978-0-444-63934-9.00004-4},
	keywords = {review, BCI, ALS},
	pages = {33--38},
}

@incollection{vansteensel_brain-computer_2020,
	title = {Brain-computer interfaces for communication},
	volume = {168},
	isbn = {978-0-444-63934-9},
	url = {https://linkinghub.elsevier.com/retrieve/pii/B978044463934900007X},
	language = {en},
	urldate = {2020-05-01},
	booktitle = {Handbook of {Clinical} {Neurology}},
	publisher = {Elsevier},
	author = {Vansteensel, Mariska J. and Jarosiewicz, Beata},
	year = {2020},
	doi = {10.1016/B978-0-444-63934-9.00007-X},
	keywords = {BCI, locked-in syndrome},
	pages = {67--85},
}

@article{martin_language_2016,
	title = {Language processing as cue integration: grounding the psychology of language in perception and neurophysiology},
	volume = {7},
	issn = {1664-1078},
	shorttitle = {Language {Processing} as {Cue} {Integration}},
	url = {http://journal.frontiersin.org/Article/10.3389/fpsyg.2016.00120/abstract},
	doi = {10.3389/fpsyg.2016.00120},
	urldate = {2020-04-26},
	journal = {Frontiers in Psychology},
	author = {Martin, Andrea E.},
	month = feb,
	year = {2016},
	keywords = {language comprehension, perception, thesis.lmrev},
}

@article{linzen_syntactic_2020,
	title = {Syntactic structure from deep learning},
	url = {http://arxiv.org/abs/2004.10827},
	doi = {10.1146/annurev-linguistics-032020-051035},
	abstract = {Modern deep neural networks achieve impressive performance in engineering applications that require extensive linguistic skills, such as machine translation. This success has sparked interest in probing whether these models are inducing human-like grammatical knowledge from the raw data they are exposed to, and, consequently, whether they can shed new light on long-standing debates concerning the innate structure necessary for language acquisition. In this article, we survey representative studies of the syntactic abilities of deep networks, and discuss the broader implications that this work has for theoretical linguistics.},
	urldate = {2020-04-26},
	journal = {arXiv:2004.10827 [cs]},
	author = {Linzen, Tal and Baroni, Marco},
	month = apr,
	year = {2020},
	note = {arXiv: 2004.10827},
	keywords = {deep learning, syntax, NLP, RNN},
}

@article{cogan_translating_2020,
	title = {Translating the brain},
	volume = {23},
	issn = {1097-6256, 1546-1726},
	url = {http://www.nature.com/articles/s41593-020-0616-8},
	doi = {10.1038/s41593-020-0616-8},
	language = {en},
	number = {4},
	urldate = {2020-04-25},
	journal = {Nature Neuroscience},
	author = {Cogan, Gregory B.},
	month = apr,
	year = {2020},
	keywords = {RNN, decoding, thesis, ECoG, thesis.rnn},
	pages = {471--472},
}

@article{herff_brain--text_2015,
	title = {Brain-to-text: decoding spoken phrases from phone representations in the brain},
	volume = {9},
	issn = {1662-453X},
	shorttitle = {Brain-to-text},
	url = {http://journal.frontiersin.org/Article/10.3389/fnins.2015.00217/abstract},
	doi = {10.3389/fnins.2015.00217},
	urldate = {2020-04-25},
	journal = {Frontiers in Neuroscience},
	author = {Herff, Christian and Heger, Dominic and de Pesters, Adriana and Telaar, Dominic and Brunner, Peter and Schalk, Gerwin and Schultz, Tanja},
	month = jun,
	year = {2015},
	keywords = {decoding, speech, ECoG, BCI, thesis.rnn},
}

@article{kietzmann_recurrence_2019,
	title = {Recurrence is required to capture the representational dynamics of the human visual system},
	volume = {116},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/lookup/doi/10.1073/pnas.1905544116},
	doi = {10.1073/pnas.1905544116},
	language = {en},
	number = {43},
	urldate = {2020-04-23},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Kietzmann, Tim C. and Spoerer, Courtney J. and Sörensen, Lynn K. A. and Cichy, Radoslaw M. and Hauk, Olaf and Kriegeskorte, Nikolaus},
	month = oct,
	year = {2019},
	keywords = {deep learning, thesis, RNNs, thesis.rnn, visual perception},
	pages = {21854--21863},
}

@article{chien_constructing_2020,
	title = {Constructing and forgetting temporal context in the human cerebral cortex},
	issn = {08966273},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627320301367},
	doi = {10.1016/j.neuron.2020.02.013},
	language = {en},
	urldate = {2020-04-23},
	journal = {Neuron},
	author = {Chien, Hsiang-Yun Sherry and Honey, Christopher J.},
	month = mar,
	year = {2020},
	keywords = {thesis, thesis.rnn, visual perception},
}

@article{mak_mental_2019-1,
	title = {Mental simulation during literary reading: {Individual} differences revealed with eye-tracking},
	volume = {34},
	issn = {2327-3798},
	shorttitle = {Mental simulation during literary reading},
	url = {https://doi.org/10.1080/23273798.2018.1552007},
	doi = {10.1080/23273798.2018.1552007},
	abstract = {People engage in simulation when reading literary narratives. In this study, we tried to pinpoint how different kinds of simulation (perceptual and motor simulation, mentalising) affect reading behaviour. Eye-tracking (gaze durations, regression probability) and questionnaire data were collected from 102 participants, who read three literary short stories. In a pre-test, 90 additional participants indicated which parts of the stories were high in one of the three kinds of simulation-eliciting content. The results show that motor simulation reduces gaze duration (faster reading), whereas perceptual simulation and mentalising increase gaze duration (slower reading). Individual differences in the effect of simulation on gaze duration were found, which were related to individual differences in aspects of story world absorption and story appreciation. These findings suggest fundamental differences between different kinds of simulation and confirm the role of simulation in absorption and appreciation.},
	number = {4},
	urldate = {2020-02-04},
	journal = {Language, Cognition and Neuroscience},
	author = {Mak, Marloes and Willems, Roel M.},
	month = apr,
	year = {2019},
	pages = {511--535},
}

@article{shanechi_brainmachine_2019,
	title = {Brain–machine interfaces from motor to mood},
	volume = {22},
	issn = {1097-6256, 1546-1726},
	url = {http://www.nature.com/articles/s41593-019-0488-y},
	doi = {10.1038/s41593-019-0488-y},
	language = {en},
	number = {10},
	urldate = {2020-04-22},
	journal = {Nature Neuroscience},
	author = {Shanechi, Maryam M.},
	month = oct,
	year = {2019},
	keywords = {motor cortex, decoding, BCI},
	pages = {1554--1564},
}

@article{van_heuven_subtlex-uk_2014,
	title = {Subtlex-{UK}: {A} new and improved word frequency database for british english},
	volume = {67},
	issn = {1747-0218, 1747-0226},
	shorttitle = {Subtlex-{UK}},
	url = {http://journals.sagepub.com/doi/10.1080/17470218.2013.850521},
	doi = {10.1080/17470218.2013.850521},
	language = {en},
	number = {6},
	urldate = {2020-04-21},
	journal = {Quarterly Journal of Experimental Psychology},
	author = {van Heuven, Walter J. B. and Mandera, Pawel and Keuleers, Emmanuel and Brysbaert, Marc},
	month = jun,
	year = {2014},
	keywords = {project.lstmMEG, thesis, SUBTLEX, lexical frequency},
	pages = {1176--1190},
}

@misc{speer_luminosoinsightwordfreq_2018,
	title = {{LuminosoInsight}/wordfreq: v2.2},
	copyright = {Creative Commons Attribution Share Alike 4.0 International, Open Access},
	shorttitle = {{LuminosoInsight}/wordfreq},
	url = {https://zenodo.org/record/1443582},
	abstract = {Access a database of word frequencies, in various natural languages.},
	urldate = {2020-04-21},
	publisher = {Zenodo},
	author = {Speer, Robyn and Chin, Joshua and Lin, Andrew and Jewett, Sara and Nathan, Lance},
	month = oct,
	year = {2018},
	doi = {10.5281/ZENODO.1443582},
	keywords = {SUBTLEX, lexical frequency, corpus linguistics},
}

@article{lebedev_brainmachine_2006,
	title = {Brain–machine interfaces: past, present and future},
	volume = {29},
	issn = {01662236},
	shorttitle = {Brain–machine interfaces},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0166223606001470},
	doi = {10.1016/j.tins.2006.07.004},
	language = {en},
	number = {9},
	urldate = {2020-04-20},
	journal = {Trends in Neurosciences},
	author = {Lebedev, Mikhail A. and Nicolelis, Miguel A.L.},
	month = sep,
	year = {2006},
	keywords = {decoding, ECoG, review, BCI},
	pages = {536--546},
}

@article{branco_decoding_2017,
	title = {Decoding hand gestures from primary somatosensory cortex using high-density {ECoG}},
	volume = {147},
	issn = {10538119},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1053811916306954},
	doi = {10.1016/j.neuroimage.2016.12.004},
	language = {en},
	urldate = {2020-04-20},
	journal = {NeuroImage},
	author = {Branco, Mariana P. and Freudenburg, Zachary V. and Aarnoutse, Erik J. and Bleichner, Martin G. and Vansteensel, Mariska J. and Ramsey, Nick F.},
	month = feb,
	year = {2017},
	keywords = {decoding, ECoG, BCI, gesture},
	pages = {130--142},
}

@article{gerven_braincomputer_2009,
	title = {The brain–computer interface cycle},
	volume = {6},
	issn = {1741-2560, 1741-2552},
	url = {https://iopscience.iop.org/article/10.1088/1741-2560/6/4/041001},
	doi = {10.1088/1741-2560/6/4/041001},
	number = {4},
	urldate = {2020-04-17},
	journal = {Journal of Neural Engineering},
	author = {Gerven, Marcel van and Farquhar, Jason and Schaefer, Rebecca and Vlek, Rutger and Geuze, Jeroen and Nijholt, Anton and Ramsey, Nick and Haselager, Pim and Vuurpijl, Louis and Gielen, Stan and Desain, Peter},
	month = aug,
	year = {2009},
	pages = {041001},
}

@article{lau_grammaticality_2017,
	title = {Grammaticality, acceptability, and probability: {A} probabilistic view of linguistic knowledge},
	volume = {41},
	issn = {03640213},
	shorttitle = {Grammaticality, {Acceptability}, and {Probability}},
	url = {http://doi.wiley.com/10.1111/cogs.12414},
	doi = {10.1111/cogs.12414},
	language = {en},
	number = {5},
	urldate = {2020-04-15},
	journal = {Cognitive Science},
	author = {Lau, Jey Han and Clark, Alexander and Lappin, Shalom},
	month = jul,
	year = {2017},
	keywords = {probabilistic models of cognition, thesis.lmrev},
	pages = {1202--1241},
}

@inproceedings{Linzen2014,
	address = {Baltimore},
	title = {Investigating the role of entropy in sentence processing},
	url = {http://www.aclweb.org/anthology/W/W14/W14-2002},
	abstract = {We outline four ways in which uncertainty might affect comprehension difficulty in human sentence processing. These four hypotheses motivate a self-paced reading experiment, in which we used verb sub-categorization distributions to manipulate the uncertainty over the next step in the syntactic derivation (single step entropy) and the surprisal of the verb's comple-ment. We additionally estimate word-by-word surprisal and total entropy over parses of the sentence using a probabilistic context-free grammar (PCFG). Surprisal and total entropy, but not single step en-tropy, were significant predictors of read-ing times in different parts of the sen-tence. This suggests that a complete model of sentence processing should incorporate both entropy and surprisal.},
	booktitle = {Proceedings of the fifth workshop on cognitive modeling and computational linguistics},
	publisher = {Association for Computational Linguistics},
	author = {Linzen, Tal and Jaeger, Florian},
	year = {2014},
	pages = {10--18},
}

@inproceedings{wu_complexity_2010,
	address = {Uppsala, Sweden},
	title = {Complexity metrics in an incremental right-corner parser},
	url = {https://www.aclweb.org/anthology/P10-1121},
	urldate = {2020-04-14},
	booktitle = {Proceedings of the 48th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Wu, Stephen and Bachrach, Asaf and Cardenas, Carlos and Schuler, William},
	month = jul,
	year = {2010},
	keywords = {computational linguistics, thesis, thesis.lmrev, parsing},
	pages = {1189--1198},
}

@article{hale_uncertainty_2006,
	title = {Uncertainty about the rest of the sentence},
	volume = {30},
	issn = {03640213},
	url = {http://doi.wiley.com/10.1207/s15516709cog0000_64},
	doi = {10.1207/s15516709cog0000_64},
	language = {en},
	number = {4},
	urldate = {2020-04-14},
	journal = {Cognitive Science},
	author = {Hale, John},
	month = jul,
	year = {2006},
	keywords = {entropy, sentence comprehension, computational linguistics, thesis, parser, thesis.lmrev},
	pages = {643--672},
}

@article{frank_uncertainty_2013,
	title = {Uncertainty reduction as a measure of cognitive load in sentence comprehension},
	volume = {5},
	issn = {17568757},
	url = {http://doi.wiley.com/10.1111/tops.12025},
	doi = {10.1111/tops.12025},
	language = {en},
	number = {3},
	urldate = {2020-04-14},
	journal = {Topics in Cognitive Science},
	author = {Frank, Stefan L.},
	month = jul,
	year = {2013},
	keywords = {computational linguistics, entropy reduction, thesis, thesis.lmrev, complexity metric},
	pages = {475--494},
}

@article{demberg_data_2008,
	title = {Data from eye-tracking corpora as evidence for theories of syntactic processing complexity},
	volume = {109},
	issn = {00100277},
	url = {http://linkinghub.elsevier.com/retrieve/pii/S0010027708001741},
	doi = {10.1016/j.cognition.2008.07.008},
	language = {en},
	number = {2},
	urldate = {2018-12-12},
	journal = {Cognition},
	author = {Demberg, Vera and Keller, Frank},
	month = nov,
	year = {2008},
	keywords = {project.streams, readme, eye-tracking, surprisal, thesis, sentence reading},
	pages = {193--210},
}

@inproceedings{roark_deriving_2009,
	address = {Singapore},
	title = {Deriving lexical and syntactic expectation-based measures for psycholinguistic modeling via incremental top-down parsing},
	volume = {1},
	isbn = {978-1-932432-59-6},
	url = {http://portal.acm.org/citation.cfm?doid=1699510.1699553},
	doi = {10.3115/1699510.1699553},
	language = {en},
	urldate = {2020-04-14},
	booktitle = {Proceedings of the 2009 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} {Volume} 1 - {EMNLP} '09},
	publisher = {Association for Computational Linguistics},
	author = {Roark, Brian and Bachrach, Asaf and Cardenas, Carlos and Pallier, Christophe},
	year = {2009},
	pages = {324},
}

@article{hale_information_2003,
	title = {The information conveyed by words in sentences},
	volume = {32},
	issn = {00906905},
	url = {http://link.springer.com/10.1023/A:1022492123056},
	doi = {10.1023/A:1022492123056},
	number = {2},
	urldate = {2020-04-14},
	journal = {Journal of Psycholinguistic Research},
	author = {Hale, John},
	year = {2003},
	keywords = {computational linguistics, parser, complexity metric},
	pages = {101--123},
}

@inproceedings{frank_uncertainty_2010,
	title = {Uncertainty reduction as a measure of cognitive processing effort},
	booktitle = {In proceedings of the 2010 workshop on cognitive modeling and computational linguistics},
	author = {Frank, Stefan L.},
	year = {2010},
	pages = {81--89},
}

@article{gibson_memory_1999,
	title = {Memory limitations and structural forgetting: {The} perception of complex ungrammatical sentences as grammatical},
	volume = {14},
	issn = {0169-0965, 1464-0732},
	shorttitle = {Memory {Limitations} and {Structural} {Forgetting}},
	url = {http://www.tandfonline.com/doi/abs/10.1080/016909699386293},
	doi = {10.1080/016909699386293},
	language = {en},
	number = {3},
	urldate = {2020-04-14},
	journal = {Language and Cognitive Processes},
	author = {Gibson, Edward and Thomas, James},
	month = jun,
	year = {1999},
	keywords = {sentence comprehension, memory, complexity metric},
	pages = {225--248},
}

@article{rabbani_potential_2019,
	title = {The potential for a speech brain–computer interface using chronic electrocorticography},
	volume = {16},
	issn = {1933-7213, 1878-7479},
	url = {http://link.springer.com/10.1007/s13311-018-00692-2},
	doi = {10.1007/s13311-018-00692-2},
	language = {en},
	number = {1},
	urldate = {2020-04-14},
	journal = {Neurotherapeutics},
	author = {Rabbani, Qinwan and Milsap, Griffin and Crone, Nathan E.},
	month = jan,
	year = {2019},
	keywords = {ECoG, review, BCI},
	pages = {144--165},
}

@article{miller_broadband_2010,
	title = {Broadband spectral change: {Evidence} for a macroscale correlate of population firing rate?},
	volume = {30},
	issn = {0270-6474, 1529-2401},
	shorttitle = {Broadband {Spectral} {Change}},
	url = {http://www.jneurosci.org/cgi/doi/10.1523/JNEUROSCI.6401-09.2010},
	doi = {10.1523/JNEUROSCI.6401-09.2010},
	language = {en},
	number = {19},
	urldate = {2020-04-14},
	journal = {Journal of Neuroscience},
	author = {Miller, K. J.},
	month = may,
	year = {2010},
	keywords = {electrophysiology, ECoG, broadband gamma},
	pages = {6477--6479},
}

@article{lopesdasilva_eeg_2013,
	title = {{EEG} and {MEG}: {Relevance} to neuroscience},
	volume = {80},
	issn = {08966273},
	shorttitle = {{EEG} and {MEG}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627313009203},
	doi = {10.1016/j.neuron.2013.10.017},
	language = {en},
	number = {5},
	urldate = {2020-04-13},
	journal = {Neuron},
	author = {Lopes da Silva, Fernando},
	month = dec,
	year = {2013},
	keywords = {MEG, EEG, oscillations, review},
	pages = {1112--1128},
}

@article{lachaux_intracranial_2003,
	title = {Intracranial {EEG} and human brain mapping},
	volume = {97},
	issn = {09284257},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0928425704000348},
	doi = {10.1016/j.jphysparis.2004.01.018},
	language = {en},
	number = {4-6},
	urldate = {2020-04-13},
	journal = {Journal of Physiology-Paris},
	author = {Lachaux, J.Ph and Rudrauf, D and Kahane, P},
	month = jul,
	year = {2003},
	keywords = {ECoG, review, iEEG},
	pages = {613--628},
}

@article{marcus_building_1993,
	title = {Building a large annotated corpus of english: the penn treebank},
	volume = {19},
	number = {2},
	journal = {Computational Linguistics},
	author = {Marcus, Mitchell and Marcinkiewicz, Mary and Santorini, Beatrice},
	year = {1993},
	pages = {313--330},
}

@article{roark_probabilistic_2001,
	title = {Probabilistic top-down parsing and language modeling},
	volume = {27},
	issn = {0891-2017, 1530-9312},
	url = {http://www.mitpressjournals.org/doi/10.1162/089120101750300526},
	doi = {10.1162/089120101750300526},
	language = {en},
	number = {2},
	urldate = {2020-04-12},
	journal = {Computational Linguistics},
	author = {Roark, Brian},
	month = jun,
	year = {2001},
	keywords = {language model, probability theory, thesis, parser, thesis.lmrev},
	pages = {249--276},
}

@article{de_mulder_survey_2015,
	title = {A survey on the application of recurrent neural networks to statistical language modeling},
	volume = {30},
	issn = {08852308},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S088523081400093X},
	doi = {10.1016/j.csl.2014.09.005},
	language = {en},
	number = {1},
	urldate = {2020-04-12},
	journal = {Computer Speech \& Language},
	author = {De Mulder, Wim and Bethard, Steven and Moens, Marie-Francine},
	month = mar,
	year = {2015},
	keywords = {language model, RNN, computational linguistics, thesis, thesis.lmrev},
	pages = {61--98},
}

@article{rosenfeld_two_2000,
	title = {Two decades of statistical language modeling: where do we go from here?},
	volume = {88},
	issn = {0018-9219, 1558-2256},
	shorttitle = {Two decades of statistical language modeling},
	url = {http://ieeexplore.ieee.org/document/880083/},
	doi = {10.1109/5.880083},
	number = {8},
	urldate = {2020-04-12},
	journal = {Proceedings of the IEEE},
	author = {Rosenfeld, R.},
	month = aug,
	year = {2000},
	keywords = {language model, thesis, thesis.lmrev},
	pages = {1270--1278},
}

@article{huettig_four_2015,
	title = {Four central questions about prediction in language processing},
	volume = {1626},
	issn = {00068993},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0006899315001146},
	doi = {10.1016/j.brainres.2015.02.014},
	language = {en},
	urldate = {2020-04-12},
	journal = {Brain Research},
	author = {Huettig, Falk},
	month = nov,
	year = {2015},
	keywords = {prediction, thesis, predictive coding, thesis.lmrev},
	pages = {118--135},
}

@article{taylor_cloze_1953,
	title = {'{Cloze} procedure': a new tool for measuring readability},
	volume = {30},
	issn = {1098-6596},
	abstract = {"Cloze Procedure" involves no formula or "element counting," but consists of sampling all potential readability influences. Although similar to sentence-completion tests, the cloze method demands deletion of random words from a passage. After administration to a group the correctly identified omissions are tallied. Experimental results show: (1) the cloze method consistently ranked three selected passages in the same way as the Flesch and Dale-Chall formulas; (2) the method was reliable; (3) the cloze method seemed to handle specialized passages more adequately than other methods; (4) the same rankings of readability were obtained when words were deleted at random or every nth word; (5) the cloze procedure could be used for comparing reading abilities of different individuals. (PsycINFO Database Record (c) 2010 APA, all rights reserved)},
	journal = {Journalism Quarterly},
	author = {Taylor, Wilson L},
	year = {1953},
	note = {tex.isbn: 0196-3031},
	pages = {415--433},
}

@article{gibson_constraints_1998,
	title = {Constraints on sentence comprehension},
	volume = {2},
	issn = {13646613},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661398011875},
	doi = {10.1016/S1364-6613(98)01187-5},
	language = {en},
	number = {7},
	urldate = {2020-04-12},
	journal = {Trends in Cognitive Sciences},
	author = {Gibson, Edward and Pearlmutter, Neal J},
	month = jul,
	year = {1998},
	keywords = {sentence comprehension, thesis, thesis.lmrev, context},
	pages = {262--268},
}

@article{seidenberg_language_1997,
	title = {Language acquisition and use: {Learning} and applying probabilistic constraints},
	volume = {275},
	issn = {0036-8075, 1095-9203},
	shorttitle = {Language {Acquisition} and {Use}},
	url = {https://www.sciencemag.org/lookup/doi/10.1126/science.275.5306.1599},
	doi = {10.1126/science.275.5306.1599},
	language = {en},
	number = {5306},
	urldate = {2020-04-12},
	journal = {Science},
	author = {Seidenberg, M. S.},
	month = mar,
	year = {1997},
	keywords = {probabilistic models of cognition, thesis, thesis.lmrev, language acquisition},
	pages = {1599--1603},
}

@article{griffiths_rethinking_2011,
	title = {Rethinking language: {How} probabilities shape the words we use},
	volume = {108},
	issn = {0027-8424, 1091-6490},
	shorttitle = {Rethinking language},
	url = {http://www.pnas.org/cgi/doi/10.1073/pnas.1100760108},
	doi = {10.1073/pnas.1100760108},
	language = {en},
	number = {10},
	urldate = {2020-04-12},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Griffiths, T. L.},
	month = mar,
	year = {2011},
	keywords = {probabilistic models of cognition, probability theory, thesis, thesis.lmrev},
	pages = {3825--3826},
}

@article{kuhl_brain_2010,
	title = {Brain mechanisms in early language acquisition},
	volume = {67},
	issn = {08966273},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627310006811},
	doi = {10.1016/j.neuron.2010.08.038},
	language = {en},
	number = {5},
	urldate = {2020-04-12},
	journal = {Neuron},
	author = {Kuhl, Patricia K.},
	month = sep,
	year = {2010},
	keywords = {thesis, language acquisition},
	pages = {713--727},
}

@book{hagoort_human_2019,
	address = {Cambridge, MA},
	title = {Human language: from genes to behavior},
	isbn = {978-0-262-04263-5},
	shorttitle = {Human language},
	publisher = {The MIT Press},
	editor = {Hagoort, Peter},
	year = {2019},
}

@article{aron_climate_2019,
	title = {The climate crisis needs attention from cognitive scientists},
	volume = {23},
	issn = {13646613},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661319302025},
	doi = {10.1016/j.tics.2019.08.001},
	language = {en},
	number = {11},
	urldate = {2020-04-09},
	journal = {Trends in Cognitive Sciences},
	author = {Aron, Adam R.},
	month = nov,
	year = {2019},
	keywords = {cognitive science, climate emergency, cognitive bias},
	pages = {903--906},
}

@article{aron_how_2020,
	title = {How can neuroscientists respond to the climate emergency?},
	volume = {106},
	issn = {08966273},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627320301422},
	doi = {10.1016/j.neuron.2020.02.019},
	language = {en},
	number = {1},
	urldate = {2020-04-09},
	journal = {Neuron},
	author = {Aron, Adam R. and Ivry, Richard B. and Jeffery, Kate J. and Poldrack, Russell A. and Schmidt, Robert and Summerfield, Christopher and Urai, Anne E.},
	month = apr,
	year = {2020},
	keywords = {open science, science policy, climate emergency},
	pages = {17--20},
}

@article{makin_machine_2020,
	title = {Machine translation of cortical activity to text with an encoder–decoder framework},
	volume = {23},
	issn = {1097-6256, 1546-1726},
	url = {http://www.nature.com/articles/s41593-020-0608-8},
	doi = {10.1038/s41593-020-0608-8},
	language = {en},
	number = {4},
	urldate = {2020-04-07},
	journal = {Nature Neuroscience},
	author = {Makin, Joseph G. and Moses, David A. and Chang, Edward F.},
	month = apr,
	year = {2020},
	keywords = {LSTM, RNN, speech production, ECoG, encoder-decoder},
	pages = {575--582},
}

@article{lesser_subdural_2010,
	title = {Subdural electrodes},
	volume = {121},
	issn = {13882457},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1388245710004876},
	doi = {10.1016/j.clinph.2010.04.037},
	language = {en},
	number = {9},
	urldate = {2020-04-07},
	journal = {Clinical Neurophysiology},
	author = {Lesser, Ronald P. and Crone, Nathan E. and Webber, W.R.S.},
	month = sep,
	year = {2010},
	keywords = {electrophysiology, ECoG, subdural electrodes},
	pages = {1376--1392},
}

@article{norgeot_call_2019,
	title = {A call for deep-learning healthcare},
	volume = {25},
	issn = {1078-8956, 1546-170X},
	url = {http://www.nature.com/articles/s41591-018-0320-3},
	doi = {10.1038/s41591-018-0320-3},
	language = {en},
	number = {1},
	urldate = {2020-04-05},
	journal = {Nature Medicine},
	author = {Norgeot, Beau and Glicksberg, Benjamin S. and Butte, Atul J.},
	month = jan,
	year = {2019},
	keywords = {deep learning, digital medicine, EHR, healthcare, precision medicine},
	pages = {14--15},
}

@article{kehl_assessment_2019,
	title = {Assessment of deep natural language processing in ascertaining oncologic outcomes from radiology reports},
	volume = {5},
	issn = {2374-2437},
	url = {https://jamanetwork.com/journals/jamaoncology/fullarticle/2738774},
	doi = {10.1001/jamaoncol.2019.1800},
	language = {en},
	number = {10},
	urldate = {2020-04-05},
	journal = {JAMA Oncology},
	author = {Kehl, Kenneth L. and Elmarakeby, Haitham and Nishino, Mizuki and Van Allen, Eliezer M. and Lepisto, Eva M. and Hassett, Michael J. and Johnson, Bruce E. and Schrag, Deborah},
	month = oct,
	year = {2019},
	keywords = {NLP, clinical NLP, digital medicine, precision medicine, oncology},
	pages = {1421},
}

@article{trister_tipping_2019,
	title = {The tipping point for deep learning in oncology},
	volume = {5},
	issn = {2374-2437},
	url = {https://jamanetwork.com/journals/jamaoncology/fullarticle/2738770},
	doi = {10.1001/jamaoncol.2019.1799},
	language = {en},
	number = {10},
	urldate = {2020-04-05},
	journal = {JAMA Oncology},
	author = {Trister, Andrew Daniel},
	month = oct,
	year = {2019},
	keywords = {NLP, clinical NLP, digital medicine, oncology},
	pages = {1429},
}

@article{pater_generative_2019,
	title = {Generative linguistics and neural networks at 60: {Foundation}, friction, and fusion},
	volume = {95},
	issn = {1535-0665},
	shorttitle = {Generative linguistics and neural networks at 60},
	url = {https://muse.jhu.edu/article/719231},
	doi = {10.1353/lan.2019.0009},
	language = {en},
	number = {1},
	urldate = {2020-04-02},
	journal = {Language},
	author = {Pater, Joe},
	year = {2019},
	pages = {e41--e74},
}

@article{lakens_justify_2017,
	title = {Justify your alpha: {A} response to “{Redefine} {Statistical} {Significance}”},
	doi = {10.17605/OSF.IO/9S3Y6},
	abstract = {In response to recommendations to redefine statistical significance to p ≤ .005, we propose that researchers should transparently report and justify all choices they make when designing a study, including the alpha level.},
	author = {Lakens, Daniel and Adolfi, Federico and Albers, Casper and Anvari, Farid and Apps, Matthew and Argamon, Shlomo and Van Assen, Marcel and Baguley, Thom and Becker, Raymond and Benning, Stephen and Bradford, Daniel and Buchanan, Erin and Caldwell, Aaron and Van Calster, Ben and Carlsson, Rickard and Chen, Sau-Chin and Chung, Bryan and Colling, Lincoln and Collins, Gary and Crook, Zander and Cross, Emily and Daniels, Sameera and Danielsson, Henrik and DeBruine, Lisa and Dunleavy, Daniel and Earp, Brian and Feist, Michele and Ferrell, Jason and Field, James and Fox, Nick and Friesen, Amanda and Gomes, Caio and Grange, Jim and Grieve, Andrew and Guggenberger, Robert and Van Harmelen, Anne-Laura and Hasselman, Fred and Hochard, Kevin and Hoffarth, Mark and Holmes, Nicholas and Ingre, Michael and Isager, Peder and Isotalus, Hanna and Johansson, Christer and Juszczyk, Konrad and Kenny, David and Khalil, Ahmed and Konat, Barbara and Lao, Junpeng and Larsen, Erik and Lodder, Gerine and Lukavsky, Jiri and Madan, Christopher and Manheim, David and Gonzalez-Marquez, Monica and Martin, Stephen and Martin, Andrea and Mayo, Deborah and McCarthy, Randy and McConway, Kevin and McFarland, Colin and Nilsonne, Gustav and Nio, Amanda and De Oliveira, Cilene and Parsons, Sam and Pfuhl, Gerit and Quinn, Kimberly and Sakon, John and Saribay, Selahattin and Schneider, Iris and Selvaraju, Manojkumar and Sjoerds, Zsuzsika and Smith, Samuel and Smits, Tim and Spies, Jeffrey and Sreekumar, Vishnu and Steltenpohl, Crystal and Stenhouse, Neil and Świ{\textbackslash}c atkowski, Wojciech and Vadillo, Miguel and Williams, Matt and Williams, Samantha and Williams, Donald and Orban De Xivry, Jean-Jacques and Yarkoni, Tal and Ziano, Ignazio and Zwaan, Rolf},
	month = jan,
	year = {2017},
	keywords = {methods, readme, statistics},
}

@article{kavadias_transformative_2016,
	title = {The transformative business model},
	issn = {0017-8012},
	url = {https://hbr.org/2016/10/the-transformative-business-model},
	abstract = {How to tell if you have one},
	number = {October 2016},
	urldate = {2020-03-30},
	journal = {Harvard Business Review},
	author = {Kavadias, Stelios and Ladas, Kostas and Loch, Christoph},
	month = oct,
	year = {2016},
	note = {Section: Strategy},
	keywords = {business model, business strategy, innovation},
	file = {Snapshot:/Users/kriarm/Zotero/storage/EUJXWXUE/the-transformative-business-model.html:text/html},
}

@article{graves_generating_2014,
	title = {Generating sequences with recurrent neural networks},
	url = {http://arxiv.org/abs/1308.0850},
	abstract = {This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles.},
	urldate = {2020-03-26},
	journal = {arXiv:1308.0850 [cs]},
	author = {Graves, Alex},
	month = jun,
	year = {2014},
	note = {arXiv: 1308.0850},
	keywords = {language model, RNN},
}

@article{lample_neural_2016,
	title = {Neural architectures for named entity recognition},
	url = {http://arxiv.org/abs/1603.01360},
	abstract = {State-of-the-art named entity recognition systems rely heavily on hand-crafted features and domain-specific knowledge in order to learn effectively from the small, supervised training corpora that are available. In this paper, we introduce two new neural architectures---one based on bidirectional LSTMs and conditional random fields, and the other that constructs and labels segments using a transition-based approach inspired by shift-reduce parsers. Our models rely on two sources of information about words: character-based word representations learned from the supervised corpus and unsupervised word representations learned from unannotated corpora. Our models obtain state-of-the-art performance in NER in four languages without resorting to any language-specific knowledge or resources such as gazetteers.},
	urldate = {2020-03-26},
	journal = {arXiv:1603.01360 [cs]},
	author = {Lample, Guillaume and Ballesteros, Miguel and Subramanian, Sandeep and Kawakami, Kazuya and Dyer, Chris},
	month = apr,
	year = {2016},
	note = {arXiv: 1603.01360},
	keywords = {NLP, LSTM, RNN, project.lstmMEG, NER},
}

@article{lipton_critical_2015,
	title = {A critical review of recurrent neural networks for sequence learning},
	url = {https://arxiv.org/abs/1506.00019},
	language = {en},
	urldate = {2018-10-02},
	author = {Lipton, Zachary C. and Berkowitz, John and Elkan, Charles},
	month = may,
	year = {2015},
	keywords = {LSTM, project.lstmMEG, readme},
}

@inproceedings{sutskever_sequence_2014,
	series = {{NIPS}'14},
	title = {Sequence to sequence learning with neural networks},
	url = {http://dl.acm.org/citation.cfm?id=2969033.2969173},
	booktitle = {Proceedings of the 27th {International} {Conference} on {Neural} {Information} {Processing} {Systems} - {Volume} 2},
	publisher = {MIT Press},
	author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V.},
	year = {2014},
	keywords = {deep learning, LSTM, readme, machine translation},
	pages = {3104--3112},
}

@article{sak_long_2014,
	title = {Long short-term memory based recurrent neural network architectures for large vocabulary speech recognition},
	url = {http://arxiv.org/abs/1402.1128},
	abstract = {Long Short-Term Memory (LSTM) is a recurrent neural network (RNN) architecture that has been designed to address the vanishing and exploding gradient problems of conventional RNNs. Unlike feedforward neural networks, RNNs have cyclic connections making them powerful for modeling sequences. They have been successfully used for sequence labeling and sequence prediction tasks, such as handwriting recognition, language modeling, phonetic labeling of acoustic frames. However, in contrast to the deep neural networks, the use of RNNs in speech recognition has been limited to phone recognition in small scale tasks. In this paper, we present novel LSTM based RNN architectures which make more effective use of model parameters to train acoustic models for large vocabulary speech recognition. We train and compare LSTM, RNN and DNN models at various numbers of parameters and configurations. We show that LSTM models converge quickly and give state of the art speech recognition performance for relatively small sized models.},
	urldate = {2020-03-26},
	journal = {arXiv:1402.1128 [cs, stat]},
	author = {Sak, Haşim and Senior, Andrew and Beaufays, Françoise},
	month = feb,
	year = {2014},
	note = {arXiv: 1402.1128},
	keywords = {NLP, LSTM, speech recognition},
}

@techreport{jain_interpretable_2020,
	type = {preprint},
	title = {Interpretable multi-timescale models for predicting {fMRI} responses to continuous natural speech},
	url = {http://biorxiv.org/lookup/doi/10.1101/2020.10.02.324392},
	abstract = {Abstract
          Natural language contains information at multiple timescales. To understand how the human brain represents this information, one approach is to build encoding models that predict fMRI responses to natural language using representations extracted from neural network language models (LMs). However, these LM-derived representations do not explicitly separate information at different timescales, making it difficult to interpret the encoding models. In this work we construct interpretable multi-timescale representations by forcing individual units in an LSTM LM to integrate information over specific temporal scales. This allows us to explicitly and directly map the timescale of information encoded by each individual fMRI voxel. Further, the standard fMRI encoding procedure does not account for varying temporal properties in the encoding features. We modify the procedure so that it can capture both short- and long-timescale information. This approach outperforms other encoding models, particularly for voxels that represent long-timescale information. It also provides a finer-grained map of timescale information in the human language pathway. This serves as a framework for future work investigating temporal hierarchies across artificial and biological language systems.},
	language = {en},
	urldate = {2021-06-14},
	institution = {Neuroscience},
	author = {Jain, Shailee and Mahto, Shivangi and Turek, Javier S. and Vo, Vy A. and LeBel, Amanda and Huth, Alexander G.},
	month = oct,
	year = {2020},
	doi = {10.1101/2020.10.02.324392},
}

@article{jacobs_fictive_2018,
	title = {The fictive brain: neurocognitive correlates of engagement in literature},
	volume = {22},
	issn = {1089-2680, 1939-1552},
	shorttitle = {The {Fictive} {Brain}},
	url = {http://journals.sagepub.com/doi/10.1037/gpr0000106},
	doi = {10.1037/gpr0000106},
	language = {en},
	number = {2},
	urldate = {2020-09-02},
	journal = {Review of General Psychology},
	author = {Jacobs, Arthur M. and Willems, Roel M.},
	month = jun,
	year = {2018},
	pages = {147--160},
}

@article{halevy_unreasonable_2009,
	title = {The unreasonable effectiveness of data},
	volume = {24},
	url = {http://www.computer.org/portal/cms_docs_intelligent/intelligent/homepage/2009/x2exp.pdf},
	journal = {IEEE Intelligent Systems},
	author = {Halevy, Alon and Norvig, Peter and Pereira, Fernando},
	year = {2009},
	keywords = {NLP, corpus, semantic web},
	pages = {8--12},
}

@article{oatley_imagining_2018,
	title = {Imagining {Possible} {Worlds}},
	volume = {22},
	issn = {1089-2680},
	url = {https://doi.org/10.1037/gpr0000149},
	doi = {10.1037/gpr0000149},
	abstract = {The reading of fiction has been found to confer benefits, including increased empathy and understanding of others. Among ongoing research questions are those of how people engage in imagined worlds while keeping in touch with the currently perceived world, as well as how far stories were important in human evolution and how the brain is involved understanding them.},
	language = {en},
	number = {2},
	urldate = {2021-06-09},
	journal = {Review of General Psychology},
	author = {Oatley, Keith and Dunbar, Robin and Budelmann, Felix},
	month = jun,
	year = {2018},
	note = {Publisher: SAGE Publications Inc},
	keywords = {evolution, narrative, default mode network, mentalizing, theory of mind, TOM},
	pages = {121--124},
	file = {SAGE PDF Full Text:/Users/kriarm/Zotero/storage/L35UF79Y/Oatley et al. - 2018 - Imagining Possible Worlds.pdf:application/pdf},
}

@article{koban_self_2021,
	title = {The self in context: brain systems linking mental and physical health},
	volume = {22},
	copyright = {2021 Springer Nature Limited},
	issn = {1471-0048},
	shorttitle = {The self in context},
	url = {https://www.nature.com/articles/s41583-021-00446-8},
	doi = {10.1038/s41583-021-00446-8},
	abstract = {Increasing evidence suggests that mental health and physical health are linked by neural systems that jointly regulate somatic physiology and high-level cognition. Key systems include the ventromedial prefrontal cortex and the related default-mode network. These systems help to construct models of the ‘self-in-context’, compressing information across time and sensory modalities into conceptions of the underlying causes of experience. Self-in-context models endow events with personal meaning and allow predictive control over behaviour and peripheral physiology, including autonomic, neuroendocrine and immune function. They guide learning from experience and the formation of narratives about the self and one’s world. Disorders of mental and physical health, especially those with high co-occurrence and convergent alterations in the functionality of the ventromedial prefrontal cortex and the default-mode network, could benefit from interventions focused on understanding and shaping mindsets and beliefs about the self, illness and treatment.},
	language = {en},
	number = {5},
	urldate = {2021-06-07},
	journal = {Nature Reviews Neuroscience},
	author = {Koban, Leonie and Gianaros, Peter J. and Kober, Hedy and Wager, Tor D.},
	month = may,
	year = {2021},
	note = {Number: 5
Publisher: Nature Publishing Group},
	keywords = {predictive coding, anxiety, depression, mindset, vmPFC},
	pages = {309--322},
	file = {Snapshot:/Users/kriarm/Zotero/storage/AX4B54KY/s41583-021-00446-8.html:text/html},
}

@article{feng_survey_2021,
	title = {A survey of data augmentation approaches for {NLP}},
	url = {http://arxiv.org/abs/2105.03075},
	abstract = {Data augmentation has recently seen increased interest in NLP due to more work in low-resource domains, new tasks, and the popularity of large-scale neural networks that require large amounts of training data. Despite this recent upsurge, this area is still relatively underexplored, perhaps due to the challenges posed by the discrete nature of language data. In this paper, we present a comprehensive and unifying survey of data augmentation for NLP by summarizing the literature in a structured manner. We first introduce and motivate data augmentation for NLP, and then discuss major methodologically representative approaches. Next, we highlight techniques that are used for popular NLP applications and tasks. We conclude by outlining current challenges and directions for future research. Overall, our paper aims to clarify the landscape of existing literature in data augmentation for NLP and motivate additional work in this area. We also present a GitHub repository with a paper list that will be continuously updated at https://github.com/styfeng/DataAug4NLP},
	urldate = {2021-06-03},
	journal = {arXiv:2105.03075 [cs]},
	author = {Feng, Steven Y. and Gangal, Varun and Wei, Jason and Chandar, Sarath and Vosoughi, Soroush and Mitamura, Teruko and Hovy, Eduard},
	month = may,
	year = {2021},
	note = {arXiv: 2105.03075
version: 3},
	keywords = {NLP, LM, data augmentation},
}

@article{henson_unchained_1996,
	title = {Unchained memory: {Error} patterns rule out chaining models of immediate serial recall},
	volume = {49},
	issn = {0272-4987, 1464-0740},
	shorttitle = {Unchained {Memory}},
	url = {http://journals.sagepub.com/doi/10.1080/713755612},
	doi = {10.1080/713755612},
	abstract = {Many models of serial recall assume a chaining mechanism whereby each item associatively evokes the next in sequence. Chaining predicts that, when sequences comprise alternating confusable and non-confusable items, confusable items should increase the probability of errors in recall of following non-confusable items. Two experiments using visual presentation and one using vocalized presentation test this prediction and demonstrate that: (1) more errors occur in recall of confusable than alternated non-confusable items, revealing a “sawtooth” in serial position curves; (2) the presence of confusable items often has no influence on recall of the non-confusable items; and (3) the confusability of items does not affect the type of errors that follow them. These results are inconsistent with the chaining hypothesis. Further analysis of errors shows that most transpositions occur over short distances (the locality constraint), confusable items tend to interchange (the similarity constraint), and repeated responses are rare and far apart (the repetition constraint). The complete pattern of errors presents problems for most current models of serial recall, whether or not they employ chaining. An alternative model is described that is consistent with these constraints and that simulates the detailed pattern of errors observed.},
	language = {en},
	number = {1},
	urldate = {2021-05-23},
	journal = {The Quarterly Journal of Experimental Psychology Section A},
	author = {Henson, Richard N. A. and Norris, Dennis G. and Page, Michael P. A. and Baddeley, Alan D.},
	month = feb,
	year = {1996},
	pages = {80--115},
}

@inproceedings{bender_dangers_2021,
	address = {New York, NY, USA},
	series = {{FAccT} '21},
	title = {On the dangers of stochastic parrots: {Can} language models be too big? \&\#x1f99c;},
	isbn = {978-1-4503-8309-7},
	shorttitle = {On the {Dangers} of {Stochastic} {Parrots}},
	url = {https://doi.org/10.1145/3442188.3445922},
	doi = {10.1145/3442188.3445922},
	abstract = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
	urldate = {2021-05-23},
	booktitle = {Proceedings of the 2021 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
	month = mar,
	year = {2021},
	pages = {610--623},
}

@article{botvinick_short-term_2006,
	title = {Short-term memory for serial order: {A} recurrent neural network model},
	volume = {113},
	issn = {1939-1471, 0033-295X},
	shorttitle = {Short-term memory for serial order},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-295X.113.2.201},
	doi = {10.1037/0033-295X.113.2.201},
	language = {en},
	number = {2},
	urldate = {2020-10-21},
	journal = {Psychological Review},
	author = {Botvinick, Matthew M. and Plaut, David C.},
	year = {2006},
	pages = {201--233},
}

@article{ishiguro_detrimental_2021,
	title = {The detrimental effect of semantic similarity in short-term memory tasks: {A} meta-regression approach},
	volume = {28},
	issn = {1069-9384, 1531-5320},
	shorttitle = {The detrimental effect of semantic similarity in short-term memory tasks},
	url = {https://link.springer.com/10.3758/s13423-020-01815-7},
	doi = {10.3758/s13423-020-01815-7},
	language = {en},
	number = {2},
	urldate = {2021-05-23},
	journal = {Psychonomic Bulletin \& Review},
	author = {Ishiguro, Sho and Saito, Satoru},
	month = apr,
	year = {2021},
	pages = {384--408},
}

@article{bandy_addressing_2021,
	title = {Addressing "documentation debt" in machine learning research: {A} retrospective datasheet for {BookCorpus}},
	shorttitle = {Addressing "{Documentation} {Debt}" in {Machine} {Learning} {Research}},
	url = {http://arxiv.org/abs/2105.05241},
	abstract = {Recent literature has underscored the importance of dataset documentation work for machine learning, and part of this work involves addressing "documentation debt" for datasets that have been used widely but documented sparsely. This paper aims to help address documentation debt for BookCorpus, a popular text dataset for training large language models. Notably, researchers have used BookCorpus to train OpenAI's GPT-N models and Google's BERT models, even though little to no documentation exists about the dataset's motivation, composition, collection process, etc. We offer a preliminary datasheet that provides key context and information about BookCorpus, highlighting several notable deficiencies. In particular, we find evidence that (1) BookCorpus likely violates copyright restrictions for many books, (2) BookCorpus contains thousands of duplicated books, and (3) BookCorpus exhibits significant skews in genre representation. We also find hints of other potential deficiencies that call for future research, including problematic content, potential skews in religious representation, and lopsided author contributions. While more work remains, this initial effort to provide a datasheet for BookCorpus adds to growing literature that urges more careful and systematic documentation for machine learning datasets.},
	urldate = {2021-05-13},
	journal = {arXiv:2105.05241 [cs]},
	author = {Bandy, Jack and Vincent, Nicholas},
	month = may,
	year = {2021},
	note = {arXiv: 2105.05241},
	keywords = {open science, BookCorpus, datasheet},
}

@misc{lecun_self-supervised_2021,
	title = {Self-supervised learning: {The} dark matter of intelligence},
	shorttitle = {Self-supervised learning},
	url = {https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/},
	abstract = {How can we build machines with human-level intelligence? There’s a limit to how far the field of AI can go with supervised learning alone. Here's why self-supervised learning is one of the most promising ways to make significant progress in AI.},
	language = {en},
	urldate = {2021-05-11},
	journal = {Facebook AI Blog},
	author = {LeCun, Yann and Mishra, Ishan},
	month = mar,
	year = {2021},
}

@inproceedings{nematzadeh_memory_2020-1,
	title = {On memory in human and artificial language processing systems},
	booktitle = {Bridging {AI} and cognitive science},
	author = {Nematzadeh, A. and Ruder, Sebastian and Yogatama, Dani},
	year = {2020},
}

@article{daneman_working_1996,
	title = {Working memory and language comprehension: {A} meta-analysis},
	volume = {3},
	issn = {1069-9384, 1531-5320},
	shorttitle = {Working memory and language comprehension},
	url = {http://link.springer.com/10.3758/BF03214546},
	doi = {10.3758/BF03214546},
	language = {en},
	number = {4},
	urldate = {2021-05-10},
	journal = {Psychonomic Bulletin \& Review},
	author = {Daneman, Meredyth and Merikle, Philip M.},
	month = dec,
	year = {1996},
	pages = {422--433},
	file = {Full Text:/Users/kriarm/Zotero/storage/NKC9UDE5/Daneman and Merikle - 1996 - Working memory and language comprehension A meta-.pdf:application/pdf},
}

@article{just_hybrid_2002,
	title = {A hybrid architecture for working memory: {Reply} to {MacDonald} and {Christiansen} (2002).},
	volume = {109},
	issn = {1939-1471, 0033-295X},
	shorttitle = {A hybrid architecture for working memory},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-295X.109.1.55},
	doi = {10.1037/0033-295X.109.1.55},
	language = {en},
	number = {1},
	urldate = {2021-05-10},
	journal = {Psychological Review},
	author = {Just, Marcel Adam and Varma, Sashank},
	year = {2002},
	keywords = {working memory},
	pages = {55--65},
}

@article{potter_conceptual_2012,
	title = {Conceptual short term memory in perception and thought},
	volume = {3},
	issn = {1664-1078},
	url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2012.00113/full},
	doi = {10.3389/fpsyg.2012.00113},
	abstract = {Conceptual short term memory (CSTM) is a theoretical construct that provides one answer to the question of how perceptual and conceptual processes are related. CSTM is a mental buffer and processor in which current perceptual stimuli and their associated concepts from long term memory (LTM) are represented briefly, allowing meaningful patterns or structures to be identified (Potter, 1993, 1999, 2009). CSTM is different from and complementary to other proposed forms of working memory: it is engaged extremely rapidly, has a large but ill-defined capacity, is largely unconscious, and is the basis for the unreflective understanding that is characteristic of everyday experience. The key idea behind CSTM is that most cognitive processing occurs without review or rehearsal of material in standard working memory and with little or no conscious reasoning. When one perceives a meaningful stimulus such as a word, picture, or object, it is rapidly identified at a conceptual level and in turn activates associated information from long term memory. New links among concurrently active concepts are formed in CSTM, shaped by parsing mechanisms of language or grouping principles in scene perception and by higher-level knowledge and current goals. The resulting structure represents the gist of a picture or the meaning of a sentence, and it is this structure that we are conscious of and that can be maintained in standard working memory and consolidated into long term memory. Momentarily activated information that is not incorporated into such structures either never becomes conscious or is rapidly forgotten. This whole cycle--identification of perceptual stimuli, memory recruitment, structuring, consolidation in long term memory, and forgetting of nonstructured material--may occur in less than 1 second when viewing a pictured scene or reading a sentence. The evidence for such a process is reviewed and its implications for the relation of perception and cognition are discussed.},
	language = {English},
	urldate = {2021-04-28},
	journal = {Frontiers in Psychology},
	author = {Potter, Mary C.},
	year = {2012},
	note = {Publisher: Frontiers},
	keywords = {memory span, phonological loop, short-term memory},
}

@inproceedings{alammar_machine_2021,
	title = {Machine learning research communication via illustrated and interactive web articles},
	url = {https://openreview.net/forum?id=WUrcJoyHud},
	abstract = {The recent explosion in machine learning research activity poses challenges for both researchers who aim to widely disseminate their work, as well as to readers who find it challenging to keep up...},
	language = {en},
	urldate = {2021-04-27},
	author = {Alammar, J.},
	month = mar,
	year = {2021},
}

@article{jordan_artificial_2019,
	title = {Artificial intelligence—{The} revolution hasn’t happened yet},
	volume = {1},
	issn = {,},
	url = {https://hdsr.mitpress.mit.edu/pub/wot7mkc1/release/9},
	doi = {10.1162/99608f92.f06c6e61},
	language = {en},
	number = {1},
	urldate = {2021-04-10},
	journal = {Harvard Data Science Review},
	author = {Jordan, Michael I.},
	month = jul,
	year = {2019},
	note = {Publisher: PubPub},
	keywords = {AI, cybernetics, civil engineering, inteligence infrastructure, intelligence augmentation},
	file = {Snapshot:/Users/kriarm/Zotero/storage/RCDD98N9/9.html:text/html},
}

@article{graves_neural_2014,
	title = {Neural turing machines},
	url = {http://arxiv.org/abs/1410.5401},
	abstract = {We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.},
	urldate = {2021-04-05},
	journal = {arXiv:1410.5401 [cs]},
	author = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
	month = dec,
	year = {2014},
	note = {arXiv: 1410.5401
version: 2},
}

@inproceedings{yogatama_memory_2018-2,
	title = {Memory architectures in recurrent neural network language models},
	url = {https://openreview.net/forum?id=SkFqf0lAZ},
	abstract = {We compare and analyze sequential, random access, and stack memory architectures for recurrent neural network language models. Our experiments on the Penn Treebank and Wikitext-2 datasets show that...},
	language = {en},
	urldate = {2021-04-05},
	author = {Yogatama, Dani and Miao, Yishu and Melis, Gabor and Ling, Wang and Kuncoro, Adhiguna and Dyer, Chris and Blunsom, Phil},
	month = feb,
	year = {2018},
}

@article{tulving_how_1985,
	title = {How many memory systems are there},
	doi = {10.1037/0003-066X.40.4.385},
	abstract = {Memory is made up of a number of interrelated systems, organized structures of operating components consisting of neural substrates and their behavioral and cognitive correlates. A ternary clas- sificatory scheme of memory is proposed in which procedural, semantic, and episodic memory constitute a "monohierarchical" arrangement: Episodic memory is a specialized subsystem of semantic memory, and semantic memory is a specialized subsystem of procedural memory. The three memory systems differ from one another in a number of ways, including the kind of consciousness that characterizes their operations. The ternary scheme overlaps with di- chotomies and trichotomies of memory proposed by others. Evidence for multiple systems is derived from many sources. Illustrative data are provided by ex- periments in which direct priming effects are found to be both functionally and stochastically independent of recognition memory. Solving puzzles in science has much in common with solving puzzles for amusement, but the two differ in important respects. Consider, for instance, the jigsaw puzzle that scientific activity frequently imitates. The everyday version of the puzzle is determinate: It consists of a target picture and jigsaw pieces that, when properly assembled, are guaranteed to match the picture. Scientific puzzles are indeter- minate: The number of pieces required to complete a picture is unpredictable; a particular piece may fit many pictures or none; it may fit only one picture, but the picture itself may be unknown; or the hypothetical picture may be imagined, but its com- ponent pieces may remain undiscovered. This article is about a current puzzle in the science of memory. It entails an imaginary picture and a search for pieces that fit it. The picture, or the hypothesis, depicts memory as consisting of a number of systems, each system serving somewhat different purposes and operating according to some- what different principles. Together they form the marvelous capacity that we call by the single name of memory, the capacity that permits organisms to benefit from their past experiences. Such a picture is at variance with conventional wisdom that holds memory to be essentially a single system, the idea that "memory is memory." The article consists of three main sections. In the first, 1 present some pretheoretical reasons for hypothesizing the existence of multiple memory systems and briefly discuss the concept of memory system. In the second, I describe a ternary classifi- catory scheme of memory--consisting of procedural, semantic, and episodic memory--and briefly com- pare this scheme with those proposed by others. In the third, I discuss the nature and logic of evidence for multiple systems and describe some experiments that have yielded data revealing independent effects of one and the same act of learning, effects seemingly at variance with the idea of a single system. I answer the question posed in the title of the article in the short concluding section.},
	author = {Tulving, E.},
	year = {1985},
}

@article{metz_meet_2020,
	chapter = {Science},
	title = {Meet {GPT}-3. {It} {Has} {Learned} to {Code} (and {Blog} and {Argue}).},
	issn = {0362-4331},
	url = {https://www.nytimes.com/2020/11/24/science/artificial-intelligence-ai-gpt3.html},
	abstract = {The latest natural-language system generates tweets, pens poetry, summarizes emails, answers trivia questions, translates languages and even writes its own computer programs.},
	language = {en-US},
	urldate = {2020-12-15},
	journal = {The New York Times},
	author = {Metz, Cade},
	month = nov,
	year = {2020},
	file = {Snapshot:/Users/kriarm/Zotero/storage/GX8YBMEC/artificial-intelligence-ai-gpt3.html:text/html},
}

@article{duarte_leveraging_2019,
	title = {Leveraging heterogeneity for neural computation with fading memory in layer 2/3 cortical microcircuits},
	volume = {15},
	issn = {1553-7358},
	url = {https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1006781},
	doi = {10.1371/journal.pcbi.1006781},
	abstract = {Complexity and heterogeneity are intrinsic to neurobiological systems, manifest in every process, at every scale, and are inextricably linked to the systems’ emergent collective behaviours and function. However, the majority of studies addressing the dynamics and computational properties of biologically inspired cortical microcircuits tend to assume (often for the sake of analytical tractability) a great degree of homogeneity in both neuronal and synaptic/connectivity parameters. While simplification and reductionism are necessary to understand the brain’s functional principles, disregarding the existence of the multiple heterogeneities in the cortical composition, which may be at the core of its computational proficiency, will inevitably fail to account for important phenomena and limit the scope and generalizability of cortical models. We address these issues by studying the individual and composite functional roles of heterogeneities in neuronal, synaptic and structural properties in a biophysically plausible layer 2/3 microcircuit model, built and constrained by multiple sources of empirical data. This approach was made possible by the emergence of large-scale, well curated databases, as well as the substantial improvements in experimental methodologies achieved over the last few years. Our results show that variability in single neuron parameters is the dominant source of functional specialization, leading to highly proficient microcircuits with much higher computational power than their homogeneous counterparts. We further show that fully heterogeneous circuits, which are closest to the biophysical reality, owe their response properties to the differential contribution of different sources of heterogeneity.},
	language = {en},
	number = {4},
	urldate = {2020-07-31},
	journal = {PLOS Computational Biology},
	author = {Duarte, Renato and Morrison, Abigail},
	month = apr,
	year = {2019},
	note = {Publisher: Public Library of Science},
	pages = {e1006781},
}

@article{adams_theories_2018,
	title = {Theories of working memory: {Differences} in definition, degree of modularity, role of attention, and purpose},
	volume = {49},
	issn = {1558-9129},
	shorttitle = {Theories of {Working} {Memory}},
	doi = {10.1044/2018_LSHSS-17-0114},
	abstract = {Purpose: The purpose of this article is to review and discuss theories of working memory with special attention to their relevance to language processing.
Method: We begin with an overview of the concept of working memory itself and review some of the major theories. Then, we show how theories of working memory can be organized according to their stances on 3 major issues that distinguish them: modularity (on a continuum from domain-general to very modular), attention (on a continuum from automatic to completely attention demanding), and purpose (on a continuum from idiographic, or concerned with individual differences, to nomothetic, or concerned with group norms). We examine recent research that has a bearing on these distinctions.
Results: Our review shows important differences between working memory theories that can be described according to positions on the 3 continua just noted.
Conclusion: Once properly understood, working memory theories, methods, and data can serve as quite useful tools for language research.},
	language = {eng},
	number = {3},
	journal = {Language, Speech, and Hearing Services in Schools},
	author = {Adams, Eryn J. and Nguyen, Anh T. and Cowan, Nelson},
	month = jul,
	year = {2018},
	pmid = {29978205},
	pmcid = {PMC6105130},
	pages = {340--355},
}

@article{rogers_primer_2020,
	title = {A {Primer} in {BERTology}: {What} we know about how {BERT} works},
	shorttitle = {A {Primer} in {BERTology}},
	url = {http://arxiv.org/abs/2002.12327},
	abstract = {Transformer-based models are now widely used in NLP, but we still do not understand a lot about their inner workings. This paper describes what is known to date about the famous BERT model (Devlin et al. 2019), synthesizing over 40 analysis studies. We also provide an overview of the proposed modifications to the model and its training regime. We then outline the directions for further research.},
	urldate = {2020-07-17},
	journal = {arXiv:2002.12327 [cs]},
	author = {Rogers, Anna and Kovaleva, Olga and Rumshisky, Anna},
	month = feb,
	year = {2020},
	note = {arXiv: 2002.12327},
}

@article{hu_systematic_2020,
	title = {A systematic assessment of syntactic generalization in neural language models},
	url = {http://arxiv.org/abs/2005.03692},
	abstract = {While state-of-the-art neural network models continue to achieve lower perplexity scores on language modeling benchmarks, it remains unknown whether optimizing for broad-coverage predictive performance leads to human-like syntactic knowledge. Furthermore, existing work has not provided a clear picture about the model properties required to produce proper syntactic generalizations. We present a systematic evaluation of the syntactic knowledge of neural language models, testing 20 combinations of model types and data sizes on a set of 34 English-language syntactic test suites. We find substantial differences in syntactic generalization performance by model architecture, with sequential models underperforming other architectures. Factorially manipulating model architecture and training dataset size (1M--40M words), we find that variability in syntactic generalization performance is substantially greater by architecture than by dataset size for the corpora tested in our experiments. Our results also reveal a dissociation between perplexity and syntactic generalization performance.},
	urldate = {2020-07-27},
	journal = {arXiv:2005.03692 [cs]},
	author = {Hu, Jennifer and Gauthier, Jon and Qian, Peng and Wilcox, Ethan and Levy, Roger P.},
	month = may,
	year = {2020},
	note = {arXiv: 2005.03692
version: 2},
}

@article{strubell_energy_2019,
	title = {Energy and policy considerations for deep learning in {NLP}},
	url = {http://arxiv.org/abs/1906.02243},
	abstract = {Recent progress in hardware and methodology for training neural networks has ushered in a new generation of large networks trained on abundant data. These models have obtained notable gains in accuracy across many NLP tasks. However, these accuracy improvements depend on the availability of exceptionally large computational resources that necessitate similarly substantial energy consumption. As a result these models are costly to train and develop, both financially, due to the cost of hardware and electricity or cloud compute time, and environmentally, due to the carbon footprint required to fuel modern tensor processing hardware. In this paper we bring this issue to the attention of NLP researchers by quantifying the approximate financial and environmental costs of training a variety of recently successful neural network models for NLP. Based on these findings, we propose actionable recommendations to reduce costs and improve equity in NLP research and practice.},
	urldate = {2020-12-15},
	journal = {arXiv:1906.02243 [cs]},
	author = {Strubell, Emma and Ganesh, Ananya and McCallum, Andrew},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.02243},
}

@article{schwartz_green_2019,
	title = {Green {AI}},
	url = {http://arxiv.org/abs/1907.10597},
	abstract = {The computations required for deep learning research have been doubling every few months, resulting in an estimated 300,000x increase from 2012 to 2018 [2]. These computations have a surprisingly large carbon footprint [38]. Ironically, deep learning was inspired by the human brain, which is remarkably energy efficient. Moreover, the financial cost of the computations can make it difficult for academics, students, and researchers, in particular those from emerging economies, to engage in deep learning research. This position paper advocates a practical solution by making efficiency an evaluation criterion for research alongside accuracy and related measures. In addition, we propose reporting the financial cost or "price tag" of developing, training, and running models to provide baselines for the investigation of increasingly efficient methods. Our goal is to make AI both greener and more inclusive---enabling any inspired undergraduate with a laptop to write high-quality research papers. Green AI is an emerging focus at the Allen Institute for AI.},
	urldate = {2020-12-15},
	journal = {arXiv:1907.10597 [cs, stat]},
	author = {Schwartz, Roy and Dodge, Jesse and Smith, Noah A. and Etzioni, Oren},
	month = aug,
	year = {2019},
	note = {arXiv: 1907.10597},
}

@article{khandelwal_sharp_2018-1,
	title = {Sharp nearby, fuzzy far away: {How} neural language models use context},
	shorttitle = {Sharp {Nearby}, {Fuzzy} {Far} {Away}},
	url = {http://arxiv.org/abs/1805.04623},
	abstract = {We know very little about how neural language models (LM) use prior linguistic context. In this paper, we investigate the role of context in an LSTM LM, through ablation studies. Specifically, we analyze the increase in perplexity when prior context words are shuffled, replaced, or dropped. On two standard datasets, Penn Treebank and WikiText-2, we find that the model is capable of using about 200 tokens of context on average, but sharply distinguishes nearby context (recent 50 tokens) from the distant history. The model is highly sensitive to the order of words within the most recent sentence, but ignores word order in the long-range context (beyond 50 tokens), suggesting the distant past is modeled only as a rough semantic field or topic. We further find that the neural caching model (Grave et al., 2017b) especially helps the LSTM to copy words from within this distant context. Overall, our analysis not only provides a better understanding of how neural LMs use their context, but also sheds light on recent success from cache-based models.},
	urldate = {2021-02-04},
	journal = {arXiv:1805.04623 [cs]},
	author = {Khandelwal, Urvashi and He, He and Qi, Peng and Jurafsky, Dan},
	month = may,
	year = {2018},
	note = {arXiv: 1805.04623},
}

@techreport{niv_primacy_2020,
	title = {The primacy of behavioral research for understanding the brain},
	url = {https://psyarxiv.com/y8mxe/},
	abstract = {Understanding the brain requires us to answer both what the brain does, and how it does it. Using a series of examples, I make the case that behavior is often more useful than neuroscientific measurements for answering the first question. Moreover, I show that even for “how” questions that pertain to neural mechanism, a well-crafted behavioral paradigm can offer deeper insight and stronger constraints on computational and mechanistic models than do many highly challenging (and very expensive) neural studies. I conclude that behavioral, rather than neuroscientific research, is essential for understanding the brain, contrary to the opinion of prominent funding bodies and scientific journals, who erroneously place neural data on a pedestal and consider behavior to be subsidiary.},
	urldate = {2020-10-26},
	institution = {PsyArXiv},
	author = {Niv, Yael},
	month = oct,
	year = {2020},
	doi = {10.31234/osf.io/y8mxe},
}

@article{norris_more_2021,
	title = {More why, less how: {What} we need from models of cognition},
	issn = {00100277},
	shorttitle = {More why, less how},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0010027721001074},
	doi = {10.1016/j.cognition.2021.104688},
	language = {en},
	urldate = {2021-03-29},
	journal = {Cognition},
	author = {Norris, Dennis and Cutler, Anne},
	month = mar,
	year = {2021},
	keywords = {explanation, connectionism, word recognition, speech recognition, cognitive psychology, Marr's levels, PDP, TRACE},
	pages = {104688},
}

@book{logie_working_2020,
	address = {New York},
	title = {Working memory: the state of the science},
	isbn = {978-0-19-884228-6},
	shorttitle = {Working memory},
	abstract = {"Working memory refers to how we keep track of what we are doing moment to moment throughout our waking lives. It allows us to remember what we have just done, focus on what we are doing now, to solve problems, be creative, think about what we will be doing in the next few seconds, and continually to update in our mind changes around us throughout the day. This book brings together in one volume, state-of-the-science chapters written by some of the most productive and well known working memory researchers worldwide. Chapters cover leading edge research on working memory, using behavioural experimental techniques, neuroimaging, computational modelling, development across the healthy human lifespan, and studies of neurodegenerative disease and focal brain damage"--},
	publisher = {Oxford University Press},
	editor = {Logie, Robert and Camos, Valérie and Cowan, Nelson},
	year = {2020},
	keywords = {working memory, LTM, STM},
}

@article{mahto_multi-timescale_2020,
	title = {Multi-timescale representation learning in {LSTM} language models},
	url = {http://arxiv.org/abs/2009.12727},
	abstract = {Although neural language models are effective at capturing statistics of natural language, their representations are challenging to interpret. In particular, it is unclear how these models retain information over multiple timescales. In this work, we construct explicitly multi-timescale language models by manipulating the input and forget gate biases in a long short-term memory (LSTM) network. The distribution of timescales is selected to approximate power law statistics of natural language through a combination of exponentially decaying memory cells. We then empirically analyze the timescale of information routed through each part of the model using word ablation experiments and forget gate visualizations. These experiments show that the multi-timescale model successfully learns representations at the desired timescales, and that the distribution includes longer timescales than a standard LSTM. Further, information about high-,mid-, and low-frequency words is routed preferentially through units with the appropriate timescales. Thus we show how to construct language models with interpretable representations of different information timescales.},
	urldate = {2021-03-17},
	journal = {arXiv:2009.12727 [cs]},
	author = {Mahto, Shivangi and Vo, Vy A. and Turek, Javier S. and Huth, Alexander G.},
	month = sep,
	year = {2020},
	note = {arXiv: 2009.12727},
}

@article{subramanian_multi-scale_2020,
	title = {Multi-scale transformer language models},
	url = {http://arxiv.org/abs/2005.00581},
	abstract = {We investigate multi-scale transformer language models that learn representations of text at multiple scales, and present three different architectures that have an inductive bias to handle the hierarchical nature of language. Experiments on large-scale language modeling benchmarks empirically demonstrate favorable likelihood vs memory footprint trade-offs, e.g. we show that it is possible to train a hierarchical variant with 30 layers that has 23\% smaller memory footprint and better perplexity, compared to a vanilla transformer with less than half the number of layers, on the Toronto BookCorpus. We analyze the advantages of learned representations at multiple scales in terms of memory footprint, compute time, and perplexity, which are particularly appealing given the quadratic scaling of transformers' run time and memory usage with respect to sequence length.},
	urldate = {2021-03-17},
	journal = {arXiv:2005.00581 [cs]},
	author = {Subramanian, Sandeep and Collobert, Ronan and Ranzato, Marc'Aurelio and Boureau, Y.-Lan},
	month = may,
	year = {2020},
	note = {arXiv: 2005.00581},
}

@article{santoro_symbolic_2021,
	title = {Symbolic behaviour in artificial intelligence},
	url = {http://arxiv.org/abs/2102.03406},
	abstract = {The ability to use symbols is the pinnacle of human intelligence, but has yet to be fully replicated in machines. Here we argue that the path towards symbolically fluent artificial intelligence (AI) begins with a reinterpretation of what symbols are, how they come to exist, and how a system behaves when it uses them. We begin by offering an interpretation of symbols as entities whose meaning is established by convention. But crucially, something is a symbol only for those who demonstrably and actively participate in this convention. We then outline how this interpretation thematically unifies the behavioural traits humans exhibit when they use symbols. This motivates our proposal that the field place a greater emphasis on symbolic behaviour rather than particular computational mechanisms inspired by more restrictive interpretations of symbols. Finally, we suggest that AI research explore social and cultural engagement as a tool to develop the cognitive machinery necessary for symbolic behaviour to emerge. This approach will allow for AI to interpret something as symbolic on its own rather than simply manipulate things that are only symbols to human onlookers, and thus will ultimately lead to AI with more human-like symbolic fluency.},
	urldate = {2021-03-17},
	journal = {arXiv:2102.03406 [cs]},
	author = {Santoro, Adam and Lampinen, Andrew and Mathewson, Kory and Lillicrap, Timothy and Raposo, David},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.03406},
	keywords = {NLP, evolution, AI, connectionism, Chinese room, ontogeny, physical symbol system hypothesis},
}

@article{desposito_cognitive_2015,
	title = {The {Cognitive} {Neuroscience} of {Working} {Memory}},
	volume = {66},
	issn = {0066-4308, 1545-2085},
	url = {http://www.annualreviews.org/doi/10.1146/annurev-psych-010814-015031},
	doi = {10.1146/annurev-psych-010814-015031},
	language = {en},
	number = {1},
	urldate = {2021-03-10},
	journal = {Annual Review of Psychology},
	author = {D'Esposito, Mark and Postle, Bradley R.},
	month = jan,
	year = {2015},
	pages = {115--142},
	file = {Accepted Version:/Users/kriarm/Zotero/storage/BE5Q24P3/D'Esposito and Postle - 2015 - The Cognitive Neuroscience of Working Memory.pdf:application/pdf},
}

@article{futrell_rnns_2018,
	title = {{RNNs} as psycholinguistic subjects: {Syntactic} state and grammatical dependency},
	shorttitle = {{RNNs} as psycholinguistic subjects},
	url = {http://arxiv.org/abs/1809.01329},
	abstract = {Recurrent neural networks (RNNs) are the state of the art in sequence modeling for natural language. However, it remains poorly understood what grammatical characteristics of natural language they implicitly learn and represent as a consequence of optimizing the language modeling objective. Here we deploy the methods of controlled psycholinguistic experimentation to shed light on to what extent RNN behavior reflects incremental syntactic state and grammatical dependency representations known to characterize human linguistic behavior. We broadly test two publicly available long short-term memory (LSTM) English sequence models, and learn and test a new Japanese LSTM. We demonstrate that these models represent and maintain incremental syntactic state, but that they do not always generalize in the same way as humans. Furthermore, none of our models learn the appropriate grammatical dependency configurations licensing reflexive pronouns or negative polarity items.},
	urldate = {2021-03-09},
	journal = {arXiv:1809.01329 [cs]},
	author = {Futrell, Richard and Wilcox, Ethan and Morita, Takashi and Levy, Roger},
	month = sep,
	year = {2018},
	note = {arXiv: 1809.01329},
	keywords = {RNN},
}

@article{franklin_structured_2020,
	title = {Structured {Event} {Memory}: {A} neuro-symbolic model of event cognition.},
	volume = {127},
	issn = {1939-1471, 0033-295X},
	shorttitle = {Structured {Event} {Memory}},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/rev0000177},
	doi = {10.1037/rev0000177},
	language = {en},
	number = {3},
	urldate = {2021-03-09},
	journal = {Psychological Review},
	author = {Franklin, Nicholas T. and Norman, Kenneth A. and Ranganath, Charan and Zacks, Jeffrey M. and Gershman, Samuel J.},
	month = apr,
	year = {2020},
	keywords = {episodic memory, event cognition},
	pages = {327--361},
}

@article{farrell_temporal_2012,
	title = {Temporal clustering and sequencing in short-term memory and episodic memory.},
	volume = {119},
	issn = {1939-1471, 0033-295X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0027371},
	doi = {10.1037/a0027371},
	language = {en},
	number = {2},
	urldate = {2021-03-09},
	journal = {Psychological Review},
	author = {Farrell, Simon},
	year = {2012},
	keywords = {episodic memory, short-term memory, chunking, clustering},
	pages = {223--271},
}

@article{kahana_computational_2020,
	title = {Computational models of memory search},
	volume = {71},
	issn = {0066-4308, 1545-2085},
	url = {https://www.annualreviews.org/doi/10.1146/annurev-psych-010418-103358},
	doi = {10.1146/annurev-psych-010418-103358},
	abstract = {The capacity to search memory for events learned in a particular context stands as one of the most remarkable feats of the human brain. How is memory search accomplished? First, I review the central ideas investigated by theorists developing models of memory. Then, I review select benchmark findings concerning memory search and analyze two influential computational approaches to modeling memory search: dual-store theory and retrieved context theory. Finally, I discuss the key theoretical ideas that have emerged from these modeling studies and the open questions that need to be answered by future research.},
	language = {en},
	number = {1},
	urldate = {2021-03-08},
	journal = {Annual Review of Psychology},
	author = {Kahana, Michael J.},
	month = jan,
	year = {2020},
	keywords = {free recall, immediate recall, memory search, serial recall},
	pages = {107--138},
}

@article{miller_library_2019,
	title = {A library of human electrocorticographic data and analyses},
	volume = {3},
	copyright = {2019 The Author(s), under exclusive licence to Springer Nature Limited},
	issn = {2397-3374},
	url = {http://www.nature.com/articles/s41562-019-0678-3},
	doi = {10.1038/s41562-019-0678-3},
	abstract = {Electrophysiological data from implanted electrodes in the human brain are rare, and therefore scientific access to such data has remained somewhat exclusive. Here we present a freely available curated library of implanted electrocorticographic data and analyses for 16 behavioural experiments, with 204 individual datasets from 34 patients recorded with the same amplifiers and at the same settings. For each dataset, electrode positions were carefully registered to brain anatomy. A large set of fully annotated analysis scripts with which to interpret these data is embedded in the library alongside them. All data, anatomical locations and analysis files (MATLAB code) are provided in a shared file structure at https://searchworks.stanford.edu/view/zk881ps0522.},
	language = {en},
	number = {11},
	urldate = {2021-03-04},
	journal = {Nature Human Behaviour},
	author = {Miller, Kai J.},
	month = nov,
	year = {2019},
	note = {Number: 11
Publisher: Nature Publishing Group},
	pages = {1225--1235},
}

@article{floridi_gpt-3_2020,
	title = {{GPT}-3: {Its} nature, scope, limits, and consequences},
	issn = {0924-6495, 1572-8641},
	shorttitle = {{GPT}-3},
	url = {http://link.springer.com/10.1007/s11023-020-09548-1},
	doi = {10.1007/s11023-020-09548-1},
	language = {en},
	urldate = {2020-11-05},
	journal = {Minds and Machines},
	author = {Floridi, Luciano and Chiriatti, Massimo},
	month = nov,
	year = {2020},
	keywords = {deep learning, GPT-3, NLP, LM, ethics, text generation},
}

@article{ho_moving_2019,
	title = {Moving beyond {P} values: data analysis with estimation graphics},
	volume = {16},
	copyright = {2019 The Author(s), under exclusive licence to Springer Nature America, Inc.},
	issn = {1548-7105},
	shorttitle = {Moving beyond {P} values},
	url = {http://www.nature.com/articles/s41592-019-0470-3},
	doi = {10.1038/s41592-019-0470-3},
	language = {en},
	number = {7},
	urldate = {2021-03-02},
	journal = {Nature Methods},
	author = {Ho, Joses and Tumkaya, Tayfun and Aryal, Sameer and Choi, Hyungwon and Claridge-Chang, Adam},
	month = jul,
	year = {2019},
	note = {Number: 7
Publisher: Nature Publishing Group},
	keywords = {statistics, inference, bootstrap, CI, estimation, NHST},
	pages = {565--566},
}

@article{desposito_cognitive_2015-1,
	title = {The cognitive neuroscience of working memory},
	volume = {66},
	url = {https://doi.org/10.1146/annurev-psych-010814-015031},
	doi = {10.1146/annurev-psych-010814-015031},
	abstract = {For more than 50 years, psychologists and neuroscientists have recognized the importance of a working memory to coordinate processing when multiple goals are active and to guide behavior with information that is not present in the immediate environment. In recent years, psychological theory and cognitive neuroscience data have converged on the idea that information is encoded into working memory by allocating attention to internal representations, whether semantic long-term memory (e.g., letters, digits, words), sensory, or motoric. Thus, information-based multivariate analyses of human functional MRI data typically find evidence for the temporary representation of stimuli in regions that also process this information in nonworking memory contexts. The prefrontal cortex (PFC), on the other hand, exerts control over behavior by biasing the salience of mnemonic representations and adjudicating among competing, context-dependent rules. The “control of the controller” emerges from a complex interplay between PFC and striatal circuits and ascending dopaminergic neuromodulatory signals.},
	number = {1},
	urldate = {2021-02-24},
	journal = {Annual Review of Psychology},
	author = {D'Esposito, Mark and Postle, Bradley R.},
	year = {2015},
	pmid = {25251486},
	note = {\_eprint: https://doi.org/10.1146/annurev-psych-010814-015031},
	pages = {115--142},
}

@article{eriksson_neurocognitive_2015,
	title = {Neurocognitive architecture of working memory},
	volume = {88},
	issn = {08966273},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627315007771},
	doi = {10.1016/j.neuron.2015.09.020},
	language = {en},
	number = {1},
	urldate = {2021-02-24},
	journal = {Neuron},
	author = {Eriksson, Johan and Vogel, Edward K. and Lansner, Anders and Bergström, Fredrik and Nyberg, Lars},
	month = oct,
	year = {2015},
	pages = {33--46},
}

@article{ma_changing_2014,
	title = {Changing concepts of working memory},
	volume = {17},
	issn = {1546-1726},
	url = {http://www.nature.com/articles/nn.3655},
	doi = {10.1038/nn.3655},
	abstract = {Working memory is thought to be limited in capacity, holding a fixed, small number of items, but it has recently been proposed that working memory might be conceptualized as a limited resource that is distributed flexibly between all items to be maintained in memory. In this review, the authors consider emerging evidence for this proposal.},
	language = {en},
	number = {3},
	urldate = {2021-02-24},
	journal = {Nature Neuroscience},
	author = {Ma, Wei Ji and Husain, Masud and Bays, Paul M.},
	month = mar,
	year = {2014},
	note = {Number: 3
Publisher: Nature Publishing Group},
	pages = {347--356},
	file = {Full Text PDF:/Users/kriarm/Zotero/storage/YLHADXJA/Ma et al. - 2014 - Changing concepts of working memory.pdf:application/pdf;Snapshot:/Users/kriarm/Zotero/storage/GDZ93ZAD/nn.html:text/html},
}

@article{hutson_who_2021,
	title = {Who should stop unethical {A}.{I}.?},
	url = {https://www.newyorker.com/tech/annals-of-technology/who-should-stop-unethical-ai},
	abstract = {At artificial-intelligence conferences, researchers are increasingly alarmed by what they see.},
	language = {en-us},
	urldate = {2021-02-22},
	journal = {The New Yorker},
	author = {Hutson, Matthew},
	month = feb,
	year = {2021},
	file = {Snapshot:/Users/kriarm/Zotero/storage/WLUAUXSX/who-should-stop-unethical-ai.html:text/html},
}

@article{beukers_is_2021,
	title = {Is activity silent working memory simply episodic memory?},
	issn = {13646613},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S136466132100005X},
	doi = {10.1016/j.tics.2021.01.003},
	language = {en},
	urldate = {2021-02-16},
	journal = {Trends in Cognitive Sciences},
	author = {Beukers, Andre O. and Buschman, Timothy J. and Cohen, Jonathan D. and Norman, Kenneth A.},
	month = feb,
	year = {2021},
	pages = {S136466132100005X},
}

@article{sahlgren_singleton_2021,
	title = {The singleton fallacy: {Why} current critiques of language models miss the point},
	shorttitle = {The {Singleton} {Fallacy}},
	url = {http://arxiv.org/abs/2102.04310},
	abstract = {This paper discusses the current critique against neural network-based Natural Language Understanding (NLU) solutions known as language models. We argue that much of the current debate rests on an argumentation error that we will refer to as the singleton fallacy: the assumption that language, meaning, and understanding are single and uniform phenomena that are unobtainable by (current) language models. By contrast, we will argue that there are many different types of language use, meaning and understanding, and that (current) language models are build with the explicit purpose of acquiring and representing one type of structural understanding of language. We will argue that such structural understanding may cover several different modalities, and as such can handle several different types of meaning. Our position is that we currently see no theoretical reason why such structural knowledge would be insufficient to count as "real" understanding.},
	urldate = {2021-02-13},
	journal = {arXiv:2102.04310 [cs]},
	author = {Sahlgren, Magnus and Carlsson, Fredrik},
	month = feb,
	year = {2021},
	note = {arXiv: 2102.04310},
	keywords = {NLP, meaning, chinese room, Dennett, intention, intentional stance, NLU, pragmatics, Sears},
}

@article{french_catastrophic_1999,
	title = {Catastrophic forgetting in connectionist networks},
	volume = {3},
	issn = {1364-6613},
	url = {https://www.sciencedirect.com/science/article/pii/S1364661399012942},
	doi = {10.1016/S1364-6613(99)01294-2},
	abstract = {All natural cognitive systems, and, in particular, our own, gradually forget previously learned information. Plausible models of human cognition should therefore exhibit similar patterns of gradual forgetting of old information as new information is acquired. Only rarely does new learning in natural cognitive systems completely disrupt or erase previously learned information; that is, natural cognitive systems do not, in general, forget ‘catastrophically’. Unfortunately, though, catastrophic forgetting does occur under certain circumstances in distributed connectionist networks. The very features that give these networks their remarkable abilities to generalize, to function in the presence of degraded input, and so on, are found to be the root cause of catastrophic forgetting. The challenge in this field is to discover how to keep the advantages of distributed connectionist networks while avoiding the problem of catastrophic forgetting. In this article the causes, consequences and numerous solutions to the problem of catastrophic forgetting in neural networks are examined. The review will consider how the brain might have overcome this problem and will also explore the consequences of this solution for distributed connectionist networks.},
	language = {en},
	number = {4},
	urldate = {2021-02-08},
	journal = {Trends in Cognitive Sciences},
	author = {French, Robert M.},
	month = apr,
	year = {1999},
	keywords = {connectionism, catastrophic forgetting},
	pages = {128--135},
}

@article{frank_sentence_2011,
	title = {Sentence comprehension as mental simulation: {An} information-theoretic perspective},
	volume = {2},
	issn = {2078-2489},
	shorttitle = {Sentence {Comprehension} as {Mental} {Simulation}},
	url = {http://www.mdpi.com/2078-2489/2/4/672},
	doi = {10.3390/info2040672},
	language = {en},
	number = {4},
	urldate = {2021-02-08},
	journal = {Information},
	author = {Frank, Stefan L. and Vigliocco, Gabriella},
	month = nov,
	year = {2011},
	pages = {672--696},
}

@article{pado_probabilistic_2009,
	title = {A probabilistic model of semantic plausibility in sentence processing},
	volume = {33},
	issn = {03640213},
	url = {http://doi.wiley.com/10.1111/j.1551-6709.2009.01033.x},
	doi = {10.1111/j.1551-6709.2009.01033.x},
	language = {en},
	number = {5},
	urldate = {2021-02-08},
	journal = {Cognitive Science},
	author = {Padó, Ulrike and Crocker, Matthew W. and Keller, Frank},
	month = jul,
	year = {2009},
	pages = {794--838},
	file = {Full Text:/Users/kriarm/Zotero/storage/E4J8IFV6/Padó et al. - 2009 - A Probabilistic Model of Semantic Plausibility in .pdf:application/pdf},
}

@article{baddeley_working_2003,
	title = {Working memory: looking back and looking forward},
	volume = {4},
	issn = {1471-003X, 1471-0048},
	shorttitle = {Working memory},
	url = {http://www.nature.com/articles/nrn1201},
	doi = {10.1038/nrn1201},
	language = {en},
	number = {10},
	urldate = {2021-02-05},
	journal = {Nature Reviews Neuroscience},
	author = {Baddeley, Alan},
	month = oct,
	year = {2003},
	keywords = {working memory},
	pages = {829--839},
}

@article{miller_magical_1956,
	title = {The magical number seven plus or minus two:  some limits on our capacity for processing information},
	volume = {63},
	issn = {0033-295X},
	shorttitle = {The magical number seven plus or minus two},
	language = {eng},
	number = {2},
	journal = {Psychological Review},
	author = {Miller, G. A.},
	month = mar,
	year = {1956},
	pmid = {13310704},
	keywords = {working memory, serial recall},
	pages = {81--97},
}

@article{cowan_magical_2001,
	title = {The magical number 4 in short-term memory: {A} reconsideration of mental storage capacity},
	volume = {24},
	issn = {0140-525X, 1469-1825},
	shorttitle = {The magical number 4 in short-term memory},
	url = {https://www.cambridge.org/core/product/identifier/S0140525X01003922/type/journal_article},
	doi = {10.1017/S0140525X01003922},
	language = {en},
	number = {1},
	urldate = {2021-02-05},
	journal = {Behavioral and Brain Sciences},
	author = {Cowan, Nelson},
	month = feb,
	year = {2001},
	keywords = {working memory, cognitive science},
	pages = {87--114},
}

@article{lewis_computational_2006,
	title = {Computational principles of working memory in sentence comprehension},
	volume = {10},
	issn = {13646613},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661306002142},
	doi = {10.1016/j.tics.2006.08.007},
	language = {en},
	number = {10},
	urldate = {2021-02-04},
	journal = {Trends in Cognitive Sciences},
	author = {Lewis, Richard L. and Vasishth, Shravan and Van Dyke, Julie A.},
	month = oct,
	year = {2006},
	pages = {447--454},
}

@article{just_capacity_1992,
	title = {A capacity theory of comprehension: {Individual} differences in working memory.},
	volume = {99},
	issn = {1939-1471, 0033-295X},
	shorttitle = {A capacity theory of comprehension},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-295X.99.1.122},
	doi = {10.1037/0033-295X.99.1.122},
	language = {en},
	number = {1},
	urldate = {2021-02-04},
	journal = {Psychological Review},
	author = {Just, Marcel A. and Carpenter, Patricia A.},
	year = {1992},
	pages = {122--149},
}

@article{macdonald_reassessing_2002,
	title = {Reassessing working memory: {Comment} on {Just} and {Carpenter} (1992) and {Waters} and {Caplan} (1996).},
	volume = {109},
	issn = {0033-295X},
	shorttitle = {Reassessing working memory},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-295X.109.1.35},
	doi = {10.1037//0033-295X.109.1.35},
	language = {en},
	number = {1},
	urldate = {2021-01-26},
	journal = {Psychological Review},
	author = {MacDonald, Maryellen C. and Christiansen, Morten H.},
	year = {2002},
	pages = {35--54},
}

@inproceedings{wolf_transformers_2020,
	address = {Online},
	title = {Transformers: {State}-of-the-{Art} natural language processing},
	shorttitle = {Transformers},
	url = {https://www.aclweb.org/anthology/2020.emnlp-demos.6},
	doi = {10.18653/v1/2020.emnlp-demos.6},
	abstract = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers.},
	urldate = {2021-01-26},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}: {System} {Demonstrations}},
	publisher = {Association for Computational Linguistics},
	author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Remi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and von Platen, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Le Scao, Teven and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander},
	month = oct,
	year = {2020},
	keywords = {NLP, BERT, GPT-2, transformer},
	pages = {38--45},
}

@article{altmann_incrementality_2009,
	title = {Incrementality and prediction in human sentence processing},
	volume = {33},
	issn = {03640213, 15516709},
	url = {http://doi.wiley.com/10.1111/j.1551-6709.2009.01022.x},
	doi = {10.1111/j.1551-6709.2009.01022.x},
	language = {en},
	number = {4},
	urldate = {2020-05-24},
	journal = {Cognitive Science},
	author = {Altmann, Gerry T. M. and Mirković, Jelena},
	month = jun,
	year = {2009},
	pages = {583--609},
}

@article{efrat_turking_2020,
	title = {The turking test: can language models understand instructions?},
	shorttitle = {The {Turking} {Test}},
	url = {http://arxiv.org/abs/2010.11982},
	abstract = {Supervised machine learning provides the learner with a set of input-output examples of the target task. Humans, however, can also learn to perform new tasks from instructions in natural language. Can machines learn to understand instructions as well? We present the Turking Test, which examines a model's ability to follow natural language instructions of varying complexity. These range from simple tasks, like retrieving the nth word of a sentence, to ones that require creativity, such as generating examples for SNLI and SQuAD in place of human intelligence workers ("turkers"). Despite our lenient evaluation methodology, we observe that a large pretrained language model performs poorly across all tasks. Analyzing the model's error patterns reveals that the model tends to ignore explicit instructions and often generates outputs that cannot be construed as an attempt to solve the task. While it is not yet clear whether instruction understanding can be captured by traditional language models, the sheer expressivity of instruction understanding makes it an appealing alternative to the rising few-shot inference paradigm.},
	urldate = {2020-10-27},
	journal = {arXiv:2010.11982 [cs]},
	author = {Efrat, Avia and Levy, Omer},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.11982},
}

@article{ma_neural_2020,
	title = {A neural network walks into a lab: towards using deep nets as models for human behavior},
	shorttitle = {A neural network walks into a lab},
	url = {http://arxiv.org/abs/2005.02181},
	abstract = {What might sound like the beginning of a joke has become an attractive prospect for many cognitive scientists: the use of deep neural network models (DNNs) as models of human behavior in perceptual and cognitive tasks. Although DNNs have taken over machine learning, attempts to use them as models of human behavior are still in the early stages. Can they become a versatile model class in the cognitive scientist's toolbox? We first argue why DNNs have the potential to be interesting models of human behavior. We then discuss how that potential can be more fully realized. On the one hand, we argue that the cycle of training, testing, and revising DNNs needs to be revisited through the lens of the cognitive scientist's goals. Specifically, we argue that methods for assessing the goodness of fit between DNN models and human behavior have to date been impoverished. On the other hand, cognitive science might have to start using more complex tasks (including richer stimulus spaces), but doing so might be beneficial for DNN-independent reasons as well. Finally, we highlight avenues where traditional cognitive process models and DNNs may show productive synergy.},
	urldate = {2020-10-07},
	journal = {arXiv:2005.02181 [cs, q-bio]},
	author = {Ma, Wei Ji and Peters, Benjamin},
	month = may,
	year = {2020},
	note = {arXiv: 2005.02181},
}

@article{rae_transformers_2020,
	title = {Do transformers need deep long-range memory},
	url = {http://arxiv.org/abs/2007.03356},
	abstract = {Deep attention models have advanced the modelling of sequential data across many domains. For language modelling in particular, the Transformer-XL -- a Transformer augmented with a long-range memory of past activations -- has been shown to be state-of-the-art across a variety of well-studied benchmarks. The Transformer-XL incorporates a long-range memory at every layer of the network, which renders its state to be thousands of times larger than RNN predecessors. However it is unclear whether this is necessary. We perform a set of interventions to show that comparable performance can be obtained with 6X fewer long range memories and better performance can be obtained by limiting the range of attention in lower layers of the network.},
	urldate = {2020-09-21},
	journal = {arXiv:2007.03356 [cs, stat]},
	author = {Rae, Jack W. and Razavi, Ali},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.03356},
}

@article{vig_analyzing_2019,
	title = {Analyzing the structure of attention in a {Transformer} language model},
	url = {http://arxiv.org/abs/1906.04284},
	abstract = {The Transformer is a fully attention-based alternative to recurrent networks that has achieved state-of-the-art results across a range of NLP tasks. In this paper, we analyze the structure of attention in a Transformer language model, the GPT-2 small pretrained model. We visualize attention for individual instances and analyze the interaction between attention and syntax over a large corpus. We find that attention targets different parts of speech at different layer depths within the model, and that attention aligns with dependency relations most strongly in the middle layers. We also find that the deepest layers of the model capture the most distant relationships. Finally, we extract exemplar sentences that reveal highly specific patterns targeted by particular attention heads.},
	urldate = {2020-09-24},
	journal = {arXiv:1906.04284 [cs, stat]},
	author = {Vig, Jesse and Belinkov, Yonatan},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.04284},
}

@misc{lacker_giving_2020,
	title = {Giving {GPT}-3 a {Turing} {Test}},
	url = {https://lacker.io/ai/2020/07/06/giving-gpt-3-a-turing-test.html},
	urldate = {2020-12-15},
	author = {Lacker, Kevin},
	month = jul,
	year = {2020},
	file = {Giving GPT-3 a Turing Test:/Users/kriarm/Zotero/storage/DIZSEEIY/giving-gpt-3-a-turing-test.html:text/html},
}

@misc{porr_feeling_2020,
	title = {Feeling unproductive? {Maybe} you should stop overthinking.},
	shorttitle = {Feeling unproductive?},
	url = {https://adolos.substack.com/p/feeling-unproductive-maybe-you-should},
	abstract = {In order to get something done, maybe we need to think less. Seems counter-intuitive, but I believe sometimes our thoughts can get in the way of the creative process. We can work better at times when we "tune out" the external world and focus on what's in front of us.},
	urldate = {2020-12-15},
	author = {Porr, Liam},
	month = jul,
	year = {2020},
	file = {Snapshot:/Users/kriarm/Zotero/storage/T6IMPPKM/feeling-unproductive-maybe-you-should.html:text/html},
}

@misc{rousseau_doctor_2020,
	title = {Doctor {GPT}-3: hype or reality?},
	shorttitle = {Doctor {GPT}-3},
	url = {https://nabla.com/blog/gpt-3/},
	abstract = {Our unique multidisciplinary team of doctors and machine learning engineers at Nabla had the chance to test this new model to tease apart what’s real and what’s hype in terms of different healthcare use cases.},
	language = {en},
	urldate = {2020-12-15},
	journal = {Nabla},
	author = {Rousseau, Anne-Laure and Baudelaire, Clément and Riera, Kevin},
	month = oct,
	year = {2020},
	note = {Section: Santé},
	keywords = {GPT-3},
	file = {Snapshot:/Users/kriarm/Zotero/storage/ZNCGARL2/gpt-3.html:text/html},
}

@article{gpt-3_robot_2020,
	chapter = {Opinion},
	title = {A robot wrote this entire article. {Are} you scared yet, human?},
	shorttitle = {A robot wrote this entire article. {Are} you scared yet, human?},
	url = {http://www.theguardian.com/commentisfree/2020/sep/08/robot-wrote-this-article-gpt-3},
	abstract = {We asked GPT-3, OpenAI’s powerful new language generator, to write an essay for us from scratch. The assignment? To convince us robots come in peace},
	language = {en},
	urldate = {2020-12-15},
	journal = {the Guardian},
	author = {GPT-3},
	month = sep,
	year = {2020},
	file = {Snapshot:/Users/kriarm/Zotero/storage/6AUDNWCQ/robot-wrote-this-article-gpt-3.html:text/html},
}

@misc{hao_college_2020,
	title = {A college kid created a fake, {AI}-generated blog. {It} reached \#1 on {Hacker} {News}.},
	url = {https://www.technologyreview.com/2020/08/14/1006780/ai-gpt-3-fake-blog-reached-top-of-hacker-news/},
	abstract = {“It was super easy actually,” he says, “which was the scary part.”},
	language = {en},
	urldate = {2020-12-15},
	journal = {MIT Technology Review},
	author = {Hao, Karen},
	month = aug,
	year = {2020},
	file = {Snapshot:/Users/kriarm/Zotero/storage/IM7G3LPU/ai-gpt-3-fake-blog-reached-top-of-hacker-news.html:text/html},
}

@article{hao_we_2020,
	title = {We read the paper that forced {Timnit} {Gebru} out of {Google}. {Here}’s what it says.},
	url = {https://www.technologyreview.com/2020/12/04/1013294/google-ai-ethics-research-paper-forced-out-timnit-gebru/},
	abstract = {The company's star ethics researcher highlighted the risks of large language models, which are key to Google's business.},
	language = {en},
	urldate = {2020-12-15},
	journal = {MIT Technology Review},
	author = {Hao, Karen},
	month = dec,
	year = {2020},
	file = {Snapshot:/Users/kriarm/Zotero/storage/EC35KDFY/google-ai-ethics-research-paper-forced-out-timnit-gebru.html:text/html},
}

@article{chollet_measure_2019,
	title = {On the measure of intelligence},
	url = {http://arxiv.org/abs/1911.01547},
	abstract = {To make deliberate progress towards more intelligent and more human-like artificial systems, we need to be following an appropriate feedback signal: we need to be able to define and evaluate intelligence in a way that enables comparisons between two systems, as well as comparisons with humans. Over the past hundred years, there has been an abundance of attempts to define and measure intelligence, across both the fields of psychology and AI. We summarize and critically assess these definitions and evaluation approaches, while making apparent the two historical conceptions of intelligence that have implicitly guided them. We note that in practice, the contemporary AI community still gravitates towards benchmarking intelligence by comparing the skill exhibited by AIs and humans at specific tasks such as board games and video games. We argue that solely measuring skill at any given task falls short of measuring intelligence, because skill is heavily modulated by prior knowledge and experience: unlimited priors or unlimited training data allow experimenters to "buy" arbitrary levels of skills for a system, in a way that masks the system's own generalization power. We then articulate a new formal definition of intelligence based on Algorithmic Information Theory, describing intelligence as skill-acquisition efficiency and highlighting the concepts of scope, generalization difficulty, priors, and experience. Using this definition, we propose a set of guidelines for what a general AI benchmark should look like. Finally, we present a benchmark closely following these guidelines, the Abstraction and Reasoning Corpus (ARC), built upon an explicit set of priors designed to be as close as possible to innate human priors. We argue that ARC can be used to measure a human-like form of general fluid intelligence and that it enables fair general intelligence comparisons between AI systems and humans.},
	urldate = {2020-12-15},
	journal = {arXiv:1911.01547 [cs]},
	author = {Chollet, François},
	month = nov,
	year = {2019},
	note = {arXiv: 1911.01547},
	keywords = {artificial intelligence, artificial general intelligence},
}

@article{marcus_gpt-3_2020,
	title = {{GPT}-3, {Bloviator}: {OpenAI}’s language generator has no idea what it’s talking about},
	shorttitle = {{GPT}-3, {Bloviator}},
	url = {https://www.technologyreview.com/2020/08/22/1007539/gpt3-openai-language-generator-artificial-intelligence-ai-opinion/},
	abstract = {Since OpenAI first described its new AI language-generating system called GPT-3 in May, hundreds of media outlets (including MIT Technology Review) have written about the system and its capabilities. Twitter has been abuzz about its power and potential. The New York Times published an op-ed about it. Later this year, OpenAI will begin charging companies…},
	language = {en},
	urldate = {2020-12-15},
	journal = {MIT Technology Review},
	author = {Marcus, Gary and Davis, Ernest},
	month = aug,
	year = {2020},
	file = {Snapshot:/Users/kriarm/Zotero/storage/5NNPYRPW/gpt3-openai-language-generator-artificial-intelligence-ai-opinion.html:text/html},
}

@article{heaven_openais_2020,
	title = {{OpenAI}’s new language generator {GPT}-3 is shockingly good—and completely mindless},
	url = {https://www.technologyreview.com/2020/07/20/1005454/openai-machine-learning-language-generator-gpt-3-nlp/},
	abstract = {The AI is the largest language model ever created and can generate amazing human-like text on demand but won't bring us closer to true intelligence.},
	language = {en},
	urldate = {2020-12-15},
	journal = {MIT Technology Review},
	author = {Heaven, Will Douglas},
	month = jul,
	year = {2020},
	file = {Snapshot:/Users/kriarm/Zotero/storage/3B2QZ6FT/openai-machine-learning-language-generator-gpt-3-nlp.html:text/html},
}

@article{holtzman_curious_2020-1,
	title = {The curious case of neural text degeneration},
	url = {http://arxiv.org/abs/1904.09751},
	abstract = {Despite considerable advancements with deep neural language models, the enigma of neural text degeneration persists when these models are tested as text generators. The counter-intuitive empirical observation is that even though the use of likelihood as training objective leads to high quality models for a broad range of language understanding tasks, using likelihood as a decoding objective leads to text that is bland and strangely repetitive. In this paper, we reveal surprising distributional differences between human text and machine text. In addition, we find that decoding strategies alone can dramatically effect the quality of machine text, even when generated from exactly the same neural language model. Our findings motivate Nucleus Sampling, a simple but effective method to draw the best out of neural generation. By sampling text from the dynamic nucleus of the probability distribution, which allows for diversity while effectively truncating the less reliable tail of the distribution, the resulting text better demonstrates the quality of human text, yielding enhanced diversity without sacrificing fluency and coherence.},
	urldate = {2020-12-03},
	journal = {arXiv:1904.09751 [cs]},
	author = {Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
	month = feb,
	year = {2020},
	note = {arXiv: 1904.09751},
	keywords = {GPT-2, natural language generation, NLG, beam search, greedy search, nucleus sampling, top-k},
}

@article{norris_short-term_2017,
	title = {Short-term memory and long-term memory are still different.},
	volume = {143},
	issn = {1939-1455, 0033-2909},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/bul0000108},
	doi = {10.1037/bul0000108},
	language = {en},
	number = {9},
	urldate = {2020-12-02},
	journal = {Psychological Bulletin},
	author = {Norris, Dennis},
	month = sep,
	year = {2017},
	keywords = {working memory, memory, short-term memory, long-term memory},
	pages = {992--1009},
}

@misc{noauthor_short-term_nodate,
	title = {Short-term memory and long-term memory are still different.},
	url = {https://doi.apa.org/fulltext/2017-22544-001.html},
	urldate = {2020-12-02},
	file = {Short-term memory and long-term memory are still different.:/Users/kriarm/Zotero/storage/DRAQJMCR/2017-22544-001.html:text/html},
}

@incollection{cowan_what_2008,
	title = {What are the differences between long-term, short-term, and working memory?},
	volume = {169},
	isbn = {978-0-444-53164-3},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0079612307000209},
	language = {en},
	urldate = {2020-12-02},
	booktitle = {Progress in {Brain} {Research}},
	publisher = {Elsevier},
	author = {Cowan, Nelson},
	year = {2008},
	doi = {10.1016/S0079-6123(07)00020-9},
	keywords = {working memory, short-term memory, long term memory},
	pages = {323--338},
	file = {Accepted Version:/Users/kriarm/Zotero/storage/H8TINKQW/Cowan - 2008 - Chapter 20 What are the differences between long-t.pdf:application/pdf},
}

@book{miyake_models_1999,
	address = {Cambridge ; New York},
	title = {Models of working memory: mechanisms of active maintenance and executive control},
	isbn = {978-0-521-58325-1 978-0-521-58721-1},
	shorttitle = {Models of working memory},
	publisher = {Cambridge University Press},
	editor = {Miyake, Akira and Shah, Priti},
	year = {1999},
	keywords = {working memory, cognitive modeling, review, short-term memory, long term memory},
}

@article{ettinger_what_2020,
	title = {What {BERT} {Is} {Not}: {Lessons} from a new suite of psycholinguistic diagnostics for language models},
	volume = {8},
	shorttitle = {What {BERT} {Is} {Not}},
	url = {https://doi.org/10.1162/tacl_a_00298},
	doi = {10.1162/tacl_a_00298},
	abstract = {Pre-training by language modeling has become a popular and successful approach to NLP tasks, but we have yet to understand exactly what linguistic capacities these pre-training processes confer upon models. In this paper we introduce a suite of diagnostics drawn from human language experiments, which allow us to ask targeted questions about information used by language models for generating predictions in context. As a case study, we apply these diagnostics to the popular BERT model, finding that it can generally distinguish good from bad completions involving shared category or role reversal, albeit with less sensitivity than humans, and it robustly retrieves noun hypernyms, but it struggles with challenging inference and role-based event prediction— and, in particular, it shows clear insensitivity to the contextual impacts of negation.},
	urldate = {2020-11-13},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Ettinger, Allyson},
	month = jan,
	year = {2020},
	note = {Publisher: MIT Press},
	keywords = {NLP, BERT, N400, MLM},
	pages = {34--48},
}

@article{schrimpf_integrative_2020,
	title = {Integrative benchmarking to advance neurally mechanistic models of human intelligence},
	volume = {108},
	issn = {0896-6273},
	url = {https://www.cell.com/neuron/abstract/S0896-6273(20)30605-X},
	doi = {10.1016/j.neuron.2020.07.040},
	language = {English},
	number = {3},
	urldate = {2020-11-13},
	journal = {Neuron},
	author = {Schrimpf, Martin and Kubilius, Jonas and Lee, Michael J. and Murty, N. Apurva Ratan and Ajemian, Robert and DiCarlo, James J.},
	month = nov,
	year = {2020},
	pmid = {32918861},
	note = {Publisher: Elsevier},
	keywords = {benchmark},
	pages = {413--423},
}

@article{oberauer_benchmarks_2018,
	title = {Benchmarks for models of short-term and working memory.},
	volume = {144},
	issn = {1939-1455, 0033-2909},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/bul0000153},
	doi = {10.1037/bul0000153},
	language = {en},
	number = {9},
	urldate = {2020-11-10},
	journal = {Psychological Bulletin},
	author = {Oberauer, Klaus and Lewandowsky, Stephan and Awh, Edward and Brown, Gordon D. A. and Conway, Andrew and Cowan, Nelson and Donkin, Christopher and Farrell, Simon and Hitch, Graham J. and Hurlstone, Mark J. and Ma, Wei Ji and Morey, Candice C. and Nee, Derek Evan and Schweppe, Judith and Vergauwe, Evie and Ward, Geoff},
	month = sep,
	year = {2018},
	keywords = {benchmark, working memory, short-term memory},
	pages = {885--958},
}

@article{petroni_language_2019,
	title = {Language models as knowledge bases?},
	url = {http://arxiv.org/abs/1909.01066},
	abstract = {Recent progress in pretraining language models on large textual corpora led to a surge of improvements for downstream NLP tasks. Whilst learning linguistic knowledge, these models may also be storing relational knowledge present in the training data, and may be able to answer queries structured as "fill-in-the-blank" cloze statements. Language models have many advantages over structured knowledge bases: they require no schema engineering, allow practitioners to query about an open class of relations, are easy to extend to more data, and require no human supervision to train. We present an in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-the-art pretrained language models. We find that (i) without fine-tuning, BERT contains relational knowledge competitive with traditional NLP methods that have some access to oracle knowledge, (ii) BERT also does remarkably well on open-domain question answering against a supervised baseline, and (iii) certain types of factual knowledge are learned much more readily than others by standard language model pretraining approaches. The surprisingly strong ability of these models to recall factual knowledge without any fine-tuning demonstrates their potential as unsupervised open-domain QA systems. The code to reproduce our analysis is available at https://github.com/facebookresearch/LAMA.},
	urldate = {2020-11-03},
	journal = {arXiv:1909.01066 [cs]},
	author = {Petroni, Fabio and Rocktäschel, Tim and Lewis, Patrick and Bakhtin, Anton and Wu, Yuxiang and Miller, Alexander H. and Riedel, Sebastian},
	month = sep,
	year = {2019},
	note = {arXiv: 1909.01066},
	keywords = {NLP, LM, BERT, relation extraction, T-REX},
}

@misc{van_schijndel_quantity_2019-1,
	title = {Quantity doesn't buy quality syntax with neural language models},
	url = {https://zenodo.org/record/3559340},
	doi = {10.5281/zenodo.3559340},
	abstract = {This repository contains the 125 LSTM models analyzed in van Schijndel, Mueller, and Linzen (2019) "Quantity doesn't buy quality syntax with neural language models". Each archive contains 25 models trained on a specific number of training tokens. All models were trained to use the vocabulary in vocab.txt. The naming convention for each model is: LSTM\_[Hidden Units]\_[Training Tokens]\_[Training Partition]\_[Random Seed]-d[Dropout Rate].pt Hidden Units: The number of hidden units per layer (there are two layers in each model) \{100, 200, 400, 800, 1600\} Training Tokens: The number of tokens used to train each model \{2m, 10m, 20m, 40m, 80m\} Training Partition: Five distinct training partitions were created for each amount of training data \{a, b, c, d, e\} Random Seed: The random seed used to train each model* Dropout Rate: All models used a dropout rate of 0.2 *A scripting bug led to a random seed of 0 for all models trained on less than 40 million tokens. This does not substantively affect the analyses since each model is distinct in terms of the model configuration or training data, so we opted to not retrain the models with unique random seeds to save time and computational resources.},
	urldate = {2020-10-29},
	publisher = {Zenodo},
	author = {van Schijndel, Marten and Mueller, Aaron and Linzen, Tal},
	month = nov,
	year = {2019},
	file = {Zenodo Snapshot:/Users/kriarm/Zotero/storage/JRPNY26N/3559340.html:text/html},
}

@article{wang_superglue_2020,
	title = {{SuperGLUE}: {A} stickier benchmark for general-purpose language understanding systems},
	shorttitle = {{SuperGLUE}},
	url = {http://arxiv.org/abs/1905.00537},
	abstract = {In the last year, new models and methods for pretraining and transfer learning have driven striking performance improvements across a range of language understanding tasks. The GLUE benchmark, introduced a little over one year ago, offers a single-number metric that summarizes progress on a diverse set of such tasks, but performance on the benchmark has recently surpassed the level of non-expert humans, suggesting limited headroom for further research. In this paper we present SuperGLUE, a new benchmark styled after GLUE with a new set of more difficult language understanding tasks, a software toolkit, and a public leaderboard. SuperGLUE is available at super.gluebenchmark.com.},
	urldate = {2020-10-29},
	journal = {arXiv:1905.00537 [cs]},
	author = {Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
	month = feb,
	year = {2020},
	note = {arXiv: 1905.00537},
	keywords = {benchmark, GLUE, NLP, NLU},
}

@article{marcus_next_2020,
	title = {The next decade in {AI}: {Four} steps towards robust artificial intelligence},
	shorttitle = {The {Next} {Decade} in {AI}},
	url = {http://arxiv.org/abs/2002.06177},
	abstract = {Recent research in artificial intelligence and machine learning has largely emphasized general-purpose learning and ever-larger training sets and more and more compute. In contrast, I propose a hybrid, knowledge-driven, reasoning-based approach, centered around cognitive models, that could provide the substrate for a richer, more robust AI than is currently possible.},
	urldate = {2020-10-29},
	journal = {arXiv:2002.06177 [cs]},
	author = {Marcus, Gary},
	month = feb,
	year = {2020},
	note = {arXiv: 2002.06177},
}

@inproceedings{bender_climbing_2020,
	address = {Online},
	title = {Climbing towards {NLU}: {On} meaning, form, and understanding in the age of data},
	shorttitle = {Climbing towards {NLU}},
	url = {https://www.aclweb.org/anthology/2020.acl-main.463},
	doi = {10.18653/v1/2020.acl-main.463},
	language = {en},
	urldate = {2020-08-09},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Bender, Emily M. and Koller, Alexander},
	year = {2020},
	keywords = {NLP, BERT, GPT-2, semantics, meaning, Turing test, Turing, Chinese room, NLU},
	pages = {5185--5198},
}

@article{firestone_performance_2020,
	title = {Performance vs. competence in human–machine comparisons},
	copyright = {© 2020 . https://www-pnas-org.ru.idm.oclc.org/site/aboutpnas/licenses.xhtmlPublished under the PNAS license.},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/content/early/2020/10/13/1905334117},
	doi = {10.1073/pnas.1905334117},
	abstract = {Does the human mind resemble the machines that can behave like it? Biologically inspired machine-learning systems approach “human-level” accuracy in an astounding variety of domains, and even predict human brain activity—raising the exciting possibility that such systems represent the world like we do. However, even seemingly intelligent machines fail in strange and “unhumanlike” ways, threatening their status as models of our minds. How can we know when human–machine behavioral differences reflect deep disparities in their underlying capacities, vs. when such failures are only superficial or peripheral? This article draws on a foundational insight from cognitive science—the distinction between performance and competence—to encourage “species-fair” comparisons between humans and machines. The performance/competence distinction urges us to consider whether the failure of a system to behave as ideally hypothesized, or the failure of one creature to behave like another, arises not because the system lacks the relevant knowledge or internal capacities (“competence”), but instead because of superficial constraints on demonstrating that knowledge (“performance”). I argue that this distinction has been neglected by research comparing human and machine behavior, and that it should be essential to any such comparison. Focusing on the domain of image classification, I identify three factors contributing to the species-fairness of human–machine comparisons, extracted from recent work that equates such constraints. Species-fair comparisons level the playing field between natural and artificial intelligence, so that we can separate more superficial differences from those that may be deep and enduring.},
	language = {en},
	urldate = {2020-10-19},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Firestone, Chaz},
	month = oct,
	year = {2020},
	pmid = {33051296},
	note = {ISBN: 9781905334117
Publisher: National Academy of Sciences
Section: Perspective},
	keywords = {AI, artificial intelligence, ASL, competence, performance},
	file = {Firestone - 2020 - Performance vs. competence in human–machine compar.pdf:/Volumes/GoogleDrive-100121059175283315603/My Drive/zotero/Firestone - 2020 - Performance vs. competence in human–machine compar.pdf:application/pdf},
}

@article{buzsaki_braincognitive_2020,
	title = {The brain–cognitive behavior problem: {A} retrospective},
	volume = {7},
	copyright = {Copyright © 2020 Buzsáki. This is an open-access article distributed under the terms of the Creative Commons Attribution 4.0 International license, which permits unrestricted use, distribution and reproduction in any medium provided that the original work is properly attributed.},
	issn = {2373-2822},
	shorttitle = {The {Brain}–{Cognitive} {Behavior} {Problem}},
	url = {https://www.eneuro.org/content/7/4/ENEURO.0069-20.2020},
	doi = {10.1523/ENEURO.0069-20.2020},
	abstract = {In 2001, I was invited to write a review for a prominent journal. I thought that the best way to exploit this opportunity was to write an essay about my problems with ill-defined scientific terms and question whether the dominant framework in neuroscience is on the right track. My main argument was that many terms in neuroscience are inherited from folk psychology and are often used in two ambiguous ways: both as the thing-to-be-explained (explanandum) and the thing-that-explains (explanans; e.g., “we have memory because we remember,” “we remember because we have memory”). These postulated terms are assumed to be entities with definable boundaries, and within this framework, the goal of neuroscience is to find homes and mechanisms for these terms in the brain with corresponding boundaries (I called this “the correlational approach”). I warned that a framework dictated by human-centric introspection might not be the right roadmap for neuroscience and argued that there should be another way of carving up the brain’s “natural kinds.”

A month later, I received the rejection letter: “Dear Gyuri, … I hope you understand that for the sake of the journal we cannot publish your manuscript” (emphasis added). One reviewer was very enthusiastic while the other strongly dismissive. I took a deep breath, put the issue on the back burner, and went back to the lab. Perhaps the harsh reviewer was right. Although I recognized the problem correctly, I failed to provide the right strategy to solve it. Yet, the issues I exposed in the manuscript kept bugging me, and I have since written two books (Buzsáki, 2006, 2019) as attempts to clarify my views and offer alternative strategies to explore brain-behavior relationships. In the intervening two decades, perhaps thinking along similar lines, other investigators have also addressed the explanandum-explanans issues (Krakauer et al., …},
	language = {en},
	number = {4},
	urldate = {2020-10-14},
	journal = {eNeuro},
	author = {Buzsáki, György},
	month = jul,
	year = {2020},
	pmid = {32769166},
	note = {Publisher: Society for Neuroscience
Section: Opinion},
	keywords = {folk psychology, philosohpy of neuroscience, causality, expanation},
	file = {Buzsáki - 2020 - The brain–cognitive behavior problem A retrospect.pdf:/Users/kriarm/Zotero/storage/DSH6TIF3/Buzsáki - 2020 - The brain–cognitive behavior problem A retrospect.pdf:application/pdf},
}

@article{botvinick_deep_2020,
	title = {Deep reinforcement learning and its neuroscientific implications},
	volume = {107},
	issn = {08966273},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627320304682},
	doi = {10.1016/j.neuron.2020.06.014},
	language = {en},
	number = {4},
	urldate = {2020-10-13},
	journal = {Neuron},
	author = {Botvinick, Matthew and Wang, Jane X. and Dabney, Will and Miller, Kevin J. and Kurth-Nelson, Zeb},
	month = aug,
	year = {2020},
	keywords = {deep learning, LSTM, gating, GRU, episodic memory, reinforcment learning, cognitive control},
	pages = {603--616},
}

@article{ericsson_long-term_1995,
	title = {Long-term working memory},
	volume = {102},
	issn = {1939-1471, 0033-295X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-295X.102.2.211},
	doi = {10.1037/0033-295X.102.2.211},
	language = {en},
	number = {2},
	urldate = {2020-09-23},
	journal = {Psychological Review},
	author = {Ericsson, K. Anders and Kintsch, Walter},
	year = {1995},
	keywords = {working memory, long term memory, long term working memory},
	pages = {211--245},
}

@article{lo_top_2020,
	title = {Top 10 essential data science topics to real-world application from the industry perspectives},
	url = {https://hdsr.mitpress.mit.edu/pub/diub13so/release/2},
	language = {en},
	urldate = {2020-10-07},
	journal = {Harvard Data Science Review},
	author = {Lo, Victor S. Y.},
	month = sep,
	year = {2020},
	note = {Publisher: PubPub},
	keywords = {NLP, ethics, business strategy, ETL, industry},
}

@article{varoquaux_cross-validation_2018,
	series = {New advances in encoding and decoding of brain signals},
	title = {Cross-validation failure: {Small} sample sizes lead to large error bars},
	volume = {180},
	issn = {1053-8119},
	shorttitle = {Cross-validation failure},
	url = {http://www.sciencedirect.com/science/article/pii/S1053811917305311},
	doi = {10.1016/j.neuroimage.2017.06.061},
	abstract = {Predictive models ground many state-of-the-art developments in statistical brain image analysis: decoding, MVPA, searchlight, or extraction of biomarkers. The principled approach to establish their validity and usefulness is cross-validation, testing prediction on unseen data. Here, I would like to raise awareness on error bars of cross-validation, which are often underestimated. Simple experiments show that sample sizes of many neuroimaging studies inherently lead to large error bars, eg ±10\% for 100 samples. The standard error across folds strongly underestimates them. These large error bars compromise the reliability of conclusions drawn with predictive models, such as biomarkers or methods developments where, unlike with cognitive neuroimaging MVPA approaches, more samples cannot be acquired by repeating the experiment across many subjects. Solutions to increase sample size must be investigated, tackling possible increases in heterogeneity of the data.},
	language = {en},
	urldate = {2020-08-26},
	journal = {NeuroImage},
	author = {Varoquaux, Gaël},
	month = oct,
	year = {2018},
	pages = {68--77},
}

@article{baddeley_working_2003-1,
	series = {{ASHA} 2002},
	title = {Working memory and language: an overview},
	volume = {36},
	issn = {0021-9924},
	shorttitle = {Working memory and language},
	url = {http://www.sciencedirect.com/science/article/pii/S0021992403000194},
	doi = {10.1016/S0021-9924(03)00019-4},
	abstract = {Working memory involves the temporary storage and manipulation of information that is assumed to be necessary for a wide range of complex cognitive activities. In 1974, Baddeley and Hitch proposed that it could be divided into three subsystems, one concerned with verbal and acoustic information, the phonological loop, a second, the visuospatial sketchpad providing its visual equivalent, while both are dependent upon a third attentionally-limited control system, the central executive. A fourth subsystem, the episodic buffer, has recently been proposed. These are described in turn, with particular reference to implications for both the normal processing of language, and its potential disorders.
Learning outcomes
The reader will be introduced to the concept of a multi-component working memory. Particular emphasis will be placed on the phonological loop component, and (a) its fractionation into a storage and processing component, (b) the neuropsychological evidence for this distinction, and (c) its implication for both native and second language learning. This will be followed by (d) a brief overview of the visuospatial sketchpad and its possible role in language, culminating in (e) discussion of the higher-level control functions of working memory which include (f) the central executive and its multi-dimensional storage system, the episodic buffer. An attempt throughout is made to link the model to its role in both normal and disordered language functions.},
	language = {en},
	number = {3},
	urldate = {2020-09-16},
	journal = {Journal of Communication Disorders},
	author = {Baddeley, Alan},
	month = may,
	year = {2003},
	keywords = {working memory, phonological loop, central executive},
	pages = {189--208},
}

@article{galassi_attention_2020,
	title = {Attention in natural language processing},
	issn = {2162-237X, 2162-2388},
	url = {http://arxiv.org/abs/1902.02181},
	doi = {10.1109/TNNLS.2020.3019893},
	abstract = {Attention is an increasingly popular mechanism used in a wide range of neural architectures. The mechanism itself has been realized in a variety of formats. However, because of the fast-paced advances in this domain, a systematic overview of attention is still missing. In this article, we define a unified model for attention architectures in natural language processing, with a focus on those designed to work with vector representations of the textual data. We propose a taxonomy of attention models according to four dimensions: the representation of the input, the compatibility function, the distribution function, and the multiplicity of the input and/or output. We present the examples of how prior information can be exploited in attention models and discuss ongoing research efforts and open challenges in the area, providing the first extensive categorization of the vast body of literature in this exciting domain.},
	urldate = {2020-09-16},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Galassi, Andrea and Lippi, Marco and Torroni, Paolo},
	year = {2020},
	note = {arXiv: 1902.02181},
	keywords = {attention, nlp},
	pages = {1--18},
}

@inproceedings{henderson_unstoppable_2020,
	address = {Online},
	title = {The unstoppable rise of computational linguistics in deep learning},
	url = {https://www.aclweb.org/anthology/2020.acl-main.561},
	doi = {10.18653/v1/2020.acl-main.561},
	abstract = {In this paper, we trace the history of neural networks applied to natural language understanding tasks, and identify key contributions which the nature of language has made to the development of neural network architectures. We focus on the importance of variable binding and its instantiation in attention-based models, and argue that Transformer is not a sequence model but an induced-structure model. This perspective leads to predictions of the challenges facing research in deep learning architectures for natural language understanding.},
	urldate = {2020-09-16},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Henderson, James},
	month = jul,
	year = {2020},
	keywords = {deep learning, hierarchy, linguistics},
	pages = {6294--6306},
}

@article{stolk_conceptual_2016,
	title = {Conceptual alignment: {How} brains achieve mutual understanding},
	volume = {20},
	issn = {13646613},
	shorttitle = {Conceptual {Alignment}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661315002867},
	doi = {10.1016/j.tics.2015.11.007},
	language = {en},
	number = {3},
	urldate = {2020-09-08},
	journal = {Trends in Cognitive Sciences},
	author = {Stolk, Arjen and Verhagen, Lennart and Toni, Ivan},
	month = mar,
	year = {2016},
	pages = {180--191},
}

@article{goodman_pragmatic_2016,
	title = {Pragmatic language interpretation as probabilistic inference},
	volume = {20},
	issn = {13646613},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S136466131630122X},
	doi = {10.1016/j.tics.2016.08.005},
	language = {en},
	number = {11},
	urldate = {2020-09-08},
	journal = {Trends in Cognitive Sciences},
	author = {Goodman, Noah D. and Frank, Michael C.},
	month = nov,
	year = {2016},
	pages = {818--829},
}

@article{kumaran_what_2016,
	title = {What learning systems do intelligent agents need? complementary learning systems theory updated},
	volume = {20},
	issn = {13646613},
	shorttitle = {What {Learning} {Systems} do {Intelligent} {Agents} {Need}?},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661316300432},
	doi = {10.1016/j.tics.2016.05.004},
	language = {en},
	number = {7},
	urldate = {2020-09-08},
	journal = {Trends in Cognitive Sciences},
	author = {Kumaran, Dharshan and Hassabis, Demis and McClelland, James L.},
	month = jul,
	year = {2016},
	pages = {512--534},
}

@article{friederici_grounding_2015,
	title = {Grounding language processing on basic neurophysiological principles},
	volume = {19},
	issn = {13646613},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661315000741},
	doi = {10.1016/j.tics.2015.03.012},
	language = {en},
	number = {6},
	urldate = {2020-09-08},
	journal = {Trends in Cognitive Sciences},
	author = {Friederici, Angela D. and Singer, Wolf},
	month = jun,
	year = {2015},
	pages = {329--338},
}

@article{battaglia_structured_2012,
	title = {Structured cognition and neural systems: {From} rats to language},
	volume = {36},
	issn = {01497634},
	shorttitle = {Structured cognition and neural systems},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0149763412000632},
	doi = {10.1016/j.neubiorev.2012.04.004},
	language = {en},
	number = {7},
	urldate = {2020-09-08},
	journal = {Neuroscience \& Biobehavioral Reviews},
	author = {Battaglia, Francesco P. and Borensztajn, Gideon and Bod, Rens},
	month = aug,
	year = {2012},
	pages = {1626--1639},
}

@article{yarkoni_generalizability_2022,
	title = {The generalizability crisis},
	volume = {45},
	issn = {0140-525X, 1469-1825},
	url = {https://www.cambridge.org/core/product/identifier/S0140525X20001685/type/journal_article},
	doi = {10.1017/S0140525X20001685},
	abstract = {Abstract
            Most theories and hypotheses in psychology are verbal in nature, yet their evaluation overwhelmingly relies on inferential statistical procedures. The validity of the move from qualitative to quantitative analysis depends on the verbal and statistical expressions of a hypothesis being closely aligned – that is, that the two must refer to roughly the same set of hypothetical observations. Here, I argue that many applications of statistical inference in psychology fail to meet this basic condition. Focusing on the most widely used class of model in psychology – the linear mixed model – I explore the consequences of failing to statistically operationalize verbal hypotheses in a way that respects researchers' actual generalization intentions. I demonstrate that although the “random effect” formalism is used pervasively in psychology to model intersubject variability, few researchers accord the same treatment to other variables they clearly intend to generalize over (e.g., stimuli, tasks, or research sites). The under-specification of random effects imposes far stronger constraints on the generalizability of results than most researchers appreciate. Ignoring these constraints can dramatically inflate false-positive rates, and often leads researchers to draw sweeping verbal generalizations that lack a meaningful connection to the statistical quantities they are putatively based on. I argue that failure to take the alignment between verbal and statistical expressions seriously lies at the heart of many of psychology's ongoing problems (e.g., the replication crisis), and conclude with a discussion of several potential avenues for improvement.},
	language = {en},
	urldate = {2022-05-11},
	journal = {Behavioral and Brain Sciences},
	author = {Yarkoni, Tal},
	year = {2022},
	keywords = {methods, inference, general linear model, regression},
	pages = {e1},
}

@article{goldstein_shared_2022,
	title = {Shared computational principles for language processing in humans and deep language models},
	volume = {25},
	issn = {1097-6256, 1546-1726},
	url = {https://www.nature.com/articles/s41593-022-01026-4},
	doi = {10.1038/s41593-022-01026-4},
	abstract = {Abstract
            Departing from traditional linguistic models, advances in deep learning have resulted in a new type of predictive (autoregressive) deep language models (DLMs). Using a self-supervised next-word prediction task, these models generate appropriate linguistic responses in a given context. In the current study, nine participants listened to a 30-min podcast while their brain responses were recorded using electrocorticography (ECoG). We provide empirical evidence that the human brain and autoregressive DLMs share three fundamental computational principles as they process the same natural narrative: (1) both are engaged in continuous next-word prediction before word onset; (2) both match their pre-onset predictions to the incoming word to calculate post-onset surprise; (3) both rely on contextual embeddings to represent words in natural contexts. Together, our findings suggest that autoregressive DLMs provide a new and biologically feasible computational framework for studying the neural basis of language.},
	language = {en},
	number = {3},
	urldate = {2022-05-19},
	journal = {Nature Neuroscience},
	author = {Goldstein, Ariel and Zada, Zaid and Buchnik, Eliav and Schain, Mariano and Price, Amy and Aubrey, Bobbi and Nastase, Samuel A. and Feder, Amir and Emanuel, Dotan and Cohen, Alon and Jansen, Aren and Gazula, Harshvardhan and Choe, Gina and Rao, Aditi and Kim, Catherine and Casto, Colton and Fanda, Lora and Doyle, Werner and Friedman, Daniel and Dugan, Patricia and Melloni, Lucia and Reichart, Roi and Devore, Sasha and Flinker, Adeen and Hasenfratz, Liat and Levy, Omer and Hassidim, Avinatan and Brenner, Michael and Matias, Yossi and Norman, Kenneth A. and Devinsky, Orrin and Hasson, Uri},
	month = mar,
	year = {2022},
	pages = {369--380},
}

@inproceedings{caucheteux_model-based_2021,
	address = {Punta Cana, Dominican Republic},
	title = {Model-based analysis of brain activity reveals the hierarchy of language in 305 subjects},
	url = {https://aclanthology.org/2021.findings-emnlp.308},
	doi = {10.18653/v1/2021.findings-emnlp.308},
	abstract = {A popular approach to decompose the neural bases of language consists in correlating, across individuals, the brain responses to different stimuli (e.g. regular speech versus scrambled words, sentences, or paragraphs). Although successful, this `model-free' approach necessitates the acquisition of a large and costly set of neuroimaging data. Here, we show that a model-based approach can reach equivalent results within subjects exposed to natural stimuli. We capitalize on the recently-discovered similarities between deep language models and the human brain to compute the mapping between i) the brain responses to regular speech and ii) the activations of deep language models elicited by modified stimuli (e.g. scrambled words, sentences, or paragraphs). Our model-based approach successfully replicates the seminal study of Lerner et al. (2011), which revealed the hierarchy of language areas by comparing the functional-magnetic resonance imaging (fMRI) of seven subjects listening to 7min of both regular and scrambled narratives. We further extend and precise these results to the brain signals of 305 individuals listening to 4.1 hours of narrated stories. Overall, this study paves the way for efficient and flexible analyses of the brain bases of language.},
	urldate = {2022-05-20},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2021},
	publisher = {Association for Computational Linguistics},
	author = {Caucheteux, Charlotte and Gramfort, Alexandre and King, Jean-Remi},
	month = nov,
	year = {2021},
	pages = {3635--3644},
}

@article{hale_neurocomputational_2022,
	title = {Neurocomputational models of language processing},
	volume = {8},
	issn = {2333-9683, 2333-9691},
	url = {https://www.annualreviews.org/doi/10.1146/annurev-linguistics-051421-020803},
	doi = {10.1146/annurev-linguistics-051421-020803},
	abstract = {Efforts to understand the brain bases of language face the Mapping Problem: At what level do linguistic computations and representations connect to human neurobiology? We review one approach to this problem that relies on rigorously defined computational models to specify the links between linguistic features and neural signals. Such tools can be used to estimate linguistic predictions, model linguistic features, and specify a sequence of processing steps that may be quantitatively fit to neural signals collected while participants use language. Progress has been helped by advances in machine learning, attention to linguistically interpretable models, and openly shared data sets that allow researchers to compare and contrast a variety of models. We describe one such data set in detail in the Supplemental Appendix .},
	language = {en},
	number = {1},
	urldate = {2022-05-26},
	journal = {Annual Review of Linguistics},
	author = {Hale, John T. and Campanelli, Luca and Li, Jixing and Bhattasali, Shohini and Pallier, Christophe and Brennan, Jonathan R.},
	month = jan,
	year = {2022},
	pages = {427--446},
	file = {Full Text:/Users/kriarm/Zotero/storage/J44HEUQA/Hale et al. - 2022 - Neurocomputational Models of Language Processing.pdf:application/pdf},
}

@article{caucheteux_brains_2022,
	title = {Brains and algorithms partially converge in natural language processing},
	volume = {5},
	issn = {2399-3642},
	url = {https://www.nature.com/articles/s42003-022-03036-1},
	doi = {10.1038/s42003-022-03036-1},
	abstract = {Abstract
            Deep learning algorithms trained to predict masked words from large amount of text have recently been shown to generate activations similar to those of the human brain. However, what drives this similarity remains currently unknown. Here, we systematically compare a variety of deep language models to identify the computational principles that lead them to generate brain-like representations of sentences. Specifically, we analyze the brain responses to 400 isolated sentences in a large cohort of 102 subjects, each recorded for two hours with functional magnetic resonance imaging (fMRI) and magnetoencephalography (MEG). We then test where and when each of these algorithms maps onto the brain responses. Finally, we estimate how the architecture, training, and performance of these models independently account for the generation of brain-like representations. Our analyses reveal two main findings. First, the similarity between the algorithms and the brain primarily depends on their ability to predict words from context. Second, this similarity reveals the rise and maintenance of perceptual, lexical, and compositional representations within each cortical region. Overall, this study shows that modern language algorithms partially converge towards brain-like solutions, and thus delineates a promising path to unravel the foundations of natural language processing.},
	language = {en},
	number = {1},
	urldate = {2022-05-31},
	journal = {Communications Biology},
	author = {Caucheteux, Charlotte and King, Jean-Rémi},
	month = dec,
	year = {2022},
	pages = {134},
}

@inproceedings{peters_deep_2018,
	address = {New Orleans, Louisiana},
	title = {Deep contextualized word representations},
	url = {https://aclanthology.org/N18-1202},
	doi = {10.18653/v1/N18-1202},
	abstract = {We introduce a new type of deep contextualized word representation that models both (1) complex characteristics of word use (e.g., syntax and semantics), and (2) how these uses vary across linguistic contexts (i.e., to model polysemy). Our word vectors are learned functions of the internal states of a deep bidirectional language model (biLM), which is pre-trained on a large text corpus. We show that these representations can be easily added to existing models and significantly improve the state of the art across six challenging NLP problems, including question answering, textual entailment and sentiment analysis. We also present an analysis showing that exposing the deep internals of the pre-trained network is crucial, allowing downstream models to mix different types of semi-supervision signals.},
	booktitle = {Proceedings of the 2018 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Peters, Matthew E. and Neumann, Mark and Iyyer, Mohit and Gardner, Matt and Clark, Christopher and Lee, Kenton and Zettlemoyer, Luke},
	month = jun,
	year = {2018},
	pages = {2227--2237},
}

@misc{binz_using_2022,
	title = {Using cognitive psychology to understand {GPT}-3},
	url = {https://psyarxiv.com/6dfgk/},
	doi = {10.31234/osf.io/6dfgk},
	abstract = {We study GPT-3, a recent large language model, using tools from cognitive psychology. More specifically, we assess GPT-3's decision-making, information search, deliberation, and causal reasoning abilities on a battery of canonical experiments from the literature. We find that much of GPT-3's behavior is impressive: it solves vignette-based tasks similarly or better than human subjects, is able to make decent decisions from descriptions, outperforms humans in a multi-armed bandit task, and shows signatures of model-based reinforcement learning. Yet we also find that small perturbations to vignette-based tasks can lead GPT-3 vastly astray, that it shows no signatures of directed exploration, and that it fails miserably in a causal reasoning task. These results enrich our understanding of current large language models and pave the way for future investigations using tools from cognitive psychology to study increasingly capable and opaque artificial agents.},
	language = {en-us},
	urldate = {2022-06-23},
	publisher = {PsyArXiv},
	author = {Binz, Marcel and Schulz, Eric},
	month = jun,
	year = {2022},
	keywords = {Language, decision-making, cognitive psychology, and Heuristics, Biases, causal reasoning, Cognitive Psychology, deliberation, Framing, information search, Judgment and Decision Making, language models, Learning, Reasoning, Social and Behavioral Sciences},
}

@misc{piantasodi_meaning_2022,
	title = {Meaning without reference in large language models},
	url = {http://arxiv.org/abs/2208.02957},
	doi = {10.48550/arXiv.2208.02957},
	abstract = {The widespread success of large language models (LLMs) has been met with skepticism that they possess anything like human concepts or meanings. Contrary to claims that LLMs possess no meaning whatsoever, we argue that they likely capture important aspects of meaning, and moreover work in a way that approximates a compelling account of human cognition in which meaning arises from conceptual role. Because conceptual role is defined by the relationships between internal representational states, meaning cannot be determined from a model's architecture, training data, or objective function, but only by examination of how its internal states relate to each other. This approach may clarify why and how LLMs are so successful and suggest how they can be made more human-like.},
	urldate = {2022-08-09},
	publisher = {arXiv},
	author = {Piantasodi, Steven T. and Hill, Felix},
	month = aug,
	year = {2022},
	note = {Number: arXiv:2208.02957
arXiv:2208.02957 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
}

@article{hagoort_language_2023,
	title = {The language marker hypothesis},
	volume = {230},
	issn = {0010-0277},
	url = {https://www.sciencedirect.com/science/article/pii/S0010027722002402},
	doi = {10.1016/j.cognition.2022.105252},
	abstract = {According to the language marker hypothesis language has provided homo sapiens with a rich symbolic system that plays a central role in interpreting signals delivered by our sensory apparatus, in shaping action goals, and in creating a powerful tool for reasoning and inferencing. This view provides an important correction on embodied accounts of language that reduce language to action, perception, emotion and mental simulation. The presence of a language system has, however, also important consequences for perception, action, emotion, and memory. Language stamps signals from perception, action, and emotional systems with rich cognitive markers that transform the role of these signals in the overall cognitive architecture of the human mind. This view does not deny that language is implemented by means of universal principles of neural organization. However, language creates the possibility to generate rich internal models of the world that are shaped and made accessible by the characteristics of a language system. This makes us less dependent on direct action-perception couplings and might even sometimes go at the expense of the veridicality of perception. In cognitive (neuro)science the pendulum has swung from language as the key to understand the organization of the human mind to the perspective that it is a byproduct of perception and action. It is time that it partly swings back again.},
	language = {en},
	urldate = {2022-09-15},
	journal = {Cognition},
	author = {Hagoort, Peter},
	month = jan,
	year = {2023},
	keywords = {Language, Brain, Mental models, Mind, Placebo, Predictive processing},
	pages = {105252},
}

@article{olsson_-context_2022,
	title = {In-context learning and induction heads},
	journal = {Transformer Circuits Thread},
	author = {Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Johnston, Scott and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
	year = {2022},
}

@article{calin-jageman_estimation_2019,
	title = {Estimation for better inference in neuroscience},
	volume = {6},
	issn = {2373-2822},
	url = {https://www.eneuro.org/lookup/doi/10.1523/ENEURO.0205-19.2019},
	doi = {10.1523/ENEURO.0205-19.2019},
	language = {en},
	number = {4},
	urldate = {2022-08-16},
	journal = {eneuro},
	author = {Calin-Jageman, Robert J. and Cumming, Geoff},
	month = jul,
	year = {2019},
	keywords = {statistics, inference, CI, estimation, Bayesian statistics, confidence interval, t-test},
	pages = {ENEURO.0205--19.2019},
}

@article{raffel_exploring_2020-1,
	title = {Exploring the limits of transfer learning with a unified text-to-text transformer},
	volume = {21},
	issn = {1532-4435},
	abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pretraining objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new "Colossal Clean Crawled Corpus", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
	number = {1},
	journal = {J. Mach. Learn. Res.},
	author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
	month = jan,
	year = {2020},
	note = {Publisher: JMLR.org},
	keywords = {deep learning, transfer learning, natural language processing, multi-task learning, attention based models},
}

@article{hoekstra_aspiring_2021,
	title = {Aspiring to greater intellectual humility in science},
	volume = {5},
	issn = {2397-3374},
	url = {https://www.nature.com/articles/s41562-021-01203-8},
	doi = {10.1038/s41562-021-01203-8},
	language = {en},
	number = {12},
	urldate = {2022-10-17},
	journal = {Nature Human Behaviour},
	author = {Hoekstra, Rink and Vazire, Simine},
	month = dec,
	year = {2021},
	keywords = {open science, peer-review, writing},
	pages = {1602--1607},
	file = {Full Text:/Users/kriarm/Zotero/storage/DDZX4QUA/Hoekstra and Vazire - 2021 - Aspiring to greater intellectual humility in scien.pdf:application/pdf},
}

@article{armeni_10-hour_2022,
	title = {A 10-hour within-participant magnetoencephalography narrative dataset to test models of language comprehension},
	volume = {9},
	issn = {2052-4463},
	url = {https://www.nature.com/articles/s41597-022-01382-7},
	doi = {10.1038/s41597-022-01382-7},
	abstract = {Abstract
            Recently, cognitive neuroscientists have increasingly studied the brain responses to narratives. At the same time, we are witnessing exciting developments in natural language processing where large-scale neural network models can be used to instantiate cognitive hypotheses in narrative processing. Yet, they learn from text alone and we lack ways of incorporating biological constraints during training. To mitigate this gap, we provide a narrative comprehension magnetoencephalography (MEG) data resource that can be used to train neural network models directly on brain data. We recorded from 3 participants, 10 separate recording hour-long sessions each, while they listened to audiobooks in English. After story listening, participants answered short questions about their experience. To minimize head movement, the participants wore MEG-compatible head casts, which immobilized their head position during recording. We report a basic evoked-response analysis showing that the responses accurately localize to primary auditory areas. The responses are robust and conserved across 10 sessions for every participant. We also provide usage notes and briefly outline possible future uses of the resource.},
	language = {en},
	number = {1},
	urldate = {2022-10-25},
	journal = {Scientific Data},
	author = {Armeni, Kristijan and Güçlü, Umut and van Gerven, Marcel and Schoffelen, Jan-Mathijs},
	month = dec,
	year = {2022},
	pages = {278},
}

@article{schwarzlose_superiority_2023,
	title = {Superiority and stigma in modern psychology and neuroscience},
	volume = {27},
	issn = {1364-6613, 1879-307X},
	url = {https://www.cell.com/trends/cognitive-sciences/abstract/S1364-6613(22)00229-7},
	doi = {10.1016/j.tics.2022.09.008},
	language = {English},
	number = {1},
	urldate = {2023-01-03},
	journal = {Trends in Cognitive Sciences},
	author = {Schwarzlose, Rebecca F.},
	month = jan,
	year = {2023},
	pmid = {36207259},
	note = {Publisher: Elsevier},
	keywords = {aging, bias, neurodevelopmental disorders, psychiatry, research practices, stigma},
	pages = {4--6},
}

@inproceedings{ryu_accounting_2021,
	address = {Online},
	title = {Accounting for agreement phenomena in sentence comprehension with transformer language models: {Effects} of similarity-based interference on surprisal and attention},
	shorttitle = {Accounting for {Agreement} {Phenomena} in {Sentence} {Comprehension} with {Transformer} {Language} {Models}},
	url = {https://aclanthology.org/2021.cmcl-1.6},
	doi = {10.18653/v1/2021.cmcl-1.6},
	abstract = {We advance a novel explanation of similarity-based interference effects in subject-verb and reflexive pronoun agreement processing, grounded in surprisal values computed from a pretrained large-scale Transformer model, GPT-2. Specifically, we show that surprisal of the verb or reflexive pronoun predicts facilitatory interference effects in ungrammatical sentences, where a distractor noun that matches in number with the verb or pronouns leads to faster reading times, despite the distractor not participating in the agreement relation. We review the human empirical evidence for such effects, including recent meta-analyses and large-scale studies. We also show that attention patterns (indexed by entropy and other measures) in the Transformer show patterns of diffuse attention in the presence of similar distractors, consistent with cue-based retrieval models of parsing. But in contrast to these models, the attentional cues and memory representations are learned entirely from the simple self-supervised task of predicting the next word.},
	urldate = {2023-01-09},
	booktitle = {Proceedings of the {Workshop} on {Cognitive} {Modeling} and {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Ryu, Soo Hyun and Lewis, Richard},
	month = jun,
	year = {2021},
	keywords = {language model, attention, GPT-2, entropy, surprisal, language},
	pages = {61--71},
}

@misc{oh_entropy-_2022,
	title = {Entropy- and distance-based predictors from {GPT}-2 attention patterns predict reading times over and above {GPT}-2 surprisal},
	url = {http://arxiv.org/abs/2212.11185},
	doi = {10.48550/arXiv.2212.11185},
	abstract = {Transformer-based large language models are trained to make predictions about the next word by aggregating representations of previous tokens through their self-attention mechanism. In the field of cognitive modeling, such attention patterns have recently been interpreted as embodying the process of cue-based retrieval, in which attention over multiple targets is taken to generate interference and latency during retrieval. Under this framework, this work first defines an entropy-based predictor that quantifies the diffuseness of self-attention, as well as distance-based predictors that capture the incremental change in attention patterns across timesteps. Moreover, following recent studies that question the informativeness of attention weights, we also experiment with alternative methods for incorporating vector norms into attention weights. Regression experiments using predictors calculated from the GPT-2 language model show that these predictors deliver a substantially better fit to held-out self-paced reading and eye-tracking data over a rigorous baseline including GPT-2 surprisal. Additionally, the distance-based predictors generally demonstrated higher predictive power, with effect sizes of up to 6.59 ms per standard deviation on self-paced reading times (compared to 2.82 ms for surprisal) and 1.05 ms per standard deviation on eye-gaze durations (compared to 3.81 ms for surprisal).},
	urldate = {2023-01-09},
	publisher = {arXiv},
	author = {Oh, Byung-Doh and Schuler, William},
	month = dec,
	year = {2022},
	note = {arXiv:2212.11185 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{perrigo_exclusive_2023,
	title = {Exclusive: {The} \$2 {Per} {Hour} {Workers} {Who} {Made} {ChatGPT} {Safer}},
	shorttitle = {Exclusive},
	url = {https://time.com/6247678/openai-chatgpt-kenya-workers/},
	abstract = {A TIME investigation reveals the difficult conditions faced by the workers who made ChatGPT possible},
	language = {en},
	urldate = {2023-01-19},
	journal = {Time},
	author = {Perrigo, Billy},
	month = jan,
	year = {2023},
	keywords = {AI ethics, AI, language models, ChatGPT},
}

@misc{shanahan_talking_2022,
	title = {Talking about large language models},
	url = {http://arxiv.org/abs/2212.03551},
	doi = {10.48550/arXiv.2212.03551},
	abstract = {Thanks to rapid progress in artificial intelligence, we have entered an era when technology and philosophy intersect in interesting ways. Sitting squarely at the centre of this intersection are large language models (LLMs). The more adept LLMs become at mimicking human language, the more vulnerable we become to anthropomorphism, to seeing the systems in which they are embedded as more human-like than they really are. This trend is amplified by the natural tendency to use philosophically loaded terms, such as "knows", "believes", and "thinks", when describing these systems. To mitigate this trend, this paper advocates the practice of repeatedly stepping back to remind ourselves of how LLMs, and the systems of which they form a part, actually work. The hope is that increased scientific precision will encourage more philosophical nuance in the discourse around artificial intelligence, both within the field and in the public sphere.},
	urldate = {2023-01-20},
	publisher = {arXiv},
	author = {Shanahan, Murray},
	month = dec,
	year = {2022},
	note = {arXiv:2212.03551 [cs]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@article{vasishth_right_2023,
	title = {Some right ways to analyze (psycho)linguistic data},
	volume = {9},
	issn = {2333-9683, 2333-9691},
	url = {https://www.annualreviews.org/doi/10.1146/annurev-linguistics-031220-010345},
	doi = {10.1146/annurev-linguistics-031220-010345},
	abstract = {Much has been written on the abuse and misuse of statistical methods, including p values, statistical significance, and so forth. I present some of the best practices in statistics using a running example data analysis. Focusing primarily on frequentist and Bayesian linear mixed models, I illustrate some defensible ways in which statistical inference—specifically, hypothesis testing using Bayes factors versus estimation or uncertainty quantification—can be carried out. The key is to not overstate the evidence and to not expect too much from statistics. Along the way, I demonstrate some powerful ideas, including the use of simulation to understand the design properties of one's experiment before running it, visualization of data before carrying out a formal analysis, and simulation of data from the fitted model to understand the model's behavior.},
	language = {en},
	number = {1},
	urldate = {2023-01-20},
	journal = {Annual Review of Linguistics},
	author = {Vasishth, Shravan},
	month = jan,
	year = {2023},
	keywords = {methods, frequentist statistics, NHST, Bayesian statistics},
	pages = {273--291},
}

@misc{dugan_real_2022,
	title = {Real or fake text?: {Investigating} human ability to detect boundaries between human-written and machine-generated text},
	shorttitle = {Real or {Fake} {Text}?},
	url = {http://arxiv.org/abs/2212.12672},
	doi = {10.48550/arXiv.2212.12672},
	abstract = {As text generated by large language models proliferates, it becomes vital to understand how humans engage with such text, and whether or not they are able to detect when the text they are reading did not originate with a human writer. Prior work on human detection of generated text focuses on the case where an entire passage is either human-written or machine-generated. In this paper, we study a more realistic setting where text begins as human-written and transitions to being generated by state-of-the-art neural language models. We show that, while annotators often struggle at this task, there is substantial variance in annotator skill and that given proper incentives, annotators can improve at this task over time. Furthermore, we conduct a detailed comparison study and analyze how a variety of variables (model size, decoding strategy, fine-tuning, prompt genre, etc.) affect human detection performance. Finally, we collect error annotations from our participants and use them to show that certain textual genres influence models to make different types of errors and that certain sentence-level features correlate highly with annotator selection. We release the RoFT dataset: a collection of over 21,000 human annotations paired with error classifications to encourage future work in human detection and evaluation of generated text.},
	urldate = {2023-01-25},
	publisher = {arXiv},
	author = {Dugan, Liam and Ippolito, Daphne and Kirubarajan, Arun and Shi, Sherry and Callison-Burch, Chris},
	month = dec,
	year = {2022},
	note = {arXiv:2212.12672 [cs]},
	keywords = {GPT-2, text generation, language models, human detection},
}

@article{rusakov_misadventure_2023,
	title = {A misadventure of the correlation coefficient},
	volume = {46},
	issn = {01662236},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0166223622001886},
	doi = {10.1016/j.tins.2022.09.009},
	language = {en},
	number = {2},
	urldate = {2023-01-25},
	journal = {Trends in Neurosciences},
	author = {Rusakov, Dmitri A.},
	month = feb,
	year = {2023},
	keywords = {methods, statistics, R, correlation coefficient, Pearson's R},
	pages = {94--96},
}

@misc{michelmann_large_2023,
	title = {Large language models can segment narrative events similarly to humans},
	url = {http://arxiv.org/abs/2301.10297},
	doi = {10.48550/arXiv.2301.10297},
	abstract = {Humans perceive discrete events such as "restaurant visits" and "train rides" in their continuous experience. One important prerequisite for studying human event perception is the ability of researchers to quantify when one event ends and another begins. Typically, this information is derived by aggregating behavioral annotations from several observers. Here we present an alternative computational approach where event boundaries are derived using a large language model, GPT-3, instead of using human annotations. We demonstrate that GPT-3 can segment continuous narrative text into events. GPT-3-annotated events are significantly correlated with human event annotations. Furthermore, these GPT-derived annotations achieve a good approximation of the "consensus" solution (obtained by averaging across human annotations); the boundaries identified by GPT-3 are closer to the consensus, on average, than boundaries identified by individual human annotators. This finding suggests that GPT-3 provides a feasible solution for automated event annotations, and it demonstrates a further parallel between human cognition and prediction in large language models. In the future, GPT-3 may thereby help to elucidate the principles underlying human event perception.},
	urldate = {2023-02-03},
	publisher = {arXiv},
	author = {Michelmann, Sebastian and Kumar, Manoj and Norman, Kenneth A. and Toneva, Mariya},
	month = jan,
	year = {2023},
	note = {arXiv:2301.10297 [cs, q-bio]},
	keywords = {Computer Science - Computation and Language, Quantitative Biology - Neurons and Cognition},
}

@article{van_dis_chatgpt_2023,
	title = {{ChatGPT}: five priorities for research},
	volume = {614},
	copyright = {2023 Springer Nature Limited},
	shorttitle = {{ChatGPT}},
	url = {https://www.nature.com/articles/d41586-023-00288-7},
	doi = {10.1038/d41586-023-00288-7},
	abstract = {Conversational AI is a game-changer for science. Here’s how to respond.},
	language = {en},
	number = {7947},
	urldate = {2023-02-27},
	journal = {Nature},
	author = {van Dis, Eva A. M. and Bollen, Johan and Zuidema, Willem and van Rooij, Robert and Bockting, Claudi L.},
	month = feb,
	year = {2023},
	note = {Bandiera\_abtest: a
Cg\_type: Comment
Number: 7947
Publisher: Nature Publishing Group
Subject\_term: Computer science, Research management, Publishing, Machine learning},
	keywords = {LM, AI, ethics, academic publishing, academic writing, peer review, language models, ChatGPT},
	pages = {224--226},
}

@misc{piantadosi_modern_2023,
	title = {Modern language models refute {Chomsky}’s approach to language},
	url = {https://lingbuzz.net/lingbuzz/007180},
	abstract = {The rise and success of large language models undermines virtually every strong claim for the innateness of language that has been proposed by generative linguistics. Modern machine learning has subverted and bypassed the entire theoretical framework of Chomsky's approach, including its core claims to particular insights, principles, structures, and processes. I describe the sense in which modern language models implement genuine theories of language, including representations of syntactic and semantic structure. I highlight the relationship between contemporary models and prior approaches in linguistics, namely those based on gradient computations and memorized constructions. I also respond to several critiques of large language models, including claims that they can't answer ``why'' questions, and skepticism that they are informative about real life acquisition. Most notably, large language models have attained remarkable success at discovering grammar without using any of the methods that some in linguistics insisted were necessary for a science of language to progress.},
	urldate = {2023-03-15},
	publisher = {LingBuzz},
	author = {Piantadosi, Steven},
	month = mar,
	year = {2023},
	note = {LingBuzz Published In:},
	keywords = {syntax, cognitive science, statistical learning, computational modeling, chomsky, emergent, generative syntax, large language model, minimalism},
	file = {LingBuzz Full Text PDF:/Users/kriarm/Zotero/storage/LIABK62X/Piantadosi - 2023 - Modern language models refute Chomsky’s approach t.pdf:application/pdf;Snapshot:/Users/kriarm/Zotero/storage/VVSCK3CH/007180.html:text/html},
}

@misc{mahowald_dissociating_2023,
	title = {Dissociating language and thought in large language models: a cognitive perspective},
	shorttitle = {Dissociating language and thought in large language models},
	url = {http://arxiv.org/abs/2301.06627},
	doi = {10.48550/arXiv.2301.06627},
	abstract = {Today's large language models (LLMs) routinely generate coherent, grammatical and seemingly meaningful paragraphs of text. This achievement has led to speculation that these networks are -- or will soon become -- "thinking machines", capable of performing tasks that require abstract knowledge and reasoning. Here, we review the capabilities of LLMs by considering their performance on two different aspects of language use: 'formal linguistic competence', which includes knowledge of rules and patterns of a given language, and 'functional linguistic competence', a host of cognitive abilities required for language understanding and use in the real world. Drawing on evidence from cognitive neuroscience, we show that formal competence in humans relies on specialized language processing mechanisms, whereas functional competence recruits multiple extralinguistic capacities that comprise human thought, such as formal reasoning, world knowledge, situation modeling, and social cognition. In line with this distinction, LLMs show impressive (although imperfect) performance on tasks requiring formal linguistic competence, but fail on many tests requiring functional competence. Based on this evidence, we argue that (1) contemporary LLMs should be taken seriously as models of formal linguistic skills; (2) models that master real-life language use would need to incorporate or develop not only a core language module, but also multiple non-language-specific cognitive capacities required for modeling thought. Overall, a distinction between formal and functional linguistic competence helps clarify the discourse surrounding LLMs' potential and provides a path toward building models that understand and use language in human-like ways.},
	urldate = {2023-03-21},
	publisher = {arXiv},
	author = {Mahowald, Kyle and Ivanova, Anna A. and Blank, Idan A. and Kanwisher, Nancy and Tenenbaum, Joshua B. and Fedorenko, Evelina},
	month = jan,
	year = {2023},
	note = {arXiv:2301.06627 [cs]},
	keywords = {cognitive neuroscience, GPT-3, NLP, LM, situation model, theory of mind, ChatGPT},
	file = {arXiv.org Snapshot:/Users/kriarm/Zotero/storage/EA5CN6LA/2301.html:text/html},
}

@misc{touvron_llama_2023,
	title = {{LLaMA}: {Open} and efficient foundation language models},
	shorttitle = {{LLaMA}},
	url = {http://arxiv.org/abs/2302.13971},
	doi = {10.48550/arXiv.2302.13971},
	abstract = {We introduce LLaMA, a collection of foundation language models ranging from 7B to 65B parameters. We train our models on trillions of tokens, and show that it is possible to train state-of-the-art models using publicly available datasets exclusively, without resorting to proprietary and inaccessible datasets. In particular, LLaMA-13B outperforms GPT-3 (175B) on most benchmarks, and LLaMA-65B is competitive with the best models, Chinchilla-70B and PaLM-540B. We release all our models to the research community.},
	urldate = {2023-03-22},
	publisher = {arXiv},
	author = {Touvron, Hugo and Lavril, Thibaut and Izacard, Gautier and Martinet, Xavier and Lachaux, Marie-Anne and Lacroix, Timothée and Rozière, Baptiste and Goyal, Naman and Hambro, Eric and Azhar, Faisal and Rodriguez, Aurelien and Joulin, Armand and Grave, Edouard and Lample, Guillaume},
	month = feb,
	year = {2023},
	note = {arXiv:2302.13971 [cs]},
	keywords = {Computer Science - Computation and Language},
}

@article{mitchell_debate_2023,
	title = {The debate over understanding in {AI}’s large language models},
	volume = {120},
	issn = {0027-8424, 1091-6490},
	url = {https://pnas.org/doi/10.1073/pnas.2215907120},
	doi = {10.1073/pnas.2215907120},
	abstract = {We survey a current, heated debate in the artificial intelligence (AI) research community on whether large pretrained language models can be said to understand language—and the physical and social situations language encodes—in any humanlike sense. We describe arguments that have been made for and against such understanding and key questions for the broader sciences of intelligence that have arisen in light of these arguments. We contend that an extended science of intelligence can be developed that will provide insight into distinct modes of understanding, their strengths and limitations, and the challenge of integrating diverse forms of cognition.},
	language = {en},
	number = {13},
	urldate = {2023-03-23},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Mitchell, Melanie and Krakauer, David C.},
	month = mar,
	year = {2023},
	keywords = {GPT-3, NLP, LM, cognitive science, semantics, understanding, meaning, pragmatics},
	pages = {e2215907120},
}

@book{desmond_evicted_2016,
	address = {New York},
	edition = {First Edition},
	title = {Evicted: poverty and profit in the {American} city},
	isbn = {978-0-553-44743-9 978-0-553-44745-3},
	shorttitle = {Evicted},
	publisher = {Crown Publishers},
	author = {Desmond, Matthew},
	year = {2016},
	keywords = {non-fiction, bookshelf, sociology, 2023},
}

@inproceedings{andreas_language_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {Language models as agent models},
	url = {https://aclanthology.org/2022.findings-emnlp.423},
	abstract = {Language models (LMs) are trained on collections of documents, written by individual human agents to achieve specific goals in the outside world. During training, LMs have access only to text of these documents, with no direct evidence of the internal states of the agents that produced them—a fact often used to argue that LMs are incapable of modeling goal-directed aspects of human language production and comprehension. Can LMs trained on text learn anything at all about the relationship between language and use? I argue that LMs are models of communicative intentions in a specific, narrow sense. When performing next word prediction given a textual context, an LM can infer and represent properties of an agent likely to have produced that context. These representations can in turn influence subsequent LM generation in the same way that agents' communicative intentions influence their language. I survey findings from the recent literature showing that—even in today's non-robust and error-prone models—LMs infer and use representations of fine-grained communicative intentions and high-level beliefs and goals. Despite the limited nature of their training data, they can thus serve as building blocks for systems that communicate and act intentionally.},
	urldate = {2023-03-31},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2022},
	publisher = {Association for Computational Linguistics},
	author = {Andreas, Jacob},
	month = dec,
	year = {2022},
	keywords = {NLP, LM, intentional stance, belief, intentionality},
	pages = {5769--5779},
}

@inproceedings{ouyang_training_2022,
	title = {Training language models to follow instructions with human feedback},
	volume = {35},
	url = {https://proceedings.neurips.cc/paper_files/paper/2022/file/b1efde53be364a73914f58805a001731-Paper-Conference.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and Schulman, John and Hilton, Jacob and Kelton, Fraser and Miller, Luke and Simens, Maddie and Askell, Amanda and Welinder, Peter and Christiano, Paul F and Leike, Jan and Lowe, Ryan},
	editor = {Koyejo, S. and Mohamed, S. and Agarwal, A. and Belgrave, D. and Cho, K. and Oh, A.},
	year = {2022},
	pages = {27730--27744},
}

@article{sejnowski_large_2023,
	title = {Large language models and the reverse {Turing} test},
	volume = {35},
	issn = {0899-7667, 1530-888X},
	url = {https://direct.mit.edu/neco/article/35/3/309/114731/Large-Language-Models-and-the-Reverse-Turing-Test},
	doi = {10.1162/neco_a_01563},
	abstract = {Large language models (LLMs) have been transformative. They are pretrained foundational models that are self-supervised and can be adapted with fine-tuning to a wide range of natural language tasks, each of which previously would have required a separate network model. This is one step closer to the extraordinary versatility of human language. GPT-3 and, more recently, LaMDA, both of them LLMs, can carry on dialogs with humans on many topics after minimal priming with a few examples. However, there has been a wide range of reactions and debate on whether these LLMs understand what they are saying or exhibit signs of intelligence. This high variance is exhibited in three interviews with LLMs reaching wildly different conclusions. A new possibility was uncovered that could explain this divergence. What appears to be intelligence in LLMs may in fact be a mirror that reflects the intelligence of the interviewer, a remarkable twist that could be considered a reverse Turing test. If so, then by studying interviews, we may be learning more about the intelligence and beliefs of the interviewer than the intelligence of the LLMs. As LLMs become more capable, they may transform the way we interact with machines and how they interact with each other. Increasingly, LLMs are being coupled with sensorimotor devices. LLMs can talk the talk, but can they walk the walk? A road map for achieving artificial general autonomy is outlined with seven major improvements inspired by brain systems and how LLMs could in turn be used to uncover new insights into brain function.},
	language = {en},
	number = {3},
	urldate = {2023-04-12},
	journal = {Neural Computation},
	author = {Sejnowski, Terrence J.},
	month = feb,
	year = {2023},
	keywords = {LM, computational cognitive neuroscience, Turing test},
	pages = {309--342},
	file = {Full Text:/Users/kriarm/Zotero/storage/V8EP6XMT/Sejnowski - 2023 - Large Language Models and the Reverse Turing Test.pdf:application/pdf},
}

@inproceedings{armeni_characterizing_2022,
	address = {Abu Dhabi, United Arab Emirates (Hybrid)},
	title = {Characterizing verbatim short-term memory in neural language models},
	url = {https://aclanthology.org/2022.conll-1.28},
	abstract = {When a language model is trained to predict natural language sequences, its prediction at each moment depends on a representation of prior context. What kind of information about the prior context can language models retrieve? We tested whether language models could retrieve the exact words that occurred previously in a text. In our paradigm, language models (transformers and an LSTM) processed English text in which a list of nouns occurred twice. We operationalized retrieval as the reduction in surprisal from the first to the second list. We found that the transformers retrieved both the identity and ordering of nouns from the first list. Further, the transformers' retrieval was markedly enhanced when they were trained on a larger corpus and with greater model depth. Lastly, their ability to index prior tokens was dependent on learned attention patterns. In contrast, the LSTM exhibited less precise retrieval, which was limited to list-initial tokens and to short intervening texts. The LSTM's retrieval was not sensitive to the order of nouns and it improved when the list was semantically coherent. We conclude that transformers implemented something akin to a working memory system that could flexibly retrieve individual token representations across arbitrary delays; conversely, the LSTM maintained a coarser and more rapidly-decaying semantic gist of prior tokens, weighted toward the earliest items.},
	booktitle = {Proceedings of the 26th {Conference} on {Computational} {Natural} {Language} {Learning} ({CoNLL})},
	publisher = {Association for Computational Linguistics},
	author = {Armeni, Kristijan and Honey, Christopher and Linzen, Tal},
	month = dec,
	year = {2022},
	pages = {405--424},
}

@article{kruschke_bayesian_2018,
	title = {The {Bayesian} {New} {Statistics}: {Hypothesis} testing, estimation, meta-analysis, and power analysis from a {Bayesian} perspective},
	volume = {25},
	issn = {1069-9384, 1531-5320},
	shorttitle = {The {Bayesian} {New} {Statistics}},
	url = {http://link.springer.com/10.3758/s13423-016-1221-4},
	doi = {10.3758/s13423-016-1221-4},
	language = {en},
	number = {1},
	urldate = {2023-04-24},
	journal = {Psychonomic Bulletin \& Review},
	author = {Kruschke, John K. and Liddell, Torrin M.},
	month = feb,
	year = {2018},
	keywords = {frequentist statistics, Bayes theorem, confidence interval, statistical inference, HDI, p-value},
	pages = {178--206},
}

@inproceedings{bastings_elephant_2020,
	address = {Online},
	title = {The elephant in the interpretability room: {Why} use attention as explanation when we have saliency methods?},
	shorttitle = {The elephant in the interpretability room},
	url = {https://aclanthology.org/2020.blackboxnlp-1.14},
	doi = {10.18653/v1/2020.blackboxnlp-1.14},
	abstract = {There is a recent surge of interest in using attention as explanation of model predictions, with mixed evidence on whether attention can be used as such. While attention conveniently gives us one weight per input token and is easily extracted, it is often unclear toward what goal it is used as explanation. We find that often that goal, whether explicitly stated or not, is to find out what input tokens are the most relevant to a prediction, and that the implied user for the explanation is a model developer. For this goal and user, we argue that input saliency methods are better suited, and that there are no compelling reasons to use attention, despite the coincidence that it provides a weight for each input. With this position paper, we hope to shift some of the recent focus on attention to saliency methods, and for authors to clearly state the goal and user for their explanations.},
	urldate = {2023-05-02},
	booktitle = {Proceedings of the {Third} {BlackboxNLP} {Workshop} on {Analyzing} and {Interpreting} {Neural} {Networks} for {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Bastings, Jasmijn and Filippova, Katja},
	month = nov,
	year = {2020},
	pages = {149--155},
}

@inproceedings{jain_attention_2019,
	address = {Minneapolis, Minnesota},
	title = {Attention is not {Explanation}},
	url = {https://aclanthology.org/N19-1357},
	doi = {10.18653/v1/N19-1357},
	abstract = {Attention mechanisms have seen wide adoption in neural NLP models. In addition to improving predictive performance, these are often touted as affording transparency: models equipped with attention provide a distribution over attended-to input units, and this is often presented (at least implicitly) as communicating the relative importance of inputs. However, it is unclear what relationship exists between attention weights and model outputs. In this work we perform extensive experiments across a variety of NLP tasks that aim to assess the degree to which attention weights provide meaningful “explanations” for predictions. We find that they largely do not. For example, learned attention weights are frequently uncorrelated with gradient-based measures of feature importance, and one can identify very different attention distributions that nonetheless yield equivalent predictions. Our findings show that standard attention modules do not provide meaningful explanations and should not be treated as though they do.},
	urldate = {2023-05-02},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Jain, Sarthak and Wallace, Byron C.},
	month = jun,
	year = {2019},
	pages = {3543--3556},
	file = {Full Text PDF:/Users/kriarm/Zotero/storage/KC9P7MPE/Jain and Wallace - 2019 - Attention is not Explanation.pdf:application/pdf},
}

@inproceedings{wiegreffe_attention_2019,
	address = {Hong Kong, China},
	title = {Attention is not not {Explanation}},
	url = {https://aclanthology.org/D19-1002},
	doi = {10.18653/v1/D19-1002},
	abstract = {Attention mechanisms play a central role in NLP systems, especially within recurrent neural network (RNN) models. Recently, there has been increasing interest in whether or not the intermediate representations offered by these modules may be used to explain the reasoning for a model's prediction, and consequently reach insights regarding the model's decision-making process. A recent paper claims that `Attention is not Explanation' (Jain and Wallace, 2019). We challenge many of the assumptions underlying this work, arguing that such a claim depends on one's definition of explanation, and that testing it needs to take into account all elements of the model. We propose four alternative tests to determine when/whether attention can be used as explanation: a simple uniform-weights baseline; a variance calibration based on multiple random seed runs; a diagnostic framework using frozen weights from pretrained models; and an end-to-end adversarial attention training protocol. Each allows for meaningful interpretation of attention mechanisms in RNN models. We show that even when reliable adversarial distributions can be found, they don't perform well on the simple diagnostic, indicating that prior work does not disprove the usefulness of attention mechanisms for explainability.},
	urldate = {2023-05-02},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Wiegreffe, Sarah and Pinter, Yuval},
	month = nov,
	year = {2019},
	keywords = {attention, explanation, interpretability, LMs},
	pages = {11--20},
}

@inproceedings{lakretz_can_2022,
	address = {Gyeongju, Republic of Korea},
	title = {Can transformers process recursive nested constructions, like humans?},
	url = {https://aclanthology.org/2022.coling-1.285},
	abstract = {Recursive processing is considered a hallmark of human linguistic abilities. A recent study evaluated recursive processing in recurrent neural language models (RNN-LMs) and showed that such models perform below chance level on embedded dependencies within nested constructions – a prototypical example of recursion in natural language. Here, we study if state-of-the-art Transformer LMs do any better. We test eight different Transformer LMs on two different types of nested constructions, which differ in whether the embedded (inner) dependency is short or long range. We find that Transformers achieve near-perfect performance on short-range embedded dependencies, significantly better than previous results reported for RNN-LMs and humans. However, on long-range embedded dependencies, Transformers' performance sharply drops below chance level. Remarkably, the addition of only three words to the embedded dependency caused Transformers to fall from near-perfect to below-chance performance. Taken together, our results reveal how brittle syntactic processing is in Transformers, compared to humans.},
	urldate = {2023-05-07},
	booktitle = {Proceedings of the 29th {International} {Conference} on {Computational} {Linguistics}},
	publisher = {International Committee on Computational Linguistics},
	author = {Lakretz, Yair and Desbordes, Théo and Hupkes, Dieuwke and Dehaene, Stanislas},
	month = oct,
	year = {2022},
	pages = {3226--3232},
}

@inproceedings{wang_interpretability_2023,
	title = {Interpretability in the wild: a circuit for indirect object identification in {GPT}-2 small},
	url = {https://openreview.net/forum?id=NpsVSN6o4ul},
	booktitle = {The {Eleventh} {International} {Conference} on {Learning} {Representations}},
	author = {Wang, Kevin Ro and Variengien, Alexandre and Conmy, Arthur and Shlegeris, Buck and Steinhardt, Jacob},
	year = {2023},
}

@inproceedings{frankle_lottery_2018,
	title = {The {Lottery} {Ticket} {Hypothesis}: {Finding} sparse, trainable neural networks},
	shorttitle = {The {Lottery} {Ticket} {Hypothesis}},
	url = {https://openreview.net/forum?id=rJl-b3RcF7},
	abstract = {Neural network pruning techniques can reduce the parameter counts of trained networks by over 90\%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance. We find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the "lottery ticket hypothesis:" dense, randomly-initialized, feed-forward networks contain subnetworks ("winning tickets") that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective. We present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20\% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy.},
	language = {en},
	urldate = {2023-05-24},
	author = {Frankle, Jonathan and Carbin, Michael},
	month = dec,
	year = {2018},
	file = {Full Text PDF:/Users/kriarm/Zotero/storage/HHTV6NZY/Frankle and Carbin - 2018 - The Lottery Ticket Hypothesis Finding Sparse, Tra.pdf:application/pdf},
}

@inproceedings{lecun_optimal_1989,
	title = {Optimal brain damage},
	volume = {2},
	url = {https://proceedings.neurips.cc/paper_files/paper/1989/file/6c9882bbac1c7093bd25041881277658-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Morgan-Kaufmann},
	author = {LeCun, Yann and Denker, John and Solla, Sara},
	editor = {Touretzky, D.},
	year = {1989},
}

@article{elhage_mathematical_2021,
	title = {A mathematical framework for transformer circuits},
	journal = {Transformer Circuits Thread},
	author = {Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
	year = {2021},
}

@misc{bietti_birth_2023,
	title = {Birth of a transformer: {A} memory viewpoint},
	shorttitle = {Birth of a {Transformer}},
	url = {http://arxiv.org/abs/2306.00802},
	doi = {10.48550/arXiv.2306.00802},
	abstract = {Large language models based on transformers have achieved great empirical successes. However, as they are deployed more widely, there is a growing need to better understand their internal mechanisms in order to make them more reliable. These models appear to store vast amounts of knowledge from their training data, and to adapt quickly to new information provided in their context or prompt. We study how transformers balance these two types of knowledge by considering a synthetic setup where tokens are generated from either global or context-specific bigram distributions. By a careful empirical analysis of the training process on a simplified two-layer transformer, we illustrate the fast learning of global bigrams and the slower development of an "induction head" mechanism for the in-context bigrams. We highlight the role of weight matrices as associative memories, provide theoretical insights on how gradients enable their learning during training, and study the role of data-distributional properties.},
	urldate = {2023-06-07},
	publisher = {arXiv},
	author = {Bietti, Alberto and Cabannes, Vivien and Bouchacourt, Diane and Jegou, Herve and Bottou, Leon},
	month = jun,
	year = {2023},
	note = {arXiv:2306.00802 [cs, stat]},
	keywords = {Computer Science - Computation and Language, Computer Science - Machine Learning, Statistics - Machine Learning},
}

@article{mitchell_abstraction_2023,
	title = {Abstraction and analogy in {AI}},
	issn = {0077-8923, 1749-6632},
	url = {https://nyaspubs.onlinelibrary.wiley.com/doi/10.1111/nyas.14995},
	doi = {10.1111/nyas.14995},
	language = {en},
	urldate = {2023-06-07},
	journal = {Annals of the New York Academy of Sciences},
	author = {Mitchell, Melanie},
	month = apr,
	year = {2023},
	pages = {nyas.14995},
}

@article{gebhart_science_2023,
	title = {Science shouldn't give data brokers cover for stealing your privacy},
	url = {https://www.scientificamerican.com/article/science-shouldnt-give-data-brokers-cover-for-stealing-your-privacy/},
	abstract = {In the guise of collecting scientific data, data brokers are running a massive privacy invasion. Researchers should stop helping them},
	language = {en},
	urldate = {2023-06-27},
	journal = {Scientific American},
	author = {Gebhart, Gennie and Richman, Josh},
	month = jun,
	year = {2023},
}

@inproceedings{vig_analyzing_2019-1,
	address = {Florence, Italy},
	title = {Analyzing the structure of attention in a transformer language model},
	url = {https://aclanthology.org/W19-4808},
	doi = {10.18653/v1/W19-4808},
	abstract = {The Transformer is a fully attention-based alternative to recurrent networks that has achieved state-of-the-art results across a range of NLP tasks. In this paper, we analyze the structure of attention in a Transformer language model, the GPT-2 small pretrained model. We visualize attention for individual instances and analyze the interaction between attention and syntax over a large corpus. We find that attention targets different parts of speech at different layer depths within the model, and that attention aligns with dependency relations most strongly in the middle layers. We also find that the deepest layers of the model capture the most distant relationships. Finally, we extract exemplar sentences that reveal highly specific patterns targeted by particular attention heads.},
	urldate = {2023-07-13},
	booktitle = {Proceedings of the 2019 {ACL} {Workshop} {BlackboxNLP}: {Analyzing} and {Interpreting} {Neural} {Networks} for {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Vig, Jesse and Belinkov, Yonatan},
	month = aug,
	year = {2019},
	pages = {63--76},
	file = {Full Text PDF:/Users/kriarm/Zotero/storage/VXDY765P/Vig and Belinkov - 2019 - Analyzing the Structure of Attention in a Transfor.pdf:application/pdf},
}

@book{bulgakov_master_2021,
	address = {New York},
	title = {The master and margarita},
	isbn = {978-1-4197-5650-4},
	publisher = {The Overlook Press},
	author = {Bulgakov, Mikhail and Burgin, Diana and Tiernan O'Connor, Katherine},
	year = {2021},
	keywords = {fiction, 2023},
}

@inproceedings{liesenfeld_opening_2023,
	address = {New York, NY, USA},
	series = {{CUI} '23},
	title = {Opening up {ChatGPT}: {Tracking} openness, transparency, and accountability in instruction-tuned text generators},
	isbn = {9798400700149},
	shorttitle = {Opening up {ChatGPT}},
	url = {https://doi.org/10.1145/3571884.3604316},
	doi = {10.1145/3571884.3604316},
	abstract = {Large language models that exhibit instruction-following behaviour represent one of the biggest recent upheavals in conversational interfaces, a trend in large part fuelled by the release of OpenAI’s ChatGPT, a proprietary large language model for text generation fine-tuned through reinforcement learning from human feedback (LLM+RLHF). We review the risks of relying on proprietary software and survey the first crop of open-source projects of comparable architecture and functionality. The main contribution of this paper is to show that openness is differentiated, and to offer scientific documentation of degrees of openness in this fast-moving field. We evaluate projects in terms of openness of code, training data, model weights, RLHF data, licensing, scientific documentation, and access methods. We find that while there is a fast-growing list of projects billing themselves as ‘open source’, many inherit undocumented data of dubious legality, few share the all-important instruction-tuning (a key site where human annotation labour is involved), and careful scientific documentation is exceedingly rare. Degrees of openness are relevant to fairness and accountability at all points, from data collection and curation to model architecture, and from training and fine-tuning to release and deployment.},
	urldate = {2023-07-18},
	booktitle = {Proceedings of the 5th {International} {Conference} on {Conversational} {User} {Interfaces}},
	publisher = {Association for Computing Machinery},
	author = {Liesenfeld, Andreas and Lopez, Alianda and Dingemanse, Mark},
	month = jul,
	year = {2023},
	keywords = {open science, LM, open source, open data, chatGPT, large language models, RLHF, survey, model cards},
	pages = {1--6},
}

@misc{liu_pre-train_2021,
	title = {Pre-train, prompt, and predict: {A} systematic survey of prompting methods in natural language processing},
	shorttitle = {Pre-train, {Prompt}, and {Predict}},
	url = {http://arxiv.org/abs/2107.13586},
	doi = {10.48550/arXiv.2107.13586},
	abstract = {This paper surveys and organizes research works in a new paradigm in natural language processing, which we dub "prompt-based learning". Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y{\textbar}x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x' that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: it allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this paper we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g.the choice of pre-trained models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts, but also release other resources, e.g., a website http://pretrain.nlpedia.ai/ including constantly-updated survey, and paperlist.},
	urldate = {2023-07-18},
	publisher = {arXiv},
	author = {Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
	month = jul,
	year = {2021},
	note = {arXiv:2107.13586 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
}

@misc{von_oswald_transformers_2023,
	title = {Transformers learn in-context by gradient descent},
	url = {http://arxiv.org/abs/2212.07677},
	doi = {10.48550/arXiv.2212.07677},
	abstract = {At present, the mechanisms of in-context learning in Transformers are not well understood and remain mostly an intuition. In this paper, we suggest that training Transformers on auto-regressive objectives is closely related to gradient-based meta-learning formulations. We start by providing a simple weight construction that shows the equivalence of data transformations induced by 1) a single linear self-attention layer and by 2) gradient-descent (GD) on a regression loss. Motivated by that construction, we show empirically that when training self-attention-only Transformers on simple regression tasks either the models learned by GD and Transformers show great similarity or, remarkably, the weights found by optimization match the construction. Thus we show how trained Transformers become mesa-optimizers i.e. learn models by gradient descent in their forward pass. This allows us, at least in the domain of regression problems, to mechanistically understand the inner workings of in-context learning in optimized Transformers. Building on this insight, we furthermore identify how Transformers surpass the performance of plain gradient descent by learning an iterative curvature correction and learn linear models on deep data representations to solve non-linear regression tasks. Finally, we discuss intriguing parallels to a mechanism identified to be crucial for in-context learning termed induction-head (Olsson et al., 2022) and show how it could be understood as a specific case of in-context learning by gradient descent learning within Transformers. Code to reproduce the experiments can be found at https://github.com/google-research/self-organising-systems/tree/master/transformers\_learn\_icl\_by\_gd .},
	urldate = {2023-07-18},
	publisher = {arXiv},
	author = {von Oswald, Johannes and Niklasson, Eyvind and Randazzo, Ettore and Sacramento, João and Mordvintsev, Alexander and Zhmoginov, Andrey and Vladymyrov, Max},
	month = may,
	year = {2023},
	note = {arXiv:2212.07677 [cs]},
	keywords = {LM, gradient descient, in-context learning, induction head},
}

@misc{chen_how_2023,
	title = {How is {ChatGPT}'s behavior changing over time?},
	url = {http://arxiv.org/abs/2307.09009},
	doi = {10.48550/arXiv.2307.09009},
	abstract = {GPT-3.5 and GPT-4 are the two most widely used large language model (LLM) services. However, when and how these models are updated over time is opaque. Here, we evaluate the March 2023 and June 2023 versions of GPT-3.5 and GPT-4 on four diverse tasks: 1) solving math problems, 2) answering sensitive/dangerous questions, 3) generating code and 4) visual reasoning. We find that the performance and behavior of both GPT-3.5 and GPT-4 can vary greatly over time. For example, GPT-4 (March 2023) was very good at identifying prime numbers (accuracy 97.6\%) but GPT-4 (June 2023) was very poor on these same questions (accuracy 2.4\%). Interestingly GPT-3.5 (June 2023) was much better than GPT-3.5 (March 2023) in this task. GPT-4 was less willing to answer sensitive questions in June than in March, and both GPT-4 and GPT-3.5 had more formatting mistakes in code generation in June than in March. Overall, our findings shows that the behavior of the same LLM service can change substantially in a relatively short amount of time, highlighting the need for continuous monitoring of LLM quality.},
	urldate = {2023-07-19},
	publisher = {arXiv},
	author = {Chen, Lingjiao and Zaharia, Matei and Zou, James},
	month = jul,
	year = {2023},
	note = {arXiv:2307.09009 [cs]},
	keywords = {ChatGPT, LLM, GPT-3.5, GPT-4},
}

@misc{raccah_memory_2022,
	title = {Memory in humans and deep language models: {Linking} hypotheses for model augmentation},
	shorttitle = {Memory in humans and deep language models},
	url = {http://arxiv.org/abs/2210.01869},
	doi = {10.48550/arXiv.2210.01869},
	abstract = {The computational complexity of the self-attention mechanism in Transformer models significantly limits their ability to generalize over long temporal durations. Memory-augmentation, or the explicit storing of past information in external memory for subsequent predictions, has become a constructive avenue for mitigating this limitation. We argue that memory-augmented Transformers can benefit substantially from considering insights from the memory literature in humans. We detail an approach for integrating evidence from the human memory system through the specification of cross-domain linking hypotheses. We then provide an empirical demonstration to evaluate the use of surprisal as a linking hypothesis, and further identify the limitations of this approach to inform future research.},
	urldate = {2023-07-19},
	publisher = {arXiv},
	author = {Raccah, Omri and Chen, Phoebe and Willke, Ted L. and Poeppel, David and Vo, Vy A.},
	month = nov,
	year = {2022},
	note = {arXiv:2210.01869 [cs]},
	keywords = {LM, transformer, surprisal, memory, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/kriarm/Zotero/storage/6NIW88I8/Raccah et al. - 2022 - Memory in humans and deep language models Linking.pdf:application/pdf;arXiv.org Snapshot:/Users/kriarm/Zotero/storage/PVD5TMQW/2210.html:text/html},
}

@book{nesvold_off-earth_2023,
	address = {Cambridge, Massachusetts},
	title = {Off-{Earth}: ethical questions and quandaries for living in outer space},
	isbn = {978-0-262-04754-8},
	shorttitle = {Off-{Earth}},
	abstract = {"Trade book looking at social and ethical issues of human space settlement"--},
	publisher = {The MIT Press},
	author = {Nesvold, Erika},
	year = {2023},
	keywords = {ethics, space ethics},
}

@misc{noauthor_zero--binder_nodate,
	title = {Zero-to-{Binder} — {The} {Turing} {Way}},
	url = {https://the-turing-way.netlify.app/communication/binder/zero-to-binder.html#changing-the-interface},
	urldate = {2023-07-22},
	file = {Zero-to-Binder — The Turing Way:/Users/kriarm/Zotero/storage/U4VNXZ5C/zero-to-binder.html:text/html},
}

@article{brembs_mastodon_2023,
	title = {Mastodon over {Mammon}: towards publicly owned scholarly knowledge},
	volume = {10},
	issn = {2054-5703},
	shorttitle = {Mastodon over {Mammon}},
	url = {https://royalsocietypublishing.org/doi/10.1098/rsos.230207},
	doi = {10.1098/rsos.230207},
	abstract = {Twitter is in turmoil and the scholarly community on the platform is once again starting to migrate. As with the early internet, scholarly organizations are at the forefront of developing and implementing a decentralized alternative to Twitter, Mastodon. Both historically and conceptually, this is not a new situation for the scholarly community. Historically, scholars were forced to leave social media platform FriendFeed after it was bought by Facebook in 2006. Conceptually, the problems associated with public scholarly discourse subjected to the whims of corporate owners are not unlike those of scholarly journals owned by monopolistic corporations: in both cases the perils associated with a public good in private hands are palpable. For both short form (Twitter/Mastodon) and longer form (journals) scholarly discourse, decentralized solutions exist, some of which are already enjoying some institutional support. Here we argue that scholarly organizations, in particular learned societies, are now facing a golden opportunity to rethink their hesitations towards such alternatives and support the migration of the scholarly community from Twitter to Mastodon by hosting Mastodon instances. Demonstrating that the scholarly community is capable of creating a truly public square for scholarly discourse, impervious to private takeover, might renew confidence and inspire the community to focus on analogous solutions for the remaining scholarly record—encompassing text, data and code—to safeguard all publicly owned scholarly knowledge.},
	language = {en},
	number = {7},
	urldate = {2023-07-25},
	journal = {Royal Society Open Science},
	author = {Brembs, Björn and Lenardic, Adrian and Murray-Rust, Peter and Chan, Leslie and Irawan, Dasapta Erwin},
	month = jul,
	year = {2023},
	pages = {230207},
}

@article{zhang_illusion_2023,
	title = {An illusion of predictability in scientific results: {Even} experts confuse inferential uncertainty and outcome variability},
	volume = {120},
	issn = {0027-8424, 1091-6490},
	shorttitle = {An illusion of predictability in scientific results},
	url = {https://pnas.org/doi/10.1073/pnas.2302491120},
	doi = {10.1073/pnas.2302491120},
	abstract = {Traditionally, scientists have placed more emphasis on communicating inferential uncertainty (i.e., the precision of statistical estimates) compared to outcome variability (i.e., the predictability of individual outcomes). Here, we show that this can lead to sizable misperceptions about the implications of scientific results. Specifically, we present three preregistered, randomized experiments where participants saw the same scientific findings visualized as showing only inferential uncertainty, only outcome variability, or both and answered questions about the size and importance of findings they were shown. Our results, composed of responses from medical professionals, professional data scientists, and tenure-track faculty, show that the prevalent form of visualizing only inferential uncertainty can lead to significant overestimates of treatment effects, even among highly trained experts. In contrast, we find that depicting both inferential uncertainty and outcome variability leads to more accurate perceptions of results while appearing to leave other subjective impressions of the results unchanged, on average.},
	language = {en},
	number = {33},
	urldate = {2023-08-18},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Zhang, Sam and Heck, Patrick R. and Meyer, Michelle N. and Chabris, Christopher F. and Goldstein, Daniel G. and Hofman, Jake M.},
	month = aug,
	year = {2023},
	keywords = {methods, statistics, dataviz, statistical inference, SEM, SD},
	pages = {e2302491120},
}

@misc{widder_open_2023,
	address = {Rochester, NY},
	type = {{SSRN} {Scholarly} {Paper}},
	title = {Open ({For} {Business}): {Big} tech, concentrated power, and the political economy of {Open} {AI}},
	shorttitle = {Open ({For} {Business})},
	url = {https://papers.ssrn.com/abstract=4543807},
	abstract = {This paper examines ‘open’ AI in the context of recent attention to open and open source AI systems. We find that the terms ‘open’ and ‘open source’ are used in confusing and diverse ways, often constituting more aspiration or marketing than technical descriptor, and frequently blending concepts from both open source software and open science. This complicates an already complex landscape, in which there is currently no agreed on definition of ‘open’ in the context of AI, and as such the term is being applied to widely divergent offerings with little reference to a stable descriptor. So, what exactly is ‘open’ about ‘open’ AI, and what does ‘open’ AI enable? To better answer these questions we begin this paper by looking at the various resources required to create and deploy AI systems, alongside the components that comprise these systems. We do this with an eye to which of these can, or cannot, be made open to scrutiny, reuse, and extension. What does ‘open’ mean in practice, and what are its limits in the context of AI? We find that while a handful of maximally open AI systems exist, which offer intentional and extensive transparency, reusability, and extensibility– the resources needed to build AI from scratch, and to deploy large AI systems at scale, remain ‘closed’—available only to those with significant (almost always corporate) resources. From here, we zoom out and examine the history of open source, its cleave from free software in the mid 1990s, and the contested processes by which open source has been incorporated into, and instrumented by, large tech corporations. As a current day example of the overbroad and ill-defined use of the term by tech companies, we look at  ‘open’ in the context of OpenAI the company. We trace its moves from a humanity-focused nonprofit to a for-profit partnered with Microsoft, and its shifting position on ‘open’ AI. Finally, we examine the current discourse around ‘open’ AI–looking at how the term and the (mis)understandings about what ‘open’ enables are being deployed to shape the public’s and policymakers’ understanding about AI, its capabilities, and the power of the AI industry. In particular, we examine the arguments being made for and against ‘open’ and open source AI, who’s making them, and how they are being deployed in the debate over AI regulation. Taken together, we find that ‘open’ AI can, in its more maximal instantiations, provide transparency, reusability, and extensibility that can enable third parties to deploy and build on top of powerful off-the-shelf AI models. These maximalist forms of ‘open’ AI can also allow some forms of auditing and oversight. But even the most open of ‘open’ AI systems do not, on their own, ensure democratic access to or meaningful competition in AI, nor does openness alone solve the problem of oversight and scrutiny. While we recognize that there is a vibrant community of earnest contributors building and contributing to ‘open’ AI efforts in the name of expanding access and insight, we also find that marketing around openness and investment in (somewhat) open AI systems is being leveraged by powerful companies to bolster their positions in the face of growing interest in AI regulation. And that some companies have moved to embrace ‘open’ AI as a mechanism to entrench dominance, using the rhetoric of ‘open’ AI to expand market power while investing in ‘open’ AI efforts in ways that allow them to set standards of development while benefiting from the free labor of open source contributors.},
	language = {en},
	urldate = {2023-08-18},
	author = {Widder, David Gray and West, Sarah and Whittaker, Meredith},
	month = aug,
	year = {2023},
	keywords = {ethics, open source, 08-2023},
}

@book{noauthor_notitle_nodate,
}

@misc{liu_lost_2023,
	title = {Lost in the middle: {How} language models use long contexts},
	shorttitle = {Lost in the {Middle}},
	url = {http://arxiv.org/abs/2307.03172},
	doi = {10.48550/arXiv.2307.03172},
	abstract = {While recent language models have the ability to take long contexts as input, relatively little is known about how well they use longer context. We analyze language model performance on two tasks that require identifying relevant information within their input contexts: multi-document question answering and key-value retrieval. We find that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts. Furthermore, performance substantially decreases as the input context grows longer, even for explicitly long-context models. Our analysis provides a better understanding of how language models use their input context and provides new evaluation protocols for future long-context models.},
	urldate = {2023-08-24},
	publisher = {arXiv},
	author = {Liu, Nelson F. and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
	month = jul,
	year = {2023},
	note = {arXiv:2307.03172 [cs]},
	keywords = {LM, transformer, memory, context, ChatGPT, GPT-3.5, document retrieval},
}

@misc{achterberg_building_2023,
	title = {Building artificial neural circuits for domain-general cognition: a primer on brain-inspired systems-level architecture},
	shorttitle = {Building artificial neural circuits for domain-general cognition},
	url = {http://arxiv.org/abs/2303.13651},
	doi = {10.48550/arXiv.2303.13651},
	abstract = {There is a concerted effort to build domain-general artificial intelligence in the form of universal neural network models with sufficient computational flexibility to solve a wide variety of cognitive tasks but without requiring fine-tuning on individual problem spaces and domains. To do this, models need appropriate priors and inductive biases, such that trained models can generalise to out-of-distribution examples and new problem sets. Here we provide an overview of the hallmarks endowing biological neural networks with the functionality needed for flexible cognition, in order to establish which features might also be important to achieve similar functionality in artificial systems. We specifically discuss the role of system-level distribution of network communication and recurrence, in addition to the role of short-term topological changes for efficient local computation. As machine learning models become more complex, these principles may provide valuable directions in an otherwise vast space of possible architectures. In addition, testing these inductive biases within artificial systems may help us to understand the biological principles underlying domain-general cognition.},
	urldate = {2023-09-06},
	publisher = {arXiv},
	author = {Achterberg, Jascha and Akarca, Danyal and Assem, Moataz and Heimbach, Moritz and Astle, Duncan E. and Duncan, John},
	month = mar,
	year = {2023},
	note = {arXiv:2303.13651 [cs, q-bio]},
	keywords = {ANN, GPT-4, multiple-demand system, neuroscience, transformer},
}

@misc{noauthor_3_nodate,
	title = {3 {Hypotheses} for {Open} {Workers}},
	url = {https://www.codeforsociety.org/news/3-hypotheses-for-open-workers},
	abstract = {We are in a moment of public reckoning about the governance of digital technologies and the future of civil society. How have ideas of openness, freedom, and decentralization served anti-corporate struggles? Where have they proved ineffective at challenging the status quo?},
	language = {en-US},
	urldate = {2023-09-07},
	journal = {Code for Science \& Society},
	keywords = {decentralization, gnu, governance, open science, open source},
	file = {Snapshot:/Users/kriarm/Zotero/storage/JZN4SVS8/3-hypotheses-for-open-workers.html:text/html},
}

@misc{wang_interpretability_2022,
	title = {Interpretability in the wild: a circuit for indirect object identification in {GPT}-2 small},
	shorttitle = {Interpretability in the {Wild}},
	url = {http://arxiv.org/abs/2211.00593},
	doi = {10.48550/arXiv.2211.00593},
	abstract = {Research in mechanistic interpretability seeks to explain behaviors of machine learning models in terms of their internal components. However, most previous work either focuses on simple behaviors in small models, or describes complicated behaviors in larger models with broad strokes. In this work, we bridge this gap by presenting an explanation for how GPT-2 small performs a natural language task called indirect object identification (IOI). Our explanation encompasses 26 attention heads grouped into 7 main classes, which we discovered using a combination of interpretability approaches relying on causal interventions. To our knowledge, this investigation is the largest end-to-end attempt at reverse-engineering a natural behavior "in the wild" in a language model. We evaluate the reliability of our explanation using three quantitative criteria--faithfulness, completeness and minimality. Though these criteria support our explanation, they also point to remaining gaps in our understanding. Our work provides evidence that a mechanistic understanding of large ML models is feasible, opening opportunities to scale our understanding to both larger models and more complex tasks.},
	urldate = {2023-09-19},
	publisher = {arXiv},
	author = {Wang, Kevin and Variengien, Alexandre and Conmy, Arthur and Shlegeris, Buck and Steinhardt, Jacob},
	month = nov,
	year = {2022},
	note = {arXiv:2211.00593 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:/Users/kriarm/Zotero/storage/6D65ERGB/2211.html:text/html;Wang et al. - 2022 - Interpretability in the wild a circuit for indire.pdf:/Users/kriarm/Zotero/storage/G4T2C79N/Wang et al. - 2022 - Interpretability in the wild a circuit for indire.pdf:application/pdf},
}
