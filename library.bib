
@article{floridi_gpt-3_2020,
	title = {{GPT}-3: {Its} nature, scope, limits, and consequences},
	issn = {0924-6495, 1572-8641},
	shorttitle = {{GPT}-3},
	url = {http://link.springer.com/10.1007/s11023-020-09548-1},
	doi = {10.1007/s11023-020-09548-1},
	language = {en},
	urldate = {2020-11-05},
	journal = {Minds and Machines},
	author = {Floridi, Luciano and Chiriatti, Massimo},
	month = nov,
	year = {2020},
	keywords = {deep learning, ethics, GPT-3, LM, NLP, text generation},
}

@article{desposito_cognitive_2015,
	title = {The cognitive neuroscience of working memory},
	volume = {66},
	url = {https://doi.org/10.1146/annurev-psych-010814-015031},
	doi = {10.1146/annurev-psych-010814-015031},
	abstract = {For more than 50 years, psychologists and neuroscientists have recognized the importance of a working memory to coordinate processing when multiple goals are active and to guide behavior with information that is not present in the immediate environment. In recent years, psychological theory and cognitive neuroscience data have converged on the idea that information is encoded into working memory by allocating attention to internal representations, whether semantic long-term memory (e.g., letters, digits, words), sensory, or motoric. Thus, information-based multivariate analyses of human functional MRI data typically find evidence for the temporary representation of stimuli in regions that also process this information in nonworking memory contexts. The prefrontal cortex (PFC), on the other hand, exerts control over behavior by biasing the salience of mnemonic representations and adjudicating among competing, context-dependent rules. The “control of the controller” emerges from a complex interplay between PFC and striatal circuits and ascending dopaminergic neuromodulatory signals.},
	number = {1},
	urldate = {2021-02-24},
	journal = {Annual Review of Psychology},
	author = {D'Esposito, Mark and Postle, Bradley R.},
	year = {2015},
	pmid = {25251486},
	note = {\_eprint: https://doi.org/10.1146/annurev-psych-010814-015031},
	pages = {115--142},
}

@article{eriksson_neurocognitive_2015,
	title = {Neurocognitive architecture of working memory},
	volume = {88},
	issn = {08966273},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627315007771},
	doi = {10.1016/j.neuron.2015.09.020},
	language = {en},
	number = {1},
	urldate = {2021-02-24},
	journal = {Neuron},
	author = {Eriksson, Johan and Vogel, Edward K. and Lansner, Anders and Bergström, Fredrik and Nyberg, Lars},
	month = oct,
	year = {2015},
	pages = {33--46},
}

@article{ma_changing_2014,
	title = {Changing concepts of working memory},
	volume = {17},
	issn = {1546-1726},
	url = {http://www.nature.com/articles/nn.3655},
	doi = {10.1038/nn.3655},
	abstract = {Working memory is thought to be limited in capacity, holding a fixed, small number of items, but it has recently been proposed that working memory might be conceptualized as a limited resource that is distributed flexibly between all items to be maintained in memory. In this review, the authors consider emerging evidence for this proposal.},
	language = {en},
	number = {3},
	urldate = {2021-02-24},
	journal = {Nature Neuroscience},
	author = {Ma, Wei Ji and Husain, Masud and Bays, Paul M.},
	month = mar,
	year = {2014},
	note = {Number: 3
Publisher: Nature Publishing Group},
	pages = {347--356},
	file = {Full Text PDF:C\:\\Users\\karmeni1\\Zotero\\storage\\YLHADXJA\\Ma et al. - 2014 - Changing concepts of working memory.pdf:application/pdf;Snapshot:C\:\\Users\\karmeni1\\Zotero\\storage\\GDZ93ZAD\\nn.html:text/html},
}

@article{beukers_is_2021,
	title = {Is activity silent working memory simply episodic memory?},
	issn = {13646613},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S136466132100005X},
	doi = {10.1016/j.tics.2021.01.003},
	language = {en},
	urldate = {2021-02-16},
	journal = {Trends in Cognitive Sciences},
	author = {Beukers, Andre O. and Buschman, Timothy J. and Cohen, Jonathan D. and Norman, Kenneth A.},
	month = feb,
	year = {2021},
	pages = {S136466132100005X},
}

@article{baddeley_working_2003,
	title = {Working memory: looking back and looking forward},
	volume = {4},
	issn = {1471-003X, 1471-0048},
	shorttitle = {Working memory},
	url = {http://www.nature.com/articles/nrn1201},
	doi = {10.1038/nrn1201},
	language = {en},
	number = {10},
	urldate = {2021-02-05},
	journal = {Nature Reviews Neuroscience},
	author = {Baddeley, Alan},
	month = oct,
	year = {2003},
	keywords = {working memory},
	pages = {829--839},
}

@article{miller_magical_1956,
	title = {The magical number seven plus or minus two:  some limits on our capacity for processing information},
	volume = {63},
	issn = {0033-295X},
	shorttitle = {The magical number seven plus or minus two},
	language = {eng},
	number = {2},
	journal = {Psychological Review},
	author = {Miller, G. A.},
	month = mar,
	year = {1956},
	pmid = {13310704},
	keywords = {working memory, serial recall},
	pages = {81--97},
}

@article{cowan_magical_2001,
	title = {The magical number 4 in short-term memory: {A} reconsideration of mental storage capacity},
	volume = {24},
	issn = {0140-525X, 1469-1825},
	shorttitle = {The magical number 4 in short-term memory},
	url = {https://www.cambridge.org/core/product/identifier/S0140525X01003922/type/journal_article},
	doi = {10.1017/S0140525X01003922},
	language = {en},
	number = {1},
	urldate = {2021-02-05},
	journal = {Behavioral and Brain Sciences},
	author = {Cowan, Nelson},
	month = feb,
	year = {2001},
	keywords = {working memory, cognitive science},
	pages = {87--114},
}

@article{lewis_computational_2006,
	title = {Computational principles of working memory in sentence comprehension},
	volume = {10},
	issn = {13646613},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661306002142},
	doi = {10.1016/j.tics.2006.08.007},
	language = {en},
	number = {10},
	urldate = {2021-02-04},
	journal = {Trends in Cognitive Sciences},
	author = {Lewis, Richard L. and Vasishth, Shravan and Van Dyke, Julie A.},
	month = oct,
	year = {2006},
	pages = {447--454},
}

@article{just_capacity_1992,
	title = {A capacity theory of comprehension: {Individual} differences in working memory.},
	volume = {99},
	issn = {1939-1471, 0033-295X},
	shorttitle = {A capacity theory of comprehension},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-295X.99.1.122},
	doi = {10.1037/0033-295X.99.1.122},
	language = {en},
	number = {1},
	urldate = {2021-02-04},
	journal = {Psychological Review},
	author = {Just, Marcel A. and Carpenter, Patricia A.},
	year = {1992},
	pages = {122--149},
}

@article{adams_theories_2018,
	title = {Theories of working memory: {Differences} in definition, degree of modularity, role of attention, and purpose},
	volume = {49},
	issn = {1558-9129},
	shorttitle = {Theories of {Working} {Memory}},
	doi = {10.1044/2018_LSHSS-17-0114},
	abstract = {Purpose: The purpose of this article is to review and discuss theories of working memory with special attention to their relevance to language processing.
Method: We begin with an overview of the concept of working memory itself and review some of the major theories. Then, we show how theories of working memory can be organized according to their stances on 3 major issues that distinguish them: modularity (on a continuum from domain-general to very modular), attention (on a continuum from automatic to completely attention demanding), and purpose (on a continuum from idiographic, or concerned with individual differences, to nomothetic, or concerned with group norms). We examine recent research that has a bearing on these distinctions.
Results: Our review shows important differences between working memory theories that can be described according to positions on the 3 continua just noted.
Conclusion: Once properly understood, working memory theories, methods, and data can serve as quite useful tools for language research.},
	language = {eng},
	number = {3},
	journal = {Language, Speech, and Hearing Services in Schools},
	author = {Adams, Eryn J. and Nguyen, Anh T. and Cowan, Nelson},
	month = jul,
	year = {2018},
	pmid = {29978205},
	pmcid = {PMC6105130},
	pages = {340--355},
}

@article{macdonald_reassessing_2002,
	title = {Reassessing working memory: {Comment} on {Just} and {Carpenter} (1992) and {Waters} and {Caplan} (1996).},
	volume = {109},
	issn = {0033-295X},
	shorttitle = {Reassessing working memory},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-295X.109.1.35},
	doi = {10.1037//0033-295X.109.1.35},
	language = {en},
	number = {1},
	urldate = {2021-01-26},
	journal = {Psychological Review},
	author = {MacDonald, Maryellen C. and Christiansen, Morten H.},
	year = {2002},
	pages = {35--54},
}

@inproceedings{wolf_transformers_2020,
	address = {Online},
	title = {Transformers: {State}-of-the-{Art} natural language processing},
	shorttitle = {Transformers},
	url = {https://www.aclweb.org/anthology/2020.emnlp-demos.6},
	doi = {10.18653/v1/2020.emnlp-demos.6},
	abstract = {Recent progress in natural language processing has been driven by advances in both model architecture and model pretraining. Transformer architectures have facilitated building higher-capacity models and pretraining has made it possible to effectively utilize this capacity for a wide variety of tasks. Transformers is an open-source library with the goal of opening up these advances to the wider machine learning community. The library consists of carefully engineered state-of-the art Transformer architectures under a unified API. Backing this library is a curated collection of pretrained models made by and available for the community. Transformers is designed to be extensible by researchers, simple for practitioners, and fast and robust in industrial deployments. The library is available at https://github.com/huggingface/transformers.},
	urldate = {2021-01-26},
	booktitle = {Proceedings of the 2020 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}: {System} {Demonstrations}},
	publisher = {Association for Computational Linguistics},
	author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, Remi and Funtowicz, Morgan and Davison, Joe and Shleifer, Sam and von Platen, Patrick and Ma, Clara and Jernite, Yacine and Plu, Julien and Xu, Canwen and Le Scao, Teven and Gugger, Sylvain and Drame, Mariama and Lhoest, Quentin and Rush, Alexander},
	month = oct,
	year = {2020},
	keywords = {NLP, BERT, GPT-2, transformer},
	pages = {38--45},
}

@article{altmann_incrementality_2009,
	title = {Incrementality and prediction in human sentence processing},
	volume = {33},
	issn = {03640213, 15516709},
	url = {http://doi.wiley.com/10.1111/j.1551-6709.2009.01022.x},
	doi = {10.1111/j.1551-6709.2009.01022.x},
	language = {en},
	number = {4},
	urldate = {2020-05-24},
	journal = {Cognitive Science},
	author = {Altmann, Gerry T. M. and Mirković, Jelena},
	month = jun,
	year = {2009},
	pages = {583--609},
}

@article{efrat_turking_2020,
	title = {The turking test: can language models understand instructions?},
	shorttitle = {The {Turking} {Test}},
	url = {http://arxiv.org/abs/2010.11982},
	abstract = {Supervised machine learning provides the learner with a set of input-output examples of the target task. Humans, however, can also learn to perform new tasks from instructions in natural language. Can machines learn to understand instructions as well? We present the Turking Test, which examines a model's ability to follow natural language instructions of varying complexity. These range from simple tasks, like retrieving the nth word of a sentence, to ones that require creativity, such as generating examples for SNLI and SQuAD in place of human intelligence workers ("turkers"). Despite our lenient evaluation methodology, we observe that a large pretrained language model performs poorly across all tasks. Analyzing the model's error patterns reveals that the model tends to ignore explicit instructions and often generates outputs that cannot be construed as an attempt to solve the task. While it is not yet clear whether instruction understanding can be captured by traditional language models, the sheer expressivity of instruction understanding makes it an appealing alternative to the rising few-shot inference paradigm.},
	urldate = {2020-10-27},
	journal = {arXiv:2010.11982 [cs]},
	author = {Efrat, Avia and Levy, Omer},
	month = oct,
	year = {2020},
	note = {arXiv: 2010.11982},
}

@article{ma_neural_2020,
	title = {A neural network walks into a lab: towards using deep nets as models for human behavior},
	shorttitle = {A neural network walks into a lab},
	url = {http://arxiv.org/abs/2005.02181},
	abstract = {What might sound like the beginning of a joke has become an attractive prospect for many cognitive scientists: the use of deep neural network models (DNNs) as models of human behavior in perceptual and cognitive tasks. Although DNNs have taken over machine learning, attempts to use them as models of human behavior are still in the early stages. Can they become a versatile model class in the cognitive scientist's toolbox? We first argue why DNNs have the potential to be interesting models of human behavior. We then discuss how that potential can be more fully realized. On the one hand, we argue that the cycle of training, testing, and revising DNNs needs to be revisited through the lens of the cognitive scientist's goals. Specifically, we argue that methods for assessing the goodness of fit between DNN models and human behavior have to date been impoverished. On the other hand, cognitive science might have to start using more complex tasks (including richer stimulus spaces), but doing so might be beneficial for DNN-independent reasons as well. Finally, we highlight avenues where traditional cognitive process models and DNNs may show productive synergy.},
	urldate = {2020-10-07},
	journal = {arXiv:2005.02181 [cs, q-bio]},
	author = {Ma, Wei Ji and Peters, Benjamin},
	month = may,
	year = {2020},
	note = {arXiv: 2005.02181},
}

@article{rae_transformers_2020,
	title = {Do transformers need deep long-range memory},
	url = {http://arxiv.org/abs/2007.03356},
	abstract = {Deep attention models have advanced the modelling of sequential data across many domains. For language modelling in particular, the Transformer-XL -- a Transformer augmented with a long-range memory of past activations -- has been shown to be state-of-the-art across a variety of well-studied benchmarks. The Transformer-XL incorporates a long-range memory at every layer of the network, which renders its state to be thousands of times larger than RNN predecessors. However it is unclear whether this is necessary. We perform a set of interventions to show that comparable performance can be obtained with 6X fewer long range memories and better performance can be obtained by limiting the range of attention in lower layers of the network.},
	urldate = {2020-09-21},
	journal = {arXiv:2007.03356 [cs, stat]},
	author = {Rae, Jack W. and Razavi, Ali},
	month = jul,
	year = {2020},
	note = {arXiv: 2007.03356},
}

@article{norris_short-term_2017,
	title = {Short-term memory and long-term memory are still different.},
	volume = {143},
	issn = {1939-1455, 0033-2909},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/bul0000108},
	doi = {10.1037/bul0000108},
	language = {en},
	number = {9},
	urldate = {2020-12-02},
	journal = {Psychological Bulletin},
	author = {Norris, Dennis},
	month = sep,
	year = {2017},
	keywords = {working memory, long-term memory, memory, short-term memory},
	pages = {992--1009},
}

@incollection{cowan_what_2008,
	title = {What are the differences between long-term, short-term, and working memory?},
	volume = {169},
	isbn = {978-0-444-53164-3},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0079612307000209},
	language = {en},
	urldate = {2020-12-02},
	booktitle = {Progress in {Brain} {Research}},
	publisher = {Elsevier},
	author = {Cowan, Nelson},
	year = {2008},
	doi = {10.1016/S0079-6123(07)00020-9},
	keywords = {working memory, short-term memory, long term memory},
	pages = {323--338},
	file = {Accepted Version:C\:\\Users\\karmeni1\\Zotero\\storage\\H8TINKQW\\Cowan - 2008 - Chapter 20 What are the differences between long-t.pdf:application/pdf},
}

@book{miyake_models_1999,
	address = {Cambridge ; New York},
	title = {Models of working memory: mechanisms of active maintenance and executive control},
	isbn = {978-0-521-58325-1 978-0-521-58721-1},
	shorttitle = {Models of working memory},
	publisher = {Cambridge University Press},
	editor = {Miyake, Akira and Shah, Priti},
	year = {1999},
	keywords = {working memory, short-term memory, long term memory, cognitive modeling, review},
}

@article{ettinger_what_2020,
	title = {What {BERT} {Is} {Not}: {Lessons} from a new suite of psycholinguistic diagnostics for language models},
	volume = {8},
	shorttitle = {What {BERT} {Is} {Not}},
	url = {https://doi.org/10.1162/tacl_a_00298},
	doi = {10.1162/tacl_a_00298},
	abstract = {Pre-training by language modeling has become a popular and successful approach to NLP tasks, but we have yet to understand exactly what linguistic capacities these pre-training processes confer upon models. In this paper we introduce a suite of diagnostics drawn from human language experiments, which allow us to ask targeted questions about information used by language models for generating predictions in context. As a case study, we apply these diagnostics to the popular BERT model, finding that it can generally distinguish good from bad completions involving shared category or role reversal, albeit with less sensitivity than humans, and it robustly retrieves noun hypernyms, but it struggles with challenging inference and role-based event prediction— and, in particular, it shows clear insensitivity to the contextual impacts of negation.},
	urldate = {2020-11-13},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Ettinger, Allyson},
	month = jan,
	year = {2020},
	note = {Publisher: MIT Press},
	keywords = {NLP, BERT, MLM, N400},
	pages = {34--48},
}

@article{schrimpf_integrative_2020,
	title = {Integrative benchmarking to advance neurally mechanistic models of human intelligence},
	volume = {108},
	issn = {0896-6273},
	url = {https://www.cell.com/neuron/abstract/S0896-6273(20)30605-X},
	doi = {10.1016/j.neuron.2020.07.040},
	language = {English},
	number = {3},
	urldate = {2020-11-13},
	journal = {Neuron},
	author = {Schrimpf, Martin and Kubilius, Jonas and Lee, Michael J. and Murty, N. Apurva Ratan and Ajemian, Robert and DiCarlo, James J.},
	month = nov,
	year = {2020},
	pmid = {32918861},
	note = {Publisher: Elsevier},
	keywords = {benchmark},
	pages = {413--423},
}

@article{oberauer_benchmarks_2018,
	title = {Benchmarks for models of short-term and working memory.},
	volume = {144},
	issn = {1939-1455, 0033-2909},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/bul0000153},
	doi = {10.1037/bul0000153},
	language = {en},
	number = {9},
	urldate = {2020-11-10},
	journal = {Psychological Bulletin},
	author = {Oberauer, Klaus and Lewandowsky, Stephan and Awh, Edward and Brown, Gordon D. A. and Conway, Andrew and Cowan, Nelson and Donkin, Christopher and Farrell, Simon and Hitch, Graham J. and Hurlstone, Mark J. and Ma, Wei Ji and Morey, Candice C. and Nee, Derek Evan and Schweppe, Judith and Vergauwe, Evie and Ward, Geoff},
	month = sep,
	year = {2018},
	keywords = {working memory, short-term memory, benchmark},
	pages = {885--958},
}

@article{petroni_language_2019,
	title = {Language models as knowledge bases?},
	url = {http://arxiv.org/abs/1909.01066},
	abstract = {Recent progress in pretraining language models on large textual corpora led to a surge of improvements for downstream NLP tasks. Whilst learning linguistic knowledge, these models may also be storing relational knowledge present in the training data, and may be able to answer queries structured as "fill-in-the-blank" cloze statements. Language models have many advantages over structured knowledge bases: they require no schema engineering, allow practitioners to query about an open class of relations, are easy to extend to more data, and require no human supervision to train. We present an in-depth analysis of the relational knowledge already present (without fine-tuning) in a wide range of state-of-the-art pretrained language models. We find that (i) without fine-tuning, BERT contains relational knowledge competitive with traditional NLP methods that have some access to oracle knowledge, (ii) BERT also does remarkably well on open-domain question answering against a supervised baseline, and (iii) certain types of factual knowledge are learned much more readily than others by standard language model pretraining approaches. The surprisingly strong ability of these models to recall factual knowledge without any fine-tuning demonstrates their potential as unsupervised open-domain QA systems. The code to reproduce our analysis is available at https://github.com/facebookresearch/LAMA.},
	urldate = {2020-11-03},
	journal = {arXiv:1909.01066 [cs]},
	author = {Petroni, Fabio and Rocktäschel, Tim and Lewis, Patrick and Bakhtin, Anton and Wu, Yuxiang and Miller, Alexander H. and Riedel, Sebastian},
	month = sep,
	year = {2019},
	note = {arXiv: 1909.01066},
	keywords = {LM, NLP, BERT, relation extraction, T-REX},
}

@article{wang_superglue_2020,
	title = {{SuperGLUE}: {A} stickier benchmark for general-purpose language understanding systems},
	shorttitle = {{SuperGLUE}},
	url = {http://arxiv.org/abs/1905.00537},
	abstract = {In the last year, new models and methods for pretraining and transfer learning have driven striking performance improvements across a range of language understanding tasks. The GLUE benchmark, introduced a little over one year ago, offers a single-number metric that summarizes progress on a diverse set of such tasks, but performance on the benchmark has recently surpassed the level of non-expert humans, suggesting limited headroom for further research. In this paper we present SuperGLUE, a new benchmark styled after GLUE with a new set of more difficult language understanding tasks, a software toolkit, and a public leaderboard. SuperGLUE is available at super.gluebenchmark.com.},
	urldate = {2020-10-29},
	journal = {arXiv:1905.00537 [cs]},
	author = {Wang, Alex and Pruksachatkun, Yada and Nangia, Nikita and Singh, Amanpreet and Michael, Julian and Hill, Felix and Levy, Omer and Bowman, Samuel R.},
	month = feb,
	year = {2020},
	note = {arXiv: 1905.00537},
	keywords = {NLP, NLU, benchmark, GLUE},
}

@article{marcus_next_2020,
	title = {The next decade in {AI}: {Four} steps towards robust artificial intelligence},
	shorttitle = {The {Next} {Decade} in {AI}},
	url = {http://arxiv.org/abs/2002.06177},
	abstract = {Recent research in artificial intelligence and machine learning has largely emphasized general-purpose learning and ever-larger training sets and more and more compute. In contrast, I propose a hybrid, knowledge-driven, reasoning-based approach, centered around cognitive models, that could provide the substrate for a richer, more robust AI than is currently possible.},
	urldate = {2020-10-29},
	journal = {arXiv:2002.06177 [cs]},
	author = {Marcus, Gary},
	month = feb,
	year = {2020},
	note = {arXiv: 2002.06177},
}

@inproceedings{bender_climbing_2020,
	address = {Online},
	title = {Climbing towards {NLU}: {On} meaning, form, and understanding in the age of data},
	shorttitle = {Climbing towards {NLU}},
	url = {https://www.aclweb.org/anthology/2020.acl-main.463},
	doi = {10.18653/v1/2020.acl-main.463},
	language = {en},
	urldate = {2020-08-09},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Bender, Emily M. and Koller, Alexander},
	year = {2020},
	keywords = {NLP, meaning, NLU, BERT, GPT-2, Chinese room, semantics, Turing, Turing test},
	pages = {5185--5198},
}

@article{botvinick_short-term_2006,
	title = {Short-term memory for serial order: {A} recurrent neural network model},
	volume = {113},
	issn = {1939-1471, 0033-295X},
	shorttitle = {Short-term memory for serial order},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-295X.113.2.201},
	doi = {10.1037/0033-295X.113.2.201},
	language = {en},
	number = {2},
	urldate = {2020-10-21},
	journal = {Psychological Review},
	author = {Botvinick, Matthew M. and Plaut, David C.},
	year = {2006},
	pages = {201--233},
}

@article{firestone_performance_2020,
	title = {Performance vs. competence in human–machine comparisons},
	copyright = {© 2020 . https://www-pnas-org.ru.idm.oclc.org/site/aboutpnas/licenses.xhtmlPublished under the PNAS license.},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/content/early/2020/10/13/1905334117},
	doi = {10.1073/pnas.1905334117},
	abstract = {Does the human mind resemble the machines that can behave like it? Biologically inspired machine-learning systems approach “human-level” accuracy in an astounding variety of domains, and even predict human brain activity—raising the exciting possibility that such systems represent the world like we do. However, even seemingly intelligent machines fail in strange and “unhumanlike” ways, threatening their status as models of our minds. How can we know when human–machine behavioral differences reflect deep disparities in their underlying capacities, vs. when such failures are only superficial or peripheral? This article draws on a foundational insight from cognitive science—the distinction between performance and competence—to encourage “species-fair” comparisons between humans and machines. The performance/competence distinction urges us to consider whether the failure of a system to behave as ideally hypothesized, or the failure of one creature to behave like another, arises not because the system lacks the relevant knowledge or internal capacities (“competence”), but instead because of superficial constraints on demonstrating that knowledge (“performance”). I argue that this distinction has been neglected by research comparing human and machine behavior, and that it should be essential to any such comparison. Focusing on the domain of image classification, I identify three factors contributing to the species-fairness of human–machine comparisons, extracted from recent work that equates such constraints. Species-fair comparisons level the playing field between natural and artificial intelligence, so that we can separate more superficial differences from those that may be deep and enduring.},
	language = {en},
	urldate = {2020-10-19},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Firestone, Chaz},
	month = oct,
	year = {2020},
	pmid = {33051296},
	note = {ISBN: 9781905334117
Publisher: National Academy of Sciences
Section: Perspective},
	keywords = {artificial intelligence, AI, ASL, competence, performance},
	file = {Firestone - 2020 - Performance vs. competence in human–machine compar.pdf:C\:\\Users\\karmeni1\\Google Drive\\zotero\\Firestone - 2020 - Performance vs. competence in human–machine compar.pdf:application/pdf},
}

@article{ericsson_long-term_1995,
	title = {Long-term working memory},
	volume = {102},
	issn = {1939-1471, 0033-295X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-295X.102.2.211},
	doi = {10.1037/0033-295X.102.2.211},
	language = {en},
	number = {2},
	urldate = {2020-09-23},
	journal = {Psychological Review},
	author = {Ericsson, K. Anders and Kintsch, Walter},
	year = {1995},
	keywords = {working memory, long term memory, long term working memory},
	pages = {211--245},
}

@article{baddeley_working_2003-1,
	series = {{ASHA} 2002},
	title = {Working memory and language: an overview},
	volume = {36},
	issn = {0021-9924},
	shorttitle = {Working memory and language},
	url = {http://www.sciencedirect.com/science/article/pii/S0021992403000194},
	doi = {10.1016/S0021-9924(03)00019-4},
	abstract = {Working memory involves the temporary storage and manipulation of information that is assumed to be necessary for a wide range of complex cognitive activities. In 1974, Baddeley and Hitch proposed that it could be divided into three subsystems, one concerned with verbal and acoustic information, the phonological loop, a second, the visuospatial sketchpad providing its visual equivalent, while both are dependent upon a third attentionally-limited control system, the central executive. A fourth subsystem, the episodic buffer, has recently been proposed. These are described in turn, with particular reference to implications for both the normal processing of language, and its potential disorders.
Learning outcomes
The reader will be introduced to the concept of a multi-component working memory. Particular emphasis will be placed on the phonological loop component, and (a) its fractionation into a storage and processing component, (b) the neuropsychological evidence for this distinction, and (c) its implication for both native and second language learning. This will be followed by (d) a brief overview of the visuospatial sketchpad and its possible role in language, culminating in (e) discussion of the higher-level control functions of working memory which include (f) the central executive and its multi-dimensional storage system, the episodic buffer. An attempt throughout is made to link the model to its role in both normal and disordered language functions.},
	language = {en},
	number = {3},
	urldate = {2020-09-16},
	journal = {Journal of Communication Disorders},
	author = {Baddeley, Alan},
	month = may,
	year = {2003},
	keywords = {working memory, central executive, phonological loop},
	pages = {189--208},
}

@article{galassi_attention_2020,
	title = {Attention in natural language processing},
	issn = {2162-237X, 2162-2388},
	url = {http://arxiv.org/abs/1902.02181},
	doi = {10.1109/TNNLS.2020.3019893},
	abstract = {Attention is an increasingly popular mechanism used in a wide range of neural architectures. The mechanism itself has been realized in a variety of formats. However, because of the fast-paced advances in this domain, a systematic overview of attention is still missing. In this article, we define a unified model for attention architectures in natural language processing, with a focus on those designed to work with vector representations of the textual data. We propose a taxonomy of attention models according to four dimensions: the representation of the input, the compatibility function, the distribution function, and the multiplicity of the input and/or output. We present the examples of how prior information can be exploited in attention models and discuss ongoing research efforts and open challenges in the area, providing the first extensive categorization of the vast body of literature in this exciting domain.},
	urldate = {2020-09-16},
	journal = {IEEE Transactions on Neural Networks and Learning Systems},
	author = {Galassi, Andrea and Lippi, Marco and Torroni, Paolo},
	year = {2020},
	note = {arXiv: 1902.02181},
	keywords = {attention, nlp},
	pages = {1--18},
}

@inproceedings{henderson_unstoppable_2020,
	address = {Online},
	title = {The unstoppable rise of computational linguistics in deep learning},
	url = {https://www.aclweb.org/anthology/2020.acl-main.561},
	doi = {10.18653/v1/2020.acl-main.561},
	abstract = {In this paper, we trace the history of neural networks applied to natural language understanding tasks, and identify key contributions which the nature of language has made to the development of neural network architectures. We focus on the importance of variable binding and its instantiation in attention-based models, and argue that Transformer is not a sequence model but an induced-structure model. This perspective leads to predictions of the challenges facing research in deep learning architectures for natural language understanding.},
	urldate = {2020-09-16},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Henderson, James},
	month = jul,
	year = {2020},
	keywords = {deep learning, hierarchy, linguistics},
	pages = {6294--6306},
}

@article{kumaran_what_2016,
	title = {What learning systems do intelligent agents need? complementary learning systems theory updated},
	volume = {20},
	issn = {13646613},
	shorttitle = {What {Learning} {Systems} do {Intelligent} {Agents} {Need}?},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S1364661316300432},
	doi = {10.1016/j.tics.2016.05.004},
	language = {en},
	number = {7},
	urldate = {2020-09-08},
	journal = {Trends in Cognitive Sciences},
	author = {Kumaran, Dharshan and Hassabis, Demis and McClelland, James L.},
	month = jul,
	year = {2016},
	pages = {512--534},
}

@article{toneva_interpreting_2019,
	title = {Interpreting and improving natural-language processing (in machines) with natural language-processing (in the brain)},
	url = {http://arxiv.org/abs/1905.11833},
	abstract = {Neural networks models for NLP are typically implemented without the explicit encoding of language rules and yet they are able to break one performance record after another. This has generated a lot of research interest in interpreting the representations learned by these networks. We propose here a novel interpretation approach that relies on the only processing system we have that does understand language: the human brain. We use brain imaging recordings of subjects reading complex natural text to interpret word and sequence embeddings from 4 recent NLP models - ELMo, USE, BERT and Transformer-XL. We study how their representations differ across layer depth, context length, and attention type. Our results reveal differences in the context-related representations across these models. Further, in the transformer models, we find an interaction between layer depth and context length, and between layer depth and attention type. We finally hypothesize that altering BERT to better align with brain recordings would enable it to also better understand language. Probing the altered BERT using syntactic NLP tasks reveals that the model with increased brain-alignment outperforms the original model. Cognitive neuroscientists have already begun using NLP networks to study the brain, and this work closes the loop to allow the interaction between NLP and cognitive neuroscience to be a true cross-pollination.},
	urldate = {2020-08-19},
	journal = {arXiv:1905.11833 [cs, q-bio]},
	author = {Toneva, Mariya and Wehbe, Leila},
	month = nov,
	year = {2019},
	note = {arXiv: 1905.11833},
}

@article{lake_building_2017,
	title = {Building machines that learn and think like people},
	volume = {40},
	issn = {0140-525X, 1469-1825},
	url = {https://www.cambridge.org/core/journals/behavioral-and-brain-sciences/article/building-machines-that-learn-and-think-like-people/A9535B1D745A0377E16C590E14B94993},
	doi = {10.1017/S0140525X16001837},
	abstract = {Abstract
Recent progress in artificial intelligence has renewed interest in building systems that learn and think like people. Many advances have come from using deep neural networks trained end-to-end in tasks such as object recognition, video games, and board games, achieving performance that equals or even beats that of humans in some respects. Despite their biological inspiration and performance achievements, these systems differ from human intelligence in crucial ways. We review progress in cognitive science suggesting that truly human-like learning and thinking machines will have to reach beyond current engineering trends in both what they learn and how they learn it. Specifically, we argue that these machines should (1) build causal models of the world that support explanation and understanding, rather than merely solving pattern recognition problems; (2) ground learning in intuitive theories of physics and psychology to support and enrich the knowledge that is learned; and (3) harness compositionality and learning-to-learn to rapidly acquire and generalize knowledge to new tasks and situations. We suggest concrete challenges and promising routes toward these goals that can combine the strengths of recent neural network advances with more structured cognitive models.},
	urldate = {2017-12-14},
	journal = {Behavioral and Brain Sciences},
	author = {Lake, Brenden M. and Ullman, Tomer D. and Tenenbaum, Joshua B. and Gershman, Samuel J.},
	year = {2017},
	keywords = {cognitive science, Turing, AI, ANN, pattern recognition, project.lsnn, project.lstmMEG, read, statistical learning},
	file = {Lake idr. - 2017 - Building machines that learn and think like people.pdf:C\:\\Users\\karmeni1\\Google Drive\\zotero\\Lake idr. - 2017 - Building machines that learn and think like people.pdf:application/pdf},
}

@inproceedings{athanasiou_neural_2018,
	address = {Santa Fe, New Mexico, USA},
	title = {Neural activation semantic models: {Computational} lexical semantic models of localized neural activations},
	shorttitle = {Neural {Activation} {Semantic} {Models}},
	url = {https://www.aclweb.org/anthology/C18-1243},
	abstract = {Neural activation models have been proposed in the literature that use a set of example words for which fMRI measurements are available in order to find a mapping between word semantics and localized neural activations. Successful mappings let us expand to the full lexicon of concrete nouns using the assumption that similarity of meaning implies similar neural activation patterns. In this paper, we propose a computational model that estimates semantic similarity in the neural activation space and investigates the relative performance of this model for various natural language processing tasks. Despite the simplicity of the proposed model and the very small number of example words used to bootstrap it, the neural activation semantic model performs surprisingly well compared to state-of-the-art word embeddings. Specifically, the neural activation semantic model performs better than the state-of-the-art for the task of semantic similarity estimation between very similar or very dissimilar words, while performing well on other tasks such as entailment and word categorization. These are strong indications that neural activation semantic models can not only shed some light into human cognition but also contribute to computation models for certain tasks.},
	urldate = {2020-08-25},
	booktitle = {Proceedings of the 27th {International} {Conference} on {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Athanasiou, Nikos and Iosif, Elias and Potamianos, Alexandros},
	month = aug,
	year = {2018},
	keywords = {fMRI, semantic similarity},
	pages = {2867--2878},
}

@book{gallistel_memory_2009,
	address = {Chichester, West Sussex, UK ; Malden, MA},
	title = {Memory and the computational brain: why cognitive science will transform neuroscience},
	isbn = {978-1-4051-2287-0 978-1-4051-2288-7},
	shorttitle = {Memory and the computational brain},
	publisher = {Wiley-Blackwell},
	author = {Gallistel, C. R. and King, Adam Philip},
	year = {2009},
	keywords = {cognitive science, hierarchy, thesis, computation, explanation, information theory, probability theory, entropy, neural coding, reading, thesis.introduction, cognitive neuroscience, mutual information, information, efficient coding},
	file = {Gallistel and King - 2009 - Memory and the computational brain.pdf:C\:\\Users\\karmeni1\\Google Drive\\zotero\\Gallistel and King - 2009 - Memory and the computational brain.pdf:application/pdf},
}

@article{hasson_hierarchical_2015,
	title = {Hierarchical process memory: memory as an integral component of information processing},
	volume = {19},
	issn = {1364-6613},
	shorttitle = {Hierarchical process memory},
	url = {http://www.sciencedirect.com/science/article/pii/S1364661315000923},
	doi = {10.1016/j.tics.2015.04.006},
	abstract = {Models of working memory (WM) commonly focus on how information is encoded into and retrieved from storage at specific moments. However, in the majority of real-life processes, past information is used continuously to process incoming information across multiple timescales. Considering single-unit, electrocorticography, and functional imaging data, we argue that (i) virtually all cortical circuits can accumulate information over time, and (ii) the timescales of accumulation vary hierarchically, from early sensory areas with short processing timescales (10s to 100s of milliseconds) to higher-order areas with long processing timescales (many seconds to minutes). In this hierarchical systems perspective, memory is not restricted to a few localized stores, but is intrinsic to information processing that unfolds throughout the brain on multiple timescales.},
	number = {6},
	urldate = {2017-11-22},
	journal = {Trends in Cognitive Sciences},
	author = {Hasson, Uri and Chen, Janice and Honey, Christopher J.},
	month = jun,
	year = {2015},
	keywords = {project.lstmMEG, read},
	pages = {304--313},
	file = {Hasson et al. - 2015 - Hierarchical process memory memory as an integral.pdf:C\:\\Users\\karmeni1\\Google Drive\\zotero\\Hasson et al. - 2015 - Hierarchical process memory memory as an integral.pdf:application/pdf},
}

@article{nelson_neurophysiological_2017,
	title = {Neurophysiological dynamics of phrase-structure building during sentence processing},
	volume = {114},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/content/114/18/E3669},
	doi = {10.1073/pnas.1701590114},
	abstract = {Although sentences unfold sequentially, one word at a time, most linguistic theories propose that their underlying syntactic structure involves a tree of nested phrases rather than a linear sequence of words. Whether and how the brain builds such structures, however, remains largely unknown. Here, we used human intracranial recordings and visual word-by-word presentation of sentences and word lists to investigate how left-hemispheric brain activity varies during the formation of phrase structures. In a broad set of language-related areas, comprising multiple superior temporal and inferior frontal sites, high-gamma power increased with each successive word in a sentence but decreased suddenly whenever words could be merged into a phrase. Regression analyses showed that each additional word or multiword phrase contributed a similar amount of additional brain activity, providing evidence for a merge operation that applies equally to linguistic objects of arbitrary complexity. More superficial models of language, based solely on sequential transition probability over lexical and syntactic categories, only captured activity in the posterior middle temporal gyrus. Formal model comparison indicated that the model of multiword phrase construction provided a better fit than probability-based models at most sites in superior temporal and inferior frontal cortices. Activity in those regions was consistent with a neural implementation of a bottom-up or left-corner parser of the incoming language stream. Our results provide initial intracranial evidence for the neurophysiological reality of the merge operation postulated by linguists and suggest that the brain compresses syntactically well-formed sequences of words into a hierarchy of nested phrases.},
	language = {en},
	number = {18},
	urldate = {2017-11-22},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Nelson, Matthew J. and Karoui, Imen El and Giber, Kristof and Yang, Xiaofang and Cohen, Laurent and Koopman, Hilda and Cash, Sydney S. and Naccache, Lionel and Hale, John T. and Pallier, Christophe and Dehaene, Stanislas},
	month = may,
	year = {2017},
	pmid = {28416691},
	keywords = {syntax, project.lstmMEG, read, fixme, project.streams, entropy, ECoG, surprisal, n-gram, model-based},
	pages = {E3669--E3678},
	file = {Nelson et al. - 2017 - Neurophysiological dynamics of phrase-structure bu.pdf:C\:\\Users\\karmeni1\\Google Drive\\zotero\\Nelson et al. - 2017 - Neurophysiological dynamics of phrase-structure bu.pdf:application/pdf},
}

@article{elman_finding_1990,
	title = {Finding structure in time},
	volume = {14},
	issn = {03640213},
	doi = {10.1016/0364-0213(90)90002-E},
	language = {en},
	number = {2},
	journal = {Cognitive Science},
	author = {Elman, J},
	month = jun,
	year = {1990},
	keywords = {ANN, project.lsnn, project.lstmMEG, machine learning, readme, neural networks, time, RNN, core},
	pages = {179--211},
	file = {Elman - 1990 - Finding structure in time.pdf:C\:\\Users\\karmeni1\\Google Drive\\zotero\\Elman - 1990 - Finding structure in time.pdf:application/pdf},
}

@article{fyshe_interpretable_2014,
	title = {Interpretable semantic vectors from a joint model of brain- and text-based meaning},
	volume = {2014},
	issn = {0736-587X},
	url = {https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4497373/},
	abstract = {Vector space models (VSMs) represent word meanings as points in a high dimensional space. VSMs are typically created using a large text corpora, and so represent word semantics as observed in text. We present a new algorithm (JNNSE) that can incorporate a measure of semantics not previously used to create VSMs: brain activation data recorded while people read words. The resulting model takes advantage of the complementary strengths and weaknesses of corpus and brain activation data to give a more complete representation of semantics. Evaluations show that the model 1) matches a behavioral measure of semantics more closely, 2) can be used to predict corpus data for unseen words and 3) has predictive power that generalizes across brain imaging technologies and across subjects. We believe that the model is thus a more faithful representation of mental vocabularies.},
	urldate = {2018-02-20},
	journal = {Proceedings of the Conference of Association for Computational Linguistics. Meeting},
	author = {Fyshe, Alona and Talukdar, Partha P and Murphy, Brian and Mitchell, Tom M},
	month = jun,
	year = {2014},
	pmid = {26166940},
	pmcid = {PMC4497373},
	keywords = {project.lstmMEG, fMRI, readme, project.vsmMEG, VSM, distributional semantics},
	pages = {489--499},
}

@article{fong_using_2018,
	title = {Using human brain activity to guide machine learning},
	volume = {8},
	copyright = {2018 The Author(s)},
	issn = {2045-2322},
	doi = {10.1038/s41598-018-23618-6},
	abstract = {Machine learning is a field of computer science that builds algorithms that learn. In many cases, machine learning algorithms are used to recreate a human ability like adding a caption to a photo, driving a car, or playing a game. While the human brain has long served as a source of inspiration for machine learning, little effort has been made to directly use data collected from working brains as a guide for machine learning algorithms. Here we demonstrate a new paradigm of “neurally-weighted” machine learning, which takes fMRI measurements of human brain activity from subjects viewing images, and infuses these data into the training process of an object recognition learning algorithm to make it more consistent with the human brain. After training, these neurally-weighted classifiers are able to classify images without requiring any additional neural data. We show that our neural-weighting approach can lead to large performance gains when used with traditional machine vision features, as well as to significant improvements with already high-performing convolutional neural network features. The effectiveness of this approach points to a path forward for a new class of hybrid machine learning algorithms which take both inspiration and direct constraints from neuronal data.},
	language = {en},
	number = {1},
	urldate = {2018-05-17},
	journal = {Scientific Reports},
	author = {Fong, Ruth C. and Scheirer, Walter J. and Cox, David D.},
	month = mar,
	year = {2018},
	keywords = {project.lstmMEG, machine learning, fMRI, readme, neural networks, computational neuroscience},
	pages = {5397},
}

@article{bahdanau_neural_2014,
	title = {Neural machine translation by jointly learning to align and translate},
	volume = {abs/1409.0473},
	url = {http://arxiv.org/abs/1409.0473},
	journal = {CoRR},
	author = {Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
	year = {2014},
	keywords = {LSTM, readme, RNN, computational linguistics},
}

@article{hochreiter_long_1997,
	title = {Long short-term memory},
	volume = {9},
	issn = {0899-7667, 1530-888X},
	url = {http://www.mitpressjournals.org/doi/10.1162/neco.1997.9.8.1735},
	doi = {10.1162/neco.1997.9.8.1735},
	language = {en},
	number = {8},
	urldate = {2018-01-20},
	journal = {Neural Computation},
	author = {Hochreiter, Sepp and Schmidhuber, Jürgen},
	month = nov,
	year = {1997},
	keywords = {LSTM, ANN, project.lsnn, project.lstmMEG, machine learning, readme, RNN, TRW},
	pages = {1735--1780},
	file = {Hochreiter and Schmidhuber - 1997 - Long short-term memory.pdf:C\:\\Users\\karmeni1\\Google Drive\\zotero\\Hochreiter and Schmidhuber - 1997 - Long short-term memory.pdf:application/pdf},
}

@article{hagoort_muc_2013,
	title = {{MUC} ({Memory}, {Unification}, {Control}) and beyond},
	volume = {4},
	issn = {1664-1078},
	url = {http://journal.frontiersin.org/article/10.3389/fpsyg.2013.00416/abstract},
	doi = {10.3389/fpsyg.2013.00416},
	urldate = {2019-05-19},
	journal = {Frontiers in Psychology},
	author = {Hagoort, Peter},
	year = {2013},
}

@article{oreilly_making_2006,
	title = {Making working memory work: {A} computational model of learning in the prefrontal cortex and basal ganglia},
	volume = {18},
	issn = {0899-7667},
	shorttitle = {Making {Working} {Memory} {Work}},
	url = {https://doi.org/10.1162/089976606775093909},
	doi = {10.1162/089976606775093909},
	abstract = {The prefrontal cortex has long been thought to subserve both working memory (the holding of information online for processing) and executive functions (deciding how to manipulate working memory and perform processing). Although many computational models of working memory have been developed, the mechanistic basis of executive function remains elusive, often amounting to a homunculus. This article presents an attempt to deconstruct this homunculus through powerful learning mechanisms that allow a computational model of the prefrontal cortex to control both itself and other brain areas in a strategic, task-appropriate manner. These learning mechanisms are based on subcortical structures in the midbrain, basal ganglia, and amygdala, which together form an actor-critic architecture. The critic system learns which prefrontal representations are task relevant and trains the actor, which in turn provides a dynamic gating mechanism for controlling working memory updating. Computationally, the learning mechanism is designed to simultaneously solve the temporal and structural credit assignment problems. The model's performance compares favorably with standard backpropagation-based temporal learning mechanisms on the challenging 1-2-AX working memory task and other benchmark working memory tasks.},
	number = {2},
	urldate = {2018-07-27},
	journal = {Neural Computation},
	author = {O'Reilly, Randall C. and Frank, Michael J.},
	month = feb,
	year = {2006},
	keywords = {working memory, memory, LSTM, project.lsnn, project.lstmMEG, readme, neural networks, recurrent neural network, 1-2-AX, basal ganglia, PFC},
	pages = {283--328},
	file = {O'Reilly and Frank - 2006 - Making working memory work.pdf:C\:\\Users\\karmeni1\\Google Drive\\zotero\\O'Reilly and Frank - 2006 - Making working memory work.pdf:application/pdf},
}

@article{cho_learning_2014,
	title = {Learning phrase representations using {RNN} encoder-decoder for statistical machine translation},
	url = {http://arxiv.org/abs/1406.1078},
	abstract = {In this paper, we propose a novel neural network model called RNN Encoder–Decoder that consists of two recurrent neural networks (RNN). One RNN encodes a sequence of symbols into a ﬁxedlength vector representation, and the other decodes the representation into another sequence of symbols. The encoder and decoder of the proposed model are jointly trained to maximize the conditional probability of a target sequence given a source sequence. The performance of a statistical machine translation system is empirically found to improve by using the conditional probabilities of phrase pairs computed by the RNN Encoder–Decoder as an additional feature in the existing log-linear model. Qualitatively, we show that the proposed model learns a semantically and syntactically meaningful representation of linguistic phrases.},
	language = {en},
	urldate = {2019-04-01},
	journal = {arXiv:1406.1078 [cs, stat]},
	author = {Cho, Kyunghyun and van Merrienboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
	month = jun,
	year = {2014},
	note = {arXiv: 1406.1078},
	keywords = {GRU, project.lstmMEG, machine learning, neural networks, RNN},
}

@article{costa_cortical_2017,
	title = {Cortical microcircuits as gated-recurrent neural networks},
	url = {http://arxiv.org/abs/1711.02448},
	abstract = {Cortical circuits exhibit intricate recurrent architectures that are remarkably similar across different brain areas. Such stereotyped structure suggests the existence of common computational principles. However, such principles have remained largely elusive. Inspired by gated-memory networks, namely long short-term memory networks (LSTMs), we introduce a recurrent neural network in which information is gated through inhibitory cells that are subtractive (subLSTM). We propose a natural mapping of subLSTMs onto known canonical excitatory-inhibitory cortical microcircuits. Our empirical evaluation across sequential image classification and language modelling tasks shows that subLSTM units can achieve similar performance to LSTM units. These results suggest that cortical circuits can be optimised to solve complex contextual problems and proposes a novel view on their computational function. Overall our work provides a step towards unifying recurrent networks as used in machine learning with their biological counterparts.},
	urldate = {2019-02-18},
	journal = {arXiv:1711.02448 [cs, q-bio, stat]},
	author = {Costa, Rui Ponte and Assael, Yannis M. and Shillingford, Brendan and de Freitas, Nando and Vogels, Tim P.},
	month = nov,
	year = {2017},
	note = {arXiv: 1711.02448},
	keywords = {deep learning, LSTM, project.lstmMEG, read},
	file = {Costa et al. - 2017 - Cortical microcircuits as gated-recurrent neural n.pdf:C\:\\Users\\karmeni1\\Google Drive\\zotero\\Costa et al. - 2017 - Cortical microcircuits as gated-recurrent neural n.pdf:application/pdf},
}

@book{clark_surfing_2016,
	address = {Oxford, New York},
	title = {Surfing uncertainty: {Prediction}, action, and the embodied mind},
	isbn = {978-0-19-021701-3},
	shorttitle = {Surfing {Uncertainty}},
	abstract = {In this ground-breaking work, philosopher and cognitive scientist Andy Clark turns a common view of the human mind upside down. In stark opposition to familiar models of human cognition, Surfing Uncertainty explores exciting new theories in neuroscience, psychology, and artificial intelligence that reveal minds like ours to be prediction machines—devices that have evolved to anticipate the incoming streams of sensory stimulation before they arrive. This keeps minds like ours a few steps ahead of the game, poised to respond rapidly and apparently effortlessly to threats and opportunities as (and sometimes even before) they arise. Creatures thus equipped are more than simple response machines. They are knowing agents deep in the business of understanding their worlds. Such agents cope with changing and uncertain worlds by combining sensory evidence with informed prediction. Remarkably, the learning that makes neural prediction possible can itself be accomplished by the ceaseless effort to make better and better predictions. A single fundamental trick (the trick of trying to predict your own sensory inputs) thus enables learning, empowers moment-by-moment perception, and installs a rich understanding of the surrounding world. Action itself now appears in a new and revealing light. For action is not so much a 'response to an input' as a neat and efficient way of selecting the next 'input'. As mobile embodied agents we are forever intervening, actively bringing about the very streams of sensory information that our brains are simultaneously trying to predict. This binds perception and action in a delicate dance, a virtuous circle in which neural circuits animate, and are animated by, the movements of our own bodies. Some of our actions, in turn, structure the physical, social, and technological worlds around us. This moves the goalposts by altering the very things we need to engage and predict. Surfing Uncertainty brings work on the predictive brain into full and satisfying contact with work on the embodied and culturally situated mind. What emerges is a bold new vision of what brains do that places circular causal flows and the active structuring of the environment, center-stage. In place of cognitive couch potatoes idly awaiting the next sensory inputs, Clark's journey reveals us as proactive predictavores, skilfully surfing the waves of sensory stimulation.},
	publisher = {Oxford University Press},
	author = {Clark, Andy},
	month = jan,
	year = {2016},
}

@article{baroni_linguistic_2020,
	title = {Linguistic generalization and compositionality in modern artificial neural networks},
	volume = {375},
	issn = {0962-8436, 1471-2970},
	url = {https://royalsocietypublishing.org/doi/10.1098/rstb.2019.0307},
	doi = {10.1098/rstb.2019.0307},
	language = {en},
	number = {1791},
	urldate = {2020-03-26},
	journal = {Philosophical Transactions of the Royal Society B: Biological Sciences},
	author = {Baroni, Marco},
	month = feb,
	year = {2020},
	keywords = {LSTM, RNN, compositionality},
	pages = {20190307},
}

@incollection{jain_incorporating_2018,
	title = {Incorporating context into language encoding models for {fMRI}},
	url = {http://papers.nips.cc/paper/7897-incorporating-context-into-language-encoding-models-for-fmri.pdf},
	urldate = {2019-01-25},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 31},
	publisher = {Curran Associates, Inc.},
	author = {Jain, Shailee and Huth, Alexander},
	editor = {Bengio, S. and Wallach, H. and Larochelle, H. and Grauman, K. and Cesa-Bianchi, N. and Garnett, R.},
	year = {2018},
	keywords = {LSTM, project.lstmMEG, fMRI, language model, encoding models},
	pages = {6629--6638},
	file = {Jain and Huth - 2018 - Incorporating context into language encoding model.pdf:C\:\\Users\\karmeni1\\Google Drive\\zotero\\Jain and Huth - 2018 - Incorporating context into language encoding model.pdf:application/pdf},
}

@article{cowan_many_2017,
	title = {The many faces of working memory and short-term storage},
	volume = {24},
	issn = {1069-9384, 1531-5320},
	url = {http://link.springer.com/10.3758/s13423-016-1191-6},
	doi = {10.3758/s13423-016-1191-6},
	language = {en},
	number = {4},
	urldate = {2020-02-08},
	journal = {Psychonomic Bulletin \& Review},
	author = {Cowan, Nelson},
	month = aug,
	year = {2017},
	keywords = {working memory, review, project.lsnn, project.lstmMEG, processing memory},
	pages = {1158--1170},
}

@article{merkx_comparing_2020,
	title = {Comparing transformers and {RNNs} on predicting human sentence processing data},
	url = {http://arxiv.org/abs/2005.09471},
	abstract = {Recurrent neural networks (RNNs) have long been an architecture of interest for computational models of human sentence processing. The more recently introduced Transformer architecture has been shown to outperform recurrent neural networks on many natural language processing tasks but little is known about their ability to model human language processing. It has long been thought that human sentence reading involves something akin to recurrence and so RNNs may still have an advantage over the Transformer as a cognitive model. In this paper we train both Transformer and RNN based language models and compare their performance as a model of human sentence processing. We use the trained language models to compute surprisal values for the stimuli used in several reading experiments and use mixed linear modelling to measure how well the surprisal explains measures of human reading effort. Our analysis shows that the Transformers outperform the RNNs as cognitive models in explaining self-paced reading times and N400 strength but not gaze durations from an eye-tracking experiment.},
	urldate = {2020-08-21},
	journal = {arXiv:2005.09471 [cs]},
	author = {Merkx, Danny and Frank, Stefan L.},
	month = may,
	year = {2020},
	note = {arXiv: 2005.09471},
	keywords = {LM, transformer, N400, reading comprehension, RSVP, self-paced reading},
}

@article{diliberto_low-frequency_2015,
	title = {Low-frequency cortical entrainment to speech reflects phoneme-level processing},
	volume = {25},
	issn = {09609822},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0960982215010015},
	doi = {10.1016/j.cub.2015.08.030},
	language = {en},
	number = {19},
	urldate = {2020-08-14},
	journal = {Current Biology},
	author = {Di Liberto, Giovanni M. and O’Sullivan, James A. and Lalor, Edmund C.},
	month = oct,
	year = {2015},
	pages = {2457--2465},
}

@unpublished{radford_language_2019,
	title = {Language models are unsupervised multitask learners},
	url = {https://d4mucfpksywv.cloudfront.net/better-language-models/language-models.pdf},
	author = {Radford, Alec and Wu, Jeff and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya},
	year = {2019},
}

@article{manning_emergent_2020,
	title = {Emergent linguistic structure in artificial neural networks trained by self-supervision},
	copyright = {© 2020 . https://www-pnas-org.ru.idm.oclc.org/site/aboutpnas/licenses.xhtmlPublished under the PNAS license.},
	issn = {0027-8424, 1091-6490},
	url = {http://www.pnas.org/content/early/2020/06/02/1907367117},
	doi = {10.1073/pnas.1907367117},
	abstract = {This paper explores the knowledge of linguistic structure learned by large artificial neural networks, trained via self-supervision, whereby the model simply tries to predict a masked word in a given context. Human language communication is via sequences of words, but language understanding requires constructing rich hierarchical structures that are never observed explicitly. The mechanisms for this have been a prime mystery of human language acquisition, while engineering work has mainly proceeded by supervised learning on treebanks of sentences hand labeled for this latent structure. However, we demonstrate that modern deep contextual language models learn major aspects of this structure, without any explicit supervision. We develop methods for identifying linguistic hierarchical structure emergent in artificial neural networks and demonstrate that components in these models focus on syntactic grammatical relationships and anaphoric coreference. Indeed, we show that a linear transformation of learned embeddings in these models captures parse tree distances to a surprising degree, allowing approximate reconstruction of the sentence tree structures normally assumed by linguists. These results help explain why these models have brought such large improvements across many language-understanding tasks.},
	language = {en},
	urldate = {2020-07-24},
	journal = {Proceedings of the National Academy of Sciences},
	author = {Manning, Christopher D. and Clark, Kevin and Hewitt, John and Khandelwal, Urvashi and Levy, Omer},
	month = jun,
	year = {2020},
	pmid = {32493748},
	note = {Publisher: National Academy of Sciences
Section: Physical Sciences},
	keywords = {BERT, transformer, syntax, RNN},
}

@techreport{schrimpf_artificial_2020,
	type = {preprint},
	title = {Artificial neural networks accurately predict language processing in the brain},
	url = {http://biorxiv.org/lookup/doi/10.1101/2020.06.26.174482},
	abstract = {The ability to share ideas through language is our species’ signature cognitive skill, but how this feat is achieved by the brain remains unknown. Inspired by the success of artificial neural networks (ANNs) in explaining neural responses in perceptual tasks (Kell et al., 2018; Khaligh-Razavi \& Kriegeskorte, 2014; Schrimpf et al., 2018; Yamins et al., 2014; Zhuang et al., 2017), we here investigated whether state-of-the-art ANN language models (e.g. Devlin et al., 2018; Pennington et al., 2014; Radford et al., 2019) capture human brain activity elicited during language comprehension. We tested 43 language models spanning major current model classes on three neural datasets (including neuroimaging and intracranial recordings) and found that the most powerful generative transformer models (Radford et al., 2019) accurately predict neural responses, in some cases achieving near-perfect predictivity relative to the noise ceiling. In contrast, simpler word-based embedding models (e.g. Pennington et al., 2014) only poorly predict neural responses ({\textless}10\% predictivity). Models’ predictivities are consistent across neural datasets, and also correlate with their success on a next-word-prediction task (but not other language tasks) and ability to explain human comprehension difficulty in an independent behavioral dataset. Intriguingly, model architecture alone drives a large portion of brain predictivity, with each model’s untrained score predictive of its trained score. These results support the hypothesis that a drive to predict future inputs may shape human language processing, and perhaps the way knowledge of language is learned and organized in the brain. In addition, the finding of strong correspondences between ANNs and human representations opens the door to using the growing suite of tools for neural network interpretation to test hypotheses about the human mind.},
	language = {en},
	urldate = {2020-07-10},
	institution = {Neuroscience},
	author = {Schrimpf, Martin and Blank, Idan and Tuckute, Greta and Kauf, Carina and Hosseini, Eghbal A. and Kanwisher, Nancy and Tenenbaum, Joshua and Fedorenko, Evelina},
	month = jun,
	year = {2020},
	doi = {10.1101/2020.06.26.174482},
}

@article{lakretz_exploring_2020,
	title = {Exploring processing of nested dependencies in neural-network language models and humans},
	url = {http://arxiv.org/abs/2006.11098},
	abstract = {Recursive processing in sentence comprehension is considered a hallmark of human linguistic abilities. However, its underlying neural mechanisms remain largely unknown. We studied whether a recurrent neural network with Long Short-Term Memory units can mimic a central aspect of human sentence processing, namely the handling of long-distance agreement dependencies. Although the network was solely trained to predict the next word in a large corpus, analysis showed the emergence of a small set of specialized units that successfully handled local and long-distance syntactic agreement for grammatical number. However, simulations showed that this mechanism does not support full recursion and fails with some long-range embedded dependencies. We tested the model's predictions in a behavioral experiment where humans detected violations in number agreement in sentences with systematic variations in the singular/plural status of multiple nouns, with or without embedding. Human and model error patterns were remarkably similar, showing that the model echoes various effects observed in human data. However, a key difference was that, with embedded long-range dependencies, humans remained above chance level, while the model's systematic errors brought it below chance. Overall, our study shows that exploring the ways in which modern artificial neural networks process sentences leads to precise and testable hypotheses about human linguistic performance.},
	urldate = {2020-07-18},
	journal = {arXiv:2006.11098 [cs]},
	author = {Lakretz, Yair and Hupkes, Dieuwke and Vergallito, Alessandra and Marelli, Marco and Baroni, Marco and Dehaene, Stanislas},
	month = jun,
	year = {2020},
	note = {arXiv: 2006.11098},
	keywords = {NLP, RNN, NA-task},
}

@article{caucheteux_language_2020,
	title = {Language processing in brains and deep neural networks: computational convergence and its limits},
	copyright = {© 2020, Posted by Cold Spring Harbor Laboratory. This pre-print is available under a Creative Commons License (Attribution 4.0 International), CC BY 4.0, as described at http://creativecommons.org/licenses/by/4.0/},
	shorttitle = {Language processing in brains and deep neural networks},
	url = {https://www.biorxiv.org/content/10.1101/2020.07.03.186288v1},
	doi = {10.1101/2020.07.03.186288},
	abstract = {{\textless}p{\textgreater}Deep learning has recently allowed substantial progress in language tasks such as translation and completion. Do such models process language similarly to humans, and is this similarity driven by systematic structural, functional and learning principles? To address these issues, we tested whether the activations of 7,400 artificial neural networks trained on image, word and sentence processing linearly map onto the hierarchy of human brain responses elicited during a reading task, using source-localized magneto-encephalography (MEG) recordings of one hundred and four subjects. Our results confirm that visual, word and language models sequentially correlate with distinct areas of the left-lateralized cortical hierarchy of reading. However, only specific subsets of these models converge towards brain-like representations during their training. Specifically, when the algorithms are trained on language modeling, their middle layers become increasingly similar to the late responses of the language network in the brain. By contrast, input and output word embedding layers often diverge away from brain activity during training. These differences are primarily rooted in the sustained and bilateral responses of the temporal and frontal cortices. Together, these results suggest that the compositional - but not the lexical - representations of modern language models converge to a brain-like solution.{\textless}/p{\textgreater}},
	language = {en},
	urldate = {2020-07-10},
	journal = {bioRxiv},
	author = {Caucheteux, Charlotte and King, Jean-Rémi},
	month = jul,
	year = {2020},
	note = {Publisher: Cold Spring Harbor Laboratory
Section: New Results},
	keywords = {transformer, MEG, CNN, word2vec, ecog-nlp},
	pages = {2020.07.03.186288},
}

@inproceedings{gauthier_linking_2019,
	address = {Hong Kong, China},
	title = {Linking artificial and human neural representations of language},
	url = {https://www.aclweb.org/anthology/D19-1050},
	doi = {10.18653/v1/D19-1050},
	language = {en},
	urldate = {2020-06-29},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Gauthier, Jon and Levy, Roger},
	year = {2019},
	keywords = {NLP, fMRI},
	pages = {529--539},
}

@article{lakretz_what_2020,
	title = {What limits our capacity to process nested long-range dependencies in sentence comprehension?},
	volume = {22},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/1099-4300/22/4/446},
	doi = {10.3390/e22040446},
	abstract = {Sentence comprehension requires inferring, from a sequence of words, the structure of syntactic relationships that bind these words into a semantic representation. Our limited ability to build some specific syntactic structures, such as nested center-embedded clauses (e.g., \&ldquo;The dog that the cat that the mouse bit chased ran away\&rdquo;), suggests a striking capacity limitation of sentence processing, and thus offers a window to understand how the human brain processes sentences. Here, we review the main hypotheses proposed in psycholinguistics to explain such capacity limitation. We then introduce an alternative approach, derived from our recent work on artificial neural networks optimized for language modeling, and predict that capacity limitation derives from the emergence of sparse and feature-specific syntactic units. Unlike psycholinguistic theories, our neural network-based framework provides precise capacity-limit predictions without making any a priori assumptions about the form of the grammar or parser. Finally, we discuss how our framework may clarify the mechanistic underpinning of language processing and its limitations in the human brain.},
	language = {en},
	number = {4},
	urldate = {2020-06-26},
	journal = {Entropy},
	author = {Lakretz, Yair and Dehaene, Stanislas and King, Jean-Rémi},
	month = apr,
	year = {2020},
	note = {Number: 4
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {LSTM, syntax, ANN, surprisal, RNN, ACT-R, DLT},
	pages = {446},
}

@article{sinz_engineering_2019,
	title = {Engineering a less artificial intelligence},
	volume = {103},
	issn = {08966273},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0896627319307408},
	doi = {10.1016/j.neuron.2019.08.034},
	language = {en},
	number = {6},
	urldate = {2020-01-19},
	journal = {Neuron},
	author = {Sinz, Fabian H. and Pitkow, Xaq and Reimer, Jacob and Bethge, Matthias and Tolias, Andreas S.},
	month = sep,
	year = {2019},
	keywords = {readme, proposal.SanD},
	pages = {967--979},
}

@article{wolf_hugging_2019,
	title = {{HuggingFace}'s transformers: state-of-the-art natural language processing},
	volume = {abs/1910.03771},
	journal = {ArXiv},
	author = {Wolf, Thomas and Debut, Lysandre and Sanh, Victor and Chaumond, Julien and Delangue, Clement and Moi, Anthony and Cistac, Pierric and Rault, Tim and Louf, R'emi and Funtowicz, Morgan and Brew, Jamie},
	year = {2019},
	keywords = {NLP, BERT, methods},
}

@article{fiebig_spiking_2017,
	title = {A spiking working memory model based on {Hebbian} short-term potentiation},
	volume = {37},
	issn = {0270-6474, 1529-2401},
	url = {http://www.jneurosci.org/lookup/doi/10.1523/JNEUROSCI.1989-16.2016},
	doi = {10.1523/JNEUROSCI.1989-16.2016},
	language = {en},
	number = {1},
	urldate = {2019-09-09},
	journal = {The Journal of Neuroscience},
	author = {Fiebig, Florian and Lansner, Anders},
	month = jan,
	year = {2017},
	keywords = {Hebbian learning, working memory, project.lsnn, readme, spiking neurons},
	pages = {83--96},
}

@article{barak_working_2014,
	title = {Working models of working memory},
	volume = {25},
	issn = {09594388},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0959438813002158},
	doi = {10.1016/j.conb.2013.10.008},
	language = {en},
	urldate = {2019-08-20},
	journal = {Current Opinion in Neurobiology},
	author = {Barak, Omri and Tsodyks, Misha},
	month = apr,
	year = {2014},
	keywords = {working memory, read, processing memory, spiking neurons},
	pages = {20--24},
}

@article{clark_what_2019,
	title = {What does {BERT} look at? {An} analysis of {BERT}'s attention},
	shorttitle = {What {Does} {BERT} {Look} {At}?},
	url = {http://arxiv.org/abs/1906.04341},
	abstract = {Large pre-trained neural networks such as BERT have had great recent success in NLP, motivating a growing body of research investigating what aspects of language they are able to learn from unlabeled data. Most recent analysis has focused on model outputs (e.g., language model surprisal) or internal vector representations (e.g., probing classifiers). Complementary to these works, we propose methods for analyzing the attention mechanisms of pre-trained models and apply them to BERT. BERT's attention heads exhibit patterns such as attending to delimiter tokens, specific positional offsets, or broadly attending over the whole sentence, with heads in the same layer often exhibiting similar behaviors. We further show that certain attention heads correspond well to linguistic notions of syntax and coreference. For example, we find heads that attend to the direct objects of verbs, determiners of nouns, objects of prepositions, and coreferent mentions with remarkably high accuracy. Lastly, we propose an attention-based probing classifier and use it to further demonstrate that substantial syntactic information is captured in BERT's attention.},
	urldate = {2019-08-12},
	journal = {arXiv:1906.04341 [cs]},
	author = {Clark, Kevin and Khandelwal, Urvashi and Levy, Omer and Manning, Christopher D.},
	month = jun,
	year = {2019},
	note = {arXiv: 1906.04341},
	keywords = {NLP, BERT, transformer, attention, readme, language model, computational linguistics},
}

@article{goldberg_assessing_2019,
	title = {Assessing {BERT}'s syntactic abilities},
	url = {http://arxiv.org/abs/1901.05287},
	abstract = {I assess the extent to which the recently introduced BERT model captures English syntactic phenomena, using (1) naturally-occurring subject-verb agreement stimuli; (2) "coloreless green ideas" subject-verb agreement stimuli, in which content words in natural sentences are randomly replaced with words sharing the same part-of-speech and inflection; and (3) manually crafted stimuli for subject-verb agreement and reflexive anaphora phenomena. The BERT model performs remarkably well on all cases.},
	urldate = {2019-08-12},
	journal = {arXiv:1901.05287 [cs]},
	author = {Goldberg, Yoav},
	month = jan,
	year = {2019},
	note = {arXiv: 1901.05287},
	keywords = {NLP, BERT, LSTM, readme, language model},
}

@article{enguehard_exploring_2017,
	title = {Exploring the syntactic abilities of {RNNs} with multi-task learning},
	url = {http://arxiv.org/abs/1706.03542},
	abstract = {Recent work has explored the syntactic abilities of RNNs using the subject-verb agreement task, which diagnoses sensitivity to sentence structure. RNNs performed this task well in common cases, but faltered in complex sentences (Linzen et al., 2016). We test whether these errors are due to inherent limitations of the architecture or to the relatively indirect supervision provided by most agreement dependencies in a corpus. We trained a single RNN to perform both the agreement task and an additional task, either CCG supertagging or language modeling. Multi-task training led to significantly lower error rates, in particular on complex sentences, suggesting that RNNs have the ability to evolve more sophisticated syntactic representations than shown before. We also show that easily available agreement training data can improve performance on other syntactic tasks, in particular when only a limited amount of training data is available for those tasks. The multi-task paradigm can also be leveraged to inject grammatical knowledge into language models.},
	urldate = {2019-08-12},
	journal = {arXiv:1706.03542 [cs]},
	author = {Enguehard, Emile and Goldberg, Yoav and Linzen, Tal},
	month = jun,
	year = {2017},
	note = {arXiv: 1706.03542},
	keywords = {readme},
}

@book{goldberg_neural_2017,
	address = {San Rafael},
	series = {Synthesis lectures on human language technologies},
	title = {Neural network methods for natural language processing},
	isbn = {978-1-62705-295-5 978-1-68173-235-0 978-1-62705-298-6},
	language = {eng},
	number = {37},
	publisher = {Morgan \& Claypool Publishers},
	author = {Goldberg, Yoav},
	year = {2017},
	note = {OCLC: 990794614},
	keywords = {NLP, machine learning, readme, neural networks},
	file = {Goldberg - 2017 - Neural Network Methods in Natural Language Processing.pdf:C\:\\Users\\karmeni1\\Google Drive\\zotero\\Goldberg - 2017 - Neural Network Methods in Natural Language Processing.pdf:application/pdf},
}

@article{kahana_computational_2020,
	title = {Computational models of memory search},
	volume = {71},
	issn = {0066-4308, 1545-2085},
	url = {https://www.annualreviews.org/doi/10.1146/annurev-psych-010418-103358},
	doi = {10.1146/annurev-psych-010418-103358},
	abstract = {The capacity to search memory for events learned in a particular context stands as one of the most remarkable feats of the human brain. How is memory search accomplished? First, I review the central ideas investigated by theorists developing models of memory. Then, I review select benchmark findings concerning memory search and analyze two influential computational approaches to modeling memory search: dual-store theory and retrieved context theory. Finally, I discuss the key theoretical ideas that have emerged from these modeling studies and the open questions that need to be answered by future research.},
	language = {en},
	number = {1},
	urldate = {2021-03-08},
	journal = {Annual Review of Psychology},
	author = {Kahana, Michael J.},
	month = jan,
	year = {2020},
	keywords = {serial recall, immediate recall, free recall, memory search},
	pages = {107--138},
}

@article{farrell_temporal_2012,
	title = {Temporal clustering and sequencing in short-term memory and episodic memory.},
	volume = {119},
	issn = {1939-1471, 0033-295X},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/a0027371},
	doi = {10.1037/a0027371},
	language = {en},
	number = {2},
	urldate = {2021-03-09},
	journal = {Psychological Review},
	author = {Farrell, Simon},
	year = {2012},
	keywords = {short-term memory, episodic memory, clustering, chunking},
	pages = {223--271},
}

@article{franklin_structured_2020,
	title = {Structured {Event} {Memory}: {A} neuro-symbolic model of event cognition.},
	volume = {127},
	issn = {1939-1471, 0033-295X},
	shorttitle = {Structured {Event} {Memory}},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/rev0000177},
	doi = {10.1037/rev0000177},
	language = {en},
	number = {3},
	urldate = {2021-03-09},
	journal = {Psychological Review},
	author = {Franklin, Nicholas T. and Norman, Kenneth A. and Ranganath, Charan and Zacks, Jeffrey M. and Gershman, Samuel J.},
	month = apr,
	year = {2020},
	keywords = {episodic memory, event cognition},
	pages = {327--361},
}

@article{futrell_rnns_2018,
	title = {{RNNs} as psycholinguistic subjects: {Syntactic} state and grammatical dependency},
	shorttitle = {{RNNs} as psycholinguistic subjects},
	url = {http://arxiv.org/abs/1809.01329},
	abstract = {Recurrent neural networks (RNNs) are the state of the art in sequence modeling for natural language. However, it remains poorly understood what grammatical characteristics of natural language they implicitly learn and represent as a consequence of optimizing the language modeling objective. Here we deploy the methods of controlled psycholinguistic experimentation to shed light on to what extent RNN behavior reflects incremental syntactic state and grammatical dependency representations known to characterize human linguistic behavior. We broadly test two publicly available long short-term memory (LSTM) English sequence models, and learn and test a new Japanese LSTM. We demonstrate that these models represent and maintain incremental syntactic state, but that they do not always generalize in the same way as humans. Furthermore, none of our models learn the appropriate grammatical dependency configurations licensing reflexive pronouns or negative polarity items.},
	urldate = {2021-03-09},
	journal = {arXiv:1809.01329 [cs]},
	author = {Futrell, Richard and Wilcox, Ethan and Morita, Takashi and Levy, Roger},
	month = sep,
	year = {2018},
	note = {arXiv: 1809.01329},
	keywords = {RNN},
}

@article{mahto_multi-timescale_2020,
	title = {Multi-timescale representation learning in {LSTM} language models},
	url = {http://arxiv.org/abs/2009.12727},
	abstract = {Although neural language models are effective at capturing statistics of natural language, their representations are challenging to interpret. In particular, it is unclear how these models retain information over multiple timescales. In this work, we construct explicitly multi-timescale language models by manipulating the input and forget gate biases in a long short-term memory (LSTM) network. The distribution of timescales is selected to approximate power law statistics of natural language through a combination of exponentially decaying memory cells. We then empirically analyze the timescale of information routed through each part of the model using word ablation experiments and forget gate visualizations. These experiments show that the multi-timescale model successfully learns representations at the desired timescales, and that the distribution includes longer timescales than a standard LSTM. Further, information about high-,mid-, and low-frequency words is routed preferentially through units with the appropriate timescales. Thus we show how to construct language models with interpretable representations of different information timescales.},
	urldate = {2021-03-17},
	journal = {arXiv:2009.12727 [cs]},
	author = {Mahto, Shivangi and Vo, Vy A. and Turek, Javier S. and Huth, Alexander G.},
	month = sep,
	year = {2020},
	note = {arXiv: 2009.12727},
}

@article{subramanian_multi-scale_2020,
	title = {Multi-scale transformer language models},
	url = {http://arxiv.org/abs/2005.00581},
	abstract = {We investigate multi-scale transformer language models that learn representations of text at multiple scales, and present three different architectures that have an inductive bias to handle the hierarchical nature of language. Experiments on large-scale language modeling benchmarks empirically demonstrate favorable likelihood vs memory footprint trade-offs, e.g. we show that it is possible to train a hierarchical variant with 30 layers that has 23\% smaller memory footprint and better perplexity, compared to a vanilla transformer with less than half the number of layers, on the Toronto BookCorpus. We analyze the advantages of learned representations at multiple scales in terms of memory footprint, compute time, and perplexity, which are particularly appealing given the quadratic scaling of transformers' run time and memory usage with respect to sequence length.},
	urldate = {2021-03-17},
	journal = {arXiv:2005.00581 [cs]},
	author = {Subramanian, Sandeep and Collobert, Ronan and Ranzato, Marc'Aurelio and Boureau, Y.-Lan},
	month = may,
	year = {2020},
	note = {arXiv: 2005.00581},
}

@book{logie_working_2020,
	address = {New York},
	title = {Working memory: the state of the science},
	isbn = {978-0-19-884228-6},
	shorttitle = {Working memory},
	abstract = {"Working memory refers to how we keep track of what we are doing moment to moment throughout our waking lives. It allows us to remember what we have just done, focus on what we are doing now, to solve problems, be creative, think about what we will be doing in the next few seconds, and continually to update in our mind changes around us throughout the day. This book brings together in one volume, state-of-the-science chapters written by some of the most productive and well known working memory researchers worldwide. Chapters cover leading edge research on working memory, using behavioural experimental techniques, neuroimaging, computational modelling, development across the healthy human lifespan, and studies of neurodegenerative disease and focal brain damage"--},
	publisher = {Oxford University Press},
	editor = {Logie, Robert and Camos, Valérie and Cowan, Nelson},
	year = {2020},
	keywords = {working memory, LTM, STM},
}

@article{tulving_how_1985,
	title = {How many memory systems are there},
	doi = {10.1037/0003-066X.40.4.385},
	abstract = {Memory is made up of a number of interrelated systems, organized structures of operating components consisting of neural substrates and their behavioral and cognitive correlates. A ternary clas- sificatory scheme of memory is proposed in which procedural, semantic, and episodic memory constitute a "monohierarchical" arrangement: Episodic memory is a specialized subsystem of semantic memory, and semantic memory is a specialized subsystem of procedural memory. The three memory systems differ from one another in a number of ways, including the kind of consciousness that characterizes their operations. The ternary scheme overlaps with di- chotomies and trichotomies of memory proposed by others. Evidence for multiple systems is derived from many sources. Illustrative data are provided by ex- periments in which direct priming effects are found to be both functionally and stochastically independent of recognition memory. Solving puzzles in science has much in common with solving puzzles for amusement, but the two differ in important respects. Consider, for instance, the jigsaw puzzle that scientific activity frequently imitates. The everyday version of the puzzle is determinate: It consists of a target picture and jigsaw pieces that, when properly assembled, are guaranteed to match the picture. Scientific puzzles are indeter- minate: The number of pieces required to complete a picture is unpredictable; a particular piece may fit many pictures or none; it may fit only one picture, but the picture itself may be unknown; or the hypothetical picture may be imagined, but its com- ponent pieces may remain undiscovered. This article is about a current puzzle in the science of memory. It entails an imaginary picture and a search for pieces that fit it. The picture, or the hypothesis, depicts memory as consisting of a number of systems, each system serving somewhat different purposes and operating according to some- what different principles. Together they form the marvelous capacity that we call by the single name of memory, the capacity that permits organisms to benefit from their past experiences. Such a picture is at variance with conventional wisdom that holds memory to be essentially a single system, the idea that "memory is memory." The article consists of three main sections. In the first, 1 present some pretheoretical reasons for hypothesizing the existence of multiple memory systems and briefly discuss the concept of memory system. In the second, I describe a ternary classifi- catory scheme of memory--consisting of procedural, semantic, and episodic memory--and briefly com- pare this scheme with those proposed by others. In the third, I discuss the nature and logic of evidence for multiple systems and describe some experiments that have yielded data revealing independent effects of one and the same act of learning, effects seemingly at variance with the idea of a single system. I answer the question posed in the title of the article in the short concluding section.},
	author = {Tulving, E.},
	year = {1985},
}

@article{sukhbaatar_end--end_2015,
	title = {End-to-end memory networks},
	url = {http://arxiv.org/abs/1503.08895},
	abstract = {We introduce a neural network with a recurrent attention model over a possibly large external memory. The architecture is a form of Memory Network (Weston et al., 2015) but unlike the model in that work, it is trained end-to-end, and hence requires significantly less supervision during training, making it more generally applicable in realistic settings. It can also be seen as an extension of RNNsearch to the case where multiple computational steps (hops) are performed per output symbol. The flexibility of the model allows us to apply it to tasks as diverse as (synthetic) question answering and to language modeling. For the former our approach is competitive with Memory Networks, but with less supervision. For the latter, on the Penn TreeBank and Text8 datasets our approach demonstrates comparable performance to RNNs and LSTMs. In both cases we show that the key concept of multiple computational hops yields improved results.},
	urldate = {2021-04-05},
	journal = {arXiv:1503.08895 [cs]},
	author = {Sukhbaatar, Sainbayar and Szlam, Arthur and Weston, Jason and Fergus, Rob},
	month = nov,
	year = {2015},
	note = {arXiv: 1503.08895},
}

@article{graves_neural_2014,
	title = {Neural turing machines},
	url = {http://arxiv.org/abs/1410.5401},
	abstract = {We extend the capabilities of neural networks by coupling them to external memory resources, which they can interact with by attentional processes. The combined system is analogous to a Turing Machine or Von Neumann architecture but is differentiable end-to-end, allowing it to be efficiently trained with gradient descent. Preliminary results demonstrate that Neural Turing Machines can infer simple algorithms such as copying, sorting, and associative recall from input and output examples.},
	urldate = {2021-04-05},
	journal = {arXiv:1410.5401 [cs]},
	author = {Graves, Alex and Wayne, Greg and Danihelka, Ivo},
	month = dec,
	year = {2014},
	note = {arXiv: 1410.5401
version: 2},
}

@inproceedings{yogatama_memory_2018,
	title = {Memory architectures in recurrent neural network language models},
	url = {https://openreview.net/forum?id=SkFqf0lAZ},
	abstract = {We compare and analyze sequential, random access, and stack memory architectures for recurrent neural network language models. Our experiments on the Penn Treebank and Wikitext-2 datasets show that...},
	language = {en},
	urldate = {2021-04-05},
	author = {Yogatama, Dani and Miao, Yishu and Melis, Gabor and Ling, Wang and Kuncoro, Adhiguna and Dyer, Chris and Blunsom, Phil},
	month = feb,
	year = {2018},
}

@article{jordan_artificial_2019,
	title = {Artificial intelligence—{The} revolution hasn’t happened yet},
	volume = {1},
	issn = {,},
	url = {https://hdsr.mitpress.mit.edu/pub/wot7mkc1/release/9},
	doi = {10.1162/99608f92.f06c6e61},
	language = {en},
	number = {1},
	urldate = {2021-04-10},
	journal = {Harvard Data Science Review},
	author = {Jordan, Michael I.},
	month = jul,
	year = {2019},
	note = {Publisher: PubPub},
	keywords = {AI, cybernetics, civil engineering, intelligence augmentation, inteligence infrastructure},
	file = {Snapshot:C\:\\Users\\karmeni1\\Zotero\\storage\\RCDD98N9\\9.html:text/html},
}

@article{potter_conceptual_2012,
	title = {Conceptual short term memory in perception and thought},
	volume = {3},
	issn = {1664-1078},
	url = {https://www.frontiersin.org/articles/10.3389/fpsyg.2012.00113/full},
	doi = {10.3389/fpsyg.2012.00113},
	abstract = {Conceptual short term memory (CSTM) is a theoretical construct that provides one answer to the question of how perceptual and conceptual processes are related. CSTM is a mental buffer and processor in which current perceptual stimuli and their associated concepts from long term memory (LTM) are represented briefly, allowing meaningful patterns or structures to be identified (Potter, 1993, 1999, 2009). CSTM is different from and complementary to other proposed forms of working memory: it is engaged extremely rapidly, has a large but ill-defined capacity, is largely unconscious, and is the basis for the unreflective understanding that is characteristic of everyday experience. The key idea behind CSTM is that most cognitive processing occurs without review or rehearsal of material in standard working memory and with little or no conscious reasoning. When one perceives a meaningful stimulus such as a word, picture, or object, it is rapidly identified at a conceptual level and in turn activates associated information from long term memory. New links among concurrently active concepts are formed in CSTM, shaped by parsing mechanisms of language or grouping principles in scene perception and by higher-level knowledge and current goals. The resulting structure represents the gist of a picture or the meaning of a sentence, and it is this structure that we are conscious of and that can be maintained in standard working memory and consolidated into long term memory. Momentarily activated information that is not incorporated into such structures either never becomes conscious or is rapidly forgotten. This whole cycle--identification of perceptual stimuli, memory recruitment, structuring, consolidation in long term memory, and forgetting of nonstructured material--may occur in less than 1 second when viewing a pictured scene or reading a sentence. The evidence for such a process is reviewed and its implications for the relation of perception and cognition are discussed.},
	language = {English},
	urldate = {2021-04-28},
	journal = {Frontiers in Psychology},
	author = {Potter, Mary C.},
	year = {2012},
	note = {Publisher: Frontiers},
	keywords = {short-term memory, phonological loop, memory span},
}

@article{just_hybrid_2002,
	title = {A hybrid architecture for working memory: {Reply} to {MacDonald} and {Christiansen} (2002).},
	volume = {109},
	issn = {1939-1471, 0033-295X},
	shorttitle = {A hybrid architecture for working memory},
	url = {http://doi.apa.org/getdoi.cfm?doi=10.1037/0033-295X.109.1.55},
	doi = {10.1037/0033-295X.109.1.55},
	language = {en},
	number = {1},
	urldate = {2021-05-10},
	journal = {Psychological Review},
	author = {Just, Marcel Adam and Varma, Sashank},
	year = {2002},
	keywords = {working memory},
	pages = {55--65},
}

@article{daneman_working_1996,
	title = {Working memory and language comprehension: {A} meta-analysis},
	volume = {3},
	issn = {1069-9384, 1531-5320},
	shorttitle = {Working memory and language comprehension},
	url = {http://link.springer.com/10.3758/BF03214546},
	doi = {10.3758/BF03214546},
	language = {en},
	number = {4},
	urldate = {2021-05-10},
	journal = {Psychonomic Bulletin \& Review},
	author = {Daneman, Meredyth and Merikle, Philip M.},
	month = dec,
	year = {1996},
	pages = {422--433},
	file = {Full Text:C\:\\Users\\karmeni1\\Zotero\\storage\\NKC9UDE5\\Daneman and Merikle - 1996 - Working memory and language comprehension A meta-.pdf:application/pdf},
}

@inproceedings{nematzadeh_memory_2020,
	title = {On memory in human and artificial language processing systems},
	booktitle = {Bridging {AI} and cognitive science},
	author = {Nematzadeh, A. and Ruder, Sebastian and Yogatama, Dani},
	year = {2020},
}

@misc{lecun_self-supervised_2021,
	title = {Self-supervised learning: {The} dark matter of intelligence},
	shorttitle = {Self-supervised learning},
	url = {https://ai.facebook.com/blog/self-supervised-learning-the-dark-matter-of-intelligence/},
	abstract = {How can we build machines with human-level intelligence? There’s a limit to how far the field of AI can go with supervised learning alone. Here's why self-supervised learning is one of the most promising ways to make significant progress in AI.},
	language = {en},
	urldate = {2021-05-11},
	journal = {Facebook AI Blog},
	author = {LeCun, Yann and Mishra, Ishan},
	month = mar,
	year = {2021},
}

@article{ishiguro_detrimental_2021,
	title = {The detrimental effect of semantic similarity in short-term memory tasks: {A} meta-regression approach},
	volume = {28},
	issn = {1069-9384, 1531-5320},
	shorttitle = {The detrimental effect of semantic similarity in short-term memory tasks},
	url = {https://link.springer.com/10.3758/s13423-020-01815-7},
	doi = {10.3758/s13423-020-01815-7},
	language = {en},
	number = {2},
	urldate = {2021-05-23},
	journal = {Psychonomic Bulletin \& Review},
	author = {Ishiguro, Sho and Saito, Satoru},
	month = apr,
	year = {2021},
	pages = {384--408},
}

@article{kaplan_scaling_2020,
	title = {Scaling laws for neural language models},
	url = {http://arxiv.org/abs/2001.08361},
	abstract = {We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence.},
	urldate = {2021-05-23},
	journal = {arXiv:2001.08361 [cs, stat]},
	author = {Kaplan, Jared and McCandlish, Sam and Henighan, Tom and Brown, Tom B. and Chess, Benjamin and Child, Rewon and Gray, Scott and Radford, Alec and Wu, Jeffrey and Amodei, Dario},
	month = jan,
	year = {2020},
	note = {arXiv: 2001.08361},
}

@inproceedings{bender_dangers_2021,
	address = {New York, NY, USA},
	series = {{FAccT} '21},
	title = {On the dangers of stochastic parrots: {Can} language models be too big? \&\#x1f99c;},
	isbn = {978-1-4503-8309-7},
	shorttitle = {On the {Dangers} of {Stochastic} {Parrots}},
	url = {https://doi.org/10.1145/3442188.3445922},
	doi = {10.1145/3442188.3445922},
	abstract = {The past 3 years of work in NLP have been characterized by the development and deployment of ever larger language models, especially for English. BERT, its variants, GPT-2/3, and others, most recently Switch-C, have pushed the boundaries of the possible both through architectural innovations and through sheer size. Using these pretrained models and the methodology of fine-tuning them for specific tasks, researchers have extended the state of the art on a wide array of tasks as measured by leaderboards on specific benchmarks for English. In this paper, we take a step back and ask: How big is too big? What are the possible risks associated with this technology and what paths are available for mitigating those risks? We provide recommendations including weighing the environmental and financial costs first, investing resources into curating and carefully documenting datasets rather than ingesting everything on the web, carrying out pre-development exercises evaluating how the planned approach fits into research and development goals and supports stakeholder values, and encouraging research directions beyond ever larger language models.},
	urldate = {2021-05-23},
	booktitle = {Proceedings of the 2021 {ACM} {Conference} on {Fairness}, {Accountability}, and {Transparency}},
	publisher = {Association for Computing Machinery},
	author = {Bender, Emily M. and Gebru, Timnit and McMillan-Major, Angelina and Shmitchell, Shmargaret},
	month = mar,
	year = {2021},
	pages = {610--623},
}

@article{henson_unchained_1996,
	title = {Unchained memory: {Error} patterns rule out chaining models of immediate serial recall},
	volume = {49},
	issn = {0272-4987, 1464-0740},
	shorttitle = {Unchained {Memory}},
	url = {http://journals.sagepub.com/doi/10.1080/713755612},
	doi = {10.1080/713755612},
	abstract = {Many models of serial recall assume a chaining mechanism whereby each item associatively evokes the next in sequence. Chaining predicts that, when sequences comprise alternating confusable and non-confusable items, confusable items should increase the probability of errors in recall of following non-confusable items. Two experiments using visual presentation and one using vocalized presentation test this prediction and demonstrate that: (1) more errors occur in recall of confusable than alternated non-confusable items, revealing a “sawtooth” in serial position curves; (2) the presence of confusable items often has no influence on recall of the non-confusable items; and (3) the confusability of items does not affect the type of errors that follow them. These results are inconsistent with the chaining hypothesis. Further analysis of errors shows that most transpositions occur over short distances (the locality constraint), confusable items tend to interchange (the similarity constraint), and repeated responses are rare and far apart (the repetition constraint). The complete pattern of errors presents problems for most current models of serial recall, whether or not they employ chaining. An alternative model is described that is consistent with these constraints and that simulates the detailed pattern of errors observed.},
	language = {en},
	number = {1},
	urldate = {2021-05-23},
	journal = {The Quarterly Journal of Experimental Psychology Section A},
	author = {Henson, Richard N. A. and Norris, Dennis G. and Page, Michael P. A. and Baddeley, Alan D.},
	month = feb,
	year = {1996},
	pages = {80--115},
}

@inproceedings{schlag_linear_2021,
	series = {Proceedings of {Machine} {Learning} {Research}},
	title = {Linear transformers are secretly fast weight programmers},
	volume = {139},
	url = {https://proceedings.mlr.press/v139/schlag21a.html},
	abstract = {We show the formal equivalence of linearised self-attention mechanisms and fast weight controllers from the early ’90s, where a slow neural net learns by gradient descent to program the fast weights of another net through sequences of elementary programming instructions which are additive outer products of self-invented activation patterns (today called keys and values). Such Fast Weight Programmers (FWPs) learn to manipulate the contents of a finite memory and dynamically interact with it. We infer a memory capacity limitation of recent linearised softmax attention variants, and replace the purely additive outer products by a delta rule-like programming instruction, such that the FWP can more easily learn to correct the current mapping from keys to values. The FWP also learns to compute dynamically changing learning rates. We also propose a new kernel function to linearise attention which balances simplicity and effectiveness. We conduct experiments on synthetic retrieval problems as well as standard machine translation and language modelling tasks which demonstrate the benefits of our methods.},
	booktitle = {Proceedings of the 38th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Schlag, Imanol and Irie, Kazuki and Schmidhuber, Jürgen},
	editor = {Meila, Marina and Zhang, Tong},
	month = jul,
	year = {2021},
	keywords = {NLP, LSTM, RNN, fast weights, meta-learning},
	pages = {9355--9366},
}

@article{schlag_learning_2021,
	title = {Learning associative inference using fast weight memory},
	url = {http://arxiv.org/abs/2011.07831},
	abstract = {Humans can quickly associate stimuli to solve problems in novel contexts. Our novel neural network model learns state representations of facts that can be composed to perform such associative inference. To this end, we augment the LSTM model with an associative memory, dubbed Fast Weight Memory (FWM). Through differentiable operations at every step of a given input sequence, the LSTM updates and maintains compositional associations stored in the rapidly changing FWM weights. Our model is trained end-to-end by gradient descent and yields excellent performance on compositional language reasoning problems, meta-reinforcement-learning for POMDPs, and small-scale word-level language modelling.},
	urldate = {2021-09-16},
	journal = {arXiv:2011.07831 [cs]},
	author = {Schlag, Imanol and Munkhdalai, Tsendsuren and Schmidhuber, Jürgen},
	month = feb,
	year = {2021},
	note = {arXiv: 2011.07831},
}

@article{schmidhuber_learning_1992,
	title = {Learning to control fast-weight memories: {An} alternative to dynamic recurrent networks},
	volume = {4},
	issn = {0899-7667, 1530-888X},
	shorttitle = {Learning to {Control} {Fast}-{Weight} {Memories}},
	url = {https://direct.mit.edu/neco/article/4/1/131-139/5620},
	doi = {10.1162/neco.1992.4.1.131},
	abstract = {Previous algorithms for supervised sequence learning are based on dynamic recurrent networks. This paper describes an alternative class of gradient-based systems consisting of two feedforward nets that learn to deal with temporal sequences using fast weights: The first net learns to produce context-dependent weight changes for the second net whose weights may vary very quickly. The method offers the potential for STM storage efficiency: A single weight (instead of a full-fledged unit) may be sufficient for storing temporal information. Various learning methods are derived. Two experiments with unknown time delays illustrate the approach. One experiment shows how the system can be used for adaptive temporary variable binding.},
	language = {en},
	number = {1},
	urldate = {2021-09-16},
	journal = {Neural Computation},
	author = {Schmidhuber, Jürgen},
	month = jan,
	year = {1992},
	keywords = {NLP, LSTM, fast weights},
	pages = {131--139},
}

@incollection{paszke_pytorch_2019,
	title = {{PyTorch}: {An} imperative style, high-performance deep learning library},
	url = {http://papers.neurips.cc/paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems} 32},
	publisher = {Curran Associates, Inc.},
	author = {Paszke, Adam and Gross, Sam and Massa, Francisco and Lerer, Adam and Bradbury, James and Chanan, Gregory and Killeen, Trevor and Lin, Zeming and Gimelshein, Natalia and Antiga, Luca and Desmaison, Alban and Kopf, Andreas and Yang, Edward and DeVito, Zachary and Raison, Martin and Tejani, Alykhan and Chilamkurthy, Sasank and Steiner, Benoit and Fang, Lu and Bai, Junjie and Chintala, Soumith},
	editor = {Wallach, H. and Larochelle, H. and Beygelzimer, A. and Alché-Buc, F. d' and Fox, E. and Garnett, R.},
	year = {2019},
	pages = {8024--8035},
}

@article{michaelov_different_2021,
	title = {Different kinds of cognitive plausibility: why are transformers better than {RNNs} at predicting {N400} amplitude?},
	shorttitle = {Different kinds of cognitive plausibility},
	url = {http://arxiv.org/abs/2107.09648},
	abstract = {Despite being designed for performance rather than cognitive plausibility, transformer language models have been found to be better at predicting metrics used to assess human language comprehension than language models with other architectures, such as recurrent neural networks. Based on how well they predict the N400, a neural signal associated with processing difficulty, we propose and provide evidence for one possible explanation - their predictions are affected by the preceding context in a way analogous to the effect of semantic facilitation in humans.},
	urldate = {2021-10-18},
	journal = {arXiv:2107.09648 [cs]},
	author = {Michaelov, James A. and Bardolph, Megan D. and Coulson, Seana and Bergen, Benjamin K.},
	month = jul,
	year = {2021},
	note = {arXiv: 2107.09648},
	keywords = {working memory, GPT-2, N400, LSTM, JRNN, WM},
}

@article{bommasani_opportunities_2021,
	title = {On the opportunities and risks of foundation models},
	url = {http://arxiv.org/abs/2108.07258},
	abstract = {AI is undergoing a paradigm shift with the rise of models (e.g., BERT, DALL-E, GPT-3) that are trained on broad data at scale and are adaptable to a wide range of downstream tasks. We call these models foundation models to underscore their critically central yet incomplete character. This report provides a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations). Though foundation models are based on standard deep learning and transfer learning, their scale results in new emergent capabilities,and their effectiveness across so many tasks incentivizes homogenization. Homogenization provides powerful leverage but demands caution, as the defects of the foundation model are inherited by all the adapted models downstream. Despite the impending widespread deployment of foundation models, we currently lack a clear understanding of how they work, when they fail, and what they are even capable of due to their emergent properties. To tackle these questions, we believe much of the critical research on foundation models will require deep interdisciplinary collaboration commensurate with their fundamentally sociotechnical nature.},
	urldate = {2021-10-18},
	journal = {arXiv:2108.07258 [cs]},
	author = {Bommasani, Rishi and Hudson, Drew A. and Adeli, Ehsan and Altman, Russ and Arora, Simran and von Arx, Sydney and Bernstein, Michael S. and Bohg, Jeannette and Bosselut, Antoine and Brunskill, Emma and Brynjolfsson, Erik and Buch, Shyamal and Card, Dallas and Castellon, Rodrigo and Chatterji, Niladri and Chen, Annie and Creel, Kathleen and Davis, Jared Quincy and Demszky, Dora and Donahue, Chris and Doumbouya, Moussa and Durmus, Esin and Ermon, Stefano and Etchemendy, John and Ethayarajh, Kawin and Fei-Fei, Li and Finn, Chelsea and Gale, Trevor and Gillespie, Lauren and Goel, Karan and Goodman, Noah and Grossman, Shelby and Guha, Neel and Hashimoto, Tatsunori and Henderson, Peter and Hewitt, John and Ho, Daniel E. and Hong, Jenny and Hsu, Kyle and Huang, Jing and Icard, Thomas and Jain, Saahil and Jurafsky, Dan and Kalluri, Pratyusha and Karamcheti, Siddharth and Keeling, Geoff and Khani, Fereshte and Khattab, Omar and Koh, Pang Wei and Krass, Mark and Krishna, Ranjay and Kuditipudi, Rohith and Kumar, Ananya and Ladhak, Faisal and Lee, Mina and Lee, Tony and Leskovec, Jure and Levent, Isabelle and Li, Xiang Lisa and Li, Xuechen and Ma, Tengyu and Malik, Ali and Manning, Christopher D. and Mirchandani, Suvir and Mitchell, Eric and Munyikwa, Zanele and Nair, Suraj and Narayan, Avanika and Narayanan, Deepak and Newman, Ben and Nie, Allen and Niebles, Juan Carlos and Nilforoshan, Hamed and Nyarko, Julian and Ogut, Giray and Orr, Laurel and Papadimitriou, Isabel and Park, Joon Sung and Piech, Chris and Portelance, Eva and Potts, Christopher and Raghunathan, Aditi and Reich, Rob and Ren, Hongyu and Rong, Frieda and Roohani, Yusuf and Ruiz, Camilo and Ryan, Jack and Ré, Christopher and Sadigh, Dorsa and Sagawa, Shiori and Santhanam, Keshav and Shih, Andy and Srinivasan, Krishnan and Tamkin, Alex and Taori, Rohan and Thomas, Armin W. and Tramèr, Florian and Wang, Rose E. and Wang, William and Wu, Bohan and Wu, Jiajun and Wu, Yuhuai and Xie, Sang Michael and Yasunaga, Michihiro and You, Jiaxuan and Zaharia, Matei and Zhang, Michael and Zhang, Tianyi and Zhang, Xikun and Zhang, Yuhui and Zheng, Lucia and Zhou, Kaitlyn and Liang, Percy},
	month = aug,
	year = {2021},
	note = {arXiv: 2108.07258},
	keywords = {LM, NLP, transfer learning, AI ethics},
	file = {arXiv Fulltext PDF:C\:\\Users\\karmeni1\\Zotero\\storage\\4I9WNBBQ\\Bommasani et al. - 2021 - On the Opportunities and Risks of Foundation Model.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\karmeni1\\Zotero\\storage\\JZJYLDIP\\2108.html:text/html},
}

@inproceedings{misra_exploring_2020,
	address = {Online},
	title = {Exploring {BERT}'s sensitivity to lexical cues using tests from semantic priming},
	url = {https://aclanthology.org/2020.findings-emnlp.415},
	doi = {10.18653/v1/2020.findings-emnlp.415},
	abstract = {Models trained to estimate word probabilities in context have become ubiquitous in natural language processing. How do these models use lexical cues in context to inform their word probabilities? To answer this question, we present a case study analyzing the pre-trained BERT model with tests informed by semantic priming. Using English lexical stimuli that show priming in humans, we find that BERT too shows “priming”, predicting a word with greater probability when the context includes a related word versus an unrelated one. This effect decreases as the amount of information provided by the context increases. Follow-up analysis shows BERT to be increasingly distracted by related prime words as context becomes more informative, assigning lower probabilities to related words. Our findings highlight the importance of considering contextual constraint effects when studying word prediction in these models, and highlight possible parallels with human processing.},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2020},
	publisher = {Association for Computational Linguistics},
	author = {Misra, Kanishka and Ettinger, Allyson and Rayz, Julia},
	month = nov,
	year = {2020},
	keywords = {LM, NLP, BERT, N400, semantic priming, priming},
	pages = {4625--4635},
}

@techreport{yang_next-generation_2021,
	title = {Next-generation of recurrent neural network models for cognition},
	url = {https://psyarxiv.com/w34n2/},
	abstract = {Recurrent Neural Networks (RNNs) trained with machine learning techniques on cognitive tasks have become a widely accepted tool for neuroscientists. In this short opinion piece, we discuss fundamental challenges faced by early work of this approach, and recent steps to overcome such challenges and build next-generation RNN models for cognition. We propose several essential questions that practitioners of this approach should address to continue building future generations of RNN models.},
	language = {en-us},
	urldate = {2021-10-19},
	institution = {PsyArXiv},
	author = {Yang, Guangyu Robert and Mazon, Manuel Molano},
	month = apr,
	year = {2021},
	doi = {10.31234/osf.io/w34n2},
	note = {type: article},
	keywords = {hierarchy, evolution, biological plausibility, RNN, learning, RNN interpretability, BPTT},
}

@article{ferreira_good_2007,
	title = {The ‘{Good} {Enough}’ approach to language comprehension},
	volume = {1},
	issn = {1749818X},
	shorttitle = {The ‘{Good} {Enough}’ {Approach} to {Language} {Comprehension}},
	url = {https://onlinelibrary.wiley.com/doi/10.1111/j.1749-818X.2007.00007.x},
	doi = {10.1111/j.1749-818X.2007.00007.x},
	language = {en},
	number = {1-2},
	urldate = {2021-11-18},
	journal = {Language and Linguistics Compass},
	author = {Ferreira, Fernanda and Patson, Nikole D.},
	month = mar,
	year = {2007},
	pages = {71--83},
}

@article{whittaker_steep_2021,
	title = {The steep cost of capture},
	volume = {28},
	issn = {1072-5520},
	url = {https://doi.org/10.1145/3488666},
	doi = {10.1145/3488666},
	number = {6},
	urldate = {2021-11-24},
	journal = {Interactions},
	author = {Whittaker, Meredith},
	month = nov,
	year = {2021},
	keywords = {GPT-3, NLP, capitalism, AI ethics, big tech},
	pages = {50--55},
}

@article{raji_ai_2021,
	title = {{AI} and the everything in the whole wide world benchmark},
	url = {http://arxiv.org/abs/2111.15366},
	abstract = {There is a tendency across different subfields in AI to valorize a small collection of influential benchmarks. These benchmarks operate as stand-ins for a range of anointed common problems that are frequently framed as foundational milestones on the path towards flexible and generalizable AI systems. State-of-the-art performance on these benchmarks is widely understood as indicative of progress towards these long-term goals. In this position paper, we explore the limits of such benchmarks in order to reveal the construct validity issues in their framing as the functionally "general" broad measures of progress they are set up to be.},
	urldate = {2021-12-08},
	journal = {arXiv:2111.15366 [cs]},
	author = {Raji, Inioluwa Deborah and Bender, Emily M. and Paullada, Amandalynne and Denton, Emily and Hanna, Alex},
	month = nov,
	year = {2021},
	note = {arXiv: 2111.15366},
	keywords = {benchmark, GLUE, language model, image recognition, imageNet, AI ethics},
	file = {arXiv Fulltext PDF:C\:\\Users\\karmeni1\\Zotero\\storage\\BXHRDADZ\\Raji et al. - 2021 - AI and the Everything in the Whole Wide World Benc.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\karmeni1\\Zotero\\storage\\XXYQTJTA\\2111.html:text/html},
}

@inproceedings{futrell_neural_2019,
	address = {Minneapolis, Minnesota},
	title = {Neural language models as psycholinguistic subjects: {Representations} of syntactic state},
	shorttitle = {Neural language models as psycholinguistic subjects},
	url = {https://aclanthology.org/N19-1004},
	doi = {10.18653/v1/N19-1004},
	abstract = {We investigate the extent to which the behavior of neural network language models reflects incremental representations of syntactic state. To do so, we employ experimental methodologies which were originally developed in the field of psycholinguistics to study syntactic representation in the human mind. We examine neural network model behavior on sets of artificial sentences containing a variety of syntactically complex structures. These sentences not only test whether the networks have a representation of syntactic state, they also reveal the specific lexical cues that networks use to update these states. We test four models: two publicly available LSTM sequence models of English (Jozefowicz et al., 2016; Gulordava et al., 2018) trained on large datasets; an RNN Grammar (Dyer et al., 2016) trained on a small, parsed dataset; and an LSTM trained on the same small corpus as the RNNG. We find evidence for basic syntactic state representations in all models, but only the models trained on large datasets are sensitive to subtle lexical cues signaling changes in syntactic state.},
	urldate = {2022-01-19},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Futrell, Richard and Wilcox, Ethan and Morita, Takashi and Qian, Peng and Ballesteros, Miguel and Levy, Roger},
	month = jun,
	year = {2019},
	pages = {32--42},
}

@article{tay_efficient_2020,
	title = {Efficient transformers: {A} survey},
	shorttitle = {Efficient {Transformers}},
	url = {http://arxiv.org/abs/2009.06732},
	abstract = {Transformer model architectures have garnered immense interest lately due to their effectiveness across a range of domains like language, vision and reinforcement learning. In the field of natural language processing for example, Transformers have become an indispensable staple in the modern deep learning stack. Recently, a dizzying number of "X-former" models have been proposed - Reformer, Linformer, Performer, Longformer, to name a few - which improve upon the original Transformer architecture, many of which make improvements around computational and memory efficiency. With the aim of helping the avid researcher navigate this flurry, this paper characterizes a large and thoughtful selection of recent efficiency-flavored "X-former" models, providing an organized and comprehensive overview of existing work and models across multiple domains.},
	urldate = {2022-01-19},
	journal = {arXiv:2009.06732 [cs]},
	author = {Tay, Yi and Dehghani, Mostafa and Bahri, Dara and Metzler, Donald},
	month = sep,
	year = {2020},
	note = {arXiv: 2009.06732
version: 2},
	file = {arXiv Fulltext PDF:C\:\\Users\\karmeni1\\Zotero\\storage\\AA49WYHI\\Tay et al. - 2020 - Efficient Transformers A Survey.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\karmeni1\\Zotero\\storage\\S5JAYTZB\\2009.html:text/html},
}

@inproceedings{devlin_bert_2019,
	address = {Minneapolis, Minnesota},
	title = {{BERT}: {Pre}-training of deep bidirectional transformers for language understanding},
	shorttitle = {{BERT}},
	url = {https://aclanthology.org/N19-1423},
	doi = {10.18653/v1/N19-1423},
	abstract = {We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7\% (4.6\% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement).},
	urldate = {2022-02-01},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
	month = jun,
	year = {2019},
	pages = {4171--4186},
}

@inproceedings{grefenstette_learning_2015,
	address = {Cambridge, MA, USA},
	series = {{NIPS}'15},
	title = {Learning to transduce with unbounded memory},
	abstract = {Recently, strong results have been demonstrated by Deep Recurrent Neural Networks on natural language transduction problems. In this paper we explore the representational power of these models using synthetic grammars designed to exhibit phenomena similar to those found in real transduction problems such as machine translation. These experiments lead us to propose new memory-based recurrent networks that implement continuously differentiable analogues of traditional data structures such as Stacks, Queues, and DeQues. We show that these architectures exhibit superior generalisation performance to Deep RNNs and are often able to learn the underlying generating algorithms in our transduction experiments.},
	urldate = {2022-02-01},
	booktitle = {Proceedings of the 28th {International} {Conference} on {Neural} {Information} {Processing} {Systems} - {Volume} 2},
	publisher = {MIT Press},
	author = {Grefenstette, Edward and Hermann, Karl Moritz and Suleyman, Mustafa and Blunsom, Phil},
	month = dec,
	year = {2015},
	pages = {1828--1836},
}

@inproceedings{gu_incorporating_2016,
	address = {Berlin, Germany},
	title = {Incorporating copying mechanism in sequence-to-sequence learning},
	url = {https://aclanthology.org/P16-1154},
	doi = {10.18653/v1/P16-1154},
	urldate = {2022-02-01},
	booktitle = {Proceedings of the 54th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Gu, Jiatao and Lu, Zhengdong and Li, Hang and Li, Victor O.K.},
	month = aug,
	year = {2016},
	pages = {1631--1640},
}

@inproceedings{gulordava_colorless_2018,
	address = {New Orleans, Louisiana},
	title = {Colorless green recurrent networks dream hierarchically},
	url = {https://aclanthology.org/N18-1108},
	doi = {10.18653/v1/N18-1108},
	abstract = {Recurrent neural networks (RNNs) achieved impressive results in a variety of linguistic processing tasks, suggesting that they can induce non-trivial properties of language. We investigate to what extent RNNs learn to track abstract hierarchical syntactic structure. We test whether RNNs trained with a generic language modeling objective in four languages (Italian, English, Hebrew, Russian) can predict long-distance number agreement in various constructions. We include in our evaluation nonsensical sentences where RNNs cannot rely on semantic or lexical cues (“The colorless green ideas I ate with the chair sleep furiously”), and, for Italian, we compare model performance to human intuitions. Our language-model-trained RNNs make reliable predictions about long-distance agreement, and do not lag much behind human performance. We thus bring support to the hypothesis that RNNs are not just shallow-pattern extractors, but they also acquire deeper grammatical competence.},
	urldate = {2022-02-01},
	booktitle = {Proceedings of the 2018 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Gulordava, Kristina and Bojanowski, Piotr and Grave, Edouard and Linzen, Tal and Baroni, Marco},
	month = jun,
	year = {2018},
	pages = {1195--1205},
}

@inproceedings{holtzman_curious_2020,
	title = {The curious case of neural text degeneration},
	url = {https://iclr.cc/virtual_2020/poster_rygGQyrFvH.html},
	abstract = {Despite considerable advances in neural language modeling, it remains an open question what the best decoding strategy is for text generation from a language model (e.g. to generate a story). The counter-intuitive empirical observation is that even though the use of likelihood as training objective leads to high quality models for a broad range of language understanding tasks, maximization-based decoding methods such as beam search lead to degeneration — output text that is bland, incoherent, or gets stuck in repetitive loops. To address this we propose Nucleus Sampling, a simple but effective method to draw considerably higher quality text out of neural language models than previous decoding strategies. Our approach avoids text degeneration by truncating the unreliable tail of the probability distribution, sampling from the dynamic nucleus of tokens containing the vast majority of the probability mass. To properly examine current maximization-based and stochastic decoding methods, we compare generations from each of these methods to the distribution of human text along several axes such as likelihood, diversity, and repetition. Our results show that (1) maximization is an inappropriate decoding objective for open-ended text generation, (2) the probability distributions of the best current language models have an unreliable tail which needs to be truncated during generation and (3) Nucleus Sampling is currently the best available decoding strategy for generating long-form text that is both high-quality — as measured by human evaluation — and as diverse as human-written text.},
	language = {en},
	urldate = {2022-02-01},
	author = {Holtzman, Ari and Buys, Jan and Du, Li and Forbes, Maxwell and Choi, Yejin},
	month = apr,
	year = {2020},
}

@inproceedings{khandelwal_sharp_2018,
	address = {Melbourne, Australia},
	title = {Sharp nearby, fuzzy far away: {How} neural language models use context},
	shorttitle = {Sharp {Nearby}, {Fuzzy} {Far} {Away}},
	url = {https://aclanthology.org/P18-1027},
	doi = {10.18653/v1/P18-1027},
	abstract = {We know very little about how neural language models (LM) use prior linguistic context. In this paper, we investigate the role of context in an LSTM LM, through ablation studies. Specifically, we analyze the increase in perplexity when prior context words are shuffled, replaced, or dropped. On two standard datasets, Penn Treebank and WikiText-2, we find that the model is capable of using about 200 tokens of context on average, but sharply distinguishes nearby context (recent 50 tokens) from the distant history. The model is highly sensitive to the order of words within the most recent sentence, but ignores word order in the long-range context (beyond 50 tokens), suggesting the distant past is modeled only as a rough semantic field or topic. We further find that the neural caching model (Grave et al., 2017b) especially helps the LSTM to copy words from within this distant context. Overall, our analysis not only provides a better understanding of how neural LMs use their context, but also sheds light on recent success from cache-based models.},
	urldate = {2022-02-01},
	booktitle = {Proceedings of the 56th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Khandelwal, Urvashi and He, He and Qi, Peng and Jurafsky, Dan},
	month = jul,
	year = {2018},
	pages = {284--294},
}

@article{lakretz_mechanisms_2021,
	title = {Mechanisms for handling nested dependencies in neural-network language models and humans},
	volume = {213},
	issn = {00100277},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0010027721001189},
	doi = {10.1016/j.cognition.2021.104699},
	language = {en},
	urldate = {2022-02-01},
	journal = {Cognition},
	author = {Lakretz, Yair and Hupkes, Dieuwke and Vergallito, Alessandra and Marelli, Marco and Baroni, Marco and Dehaene, Stanislas},
	month = aug,
	year = {2021},
	pages = {104699},
}

@inproceedings{lakretz_emergence_2019,
	address = {Minneapolis, Minnesota},
	title = {The emergence of number and syntax units in {LSTM} language models},
	url = {https://aclanthology.org/N19-1002},
	doi = {10.18653/v1/N19-1002},
	abstract = {Recent work has shown that LSTMs trained on a generic language modeling objective capture syntax-sensitive generalizations such as long-distance number agreement. We have however no mechanistic understanding of how they accomplish this remarkable feat. Some have conjectured it depends on heuristics that do not truly take hierarchical structure into account. We present here a detailed study of the inner mechanics of number tracking in LSTMs at the single neuron level. We discover that long-distance number information is largely managed by two “number units”. Importantly, the behaviour of these units is partially controlled by other units independently shown to track syntactic structure. We conclude that LSTMs are, to some extent, implementing genuinely syntactic processing mechanisms, paving the way to a more general understanding of grammatical encoding in LSTMs.},
	urldate = {2022-02-01},
	booktitle = {Proceedings of the 2019 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}, {Volume} 1 ({Long} and {Short} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {Lakretz, Yair and Kruszewski, German and Desbordes, Theo and Hupkes, Dieuwke and Dehaene, Stanislas and Baroni, Marco},
	month = jun,
	year = {2019},
	pages = {11--20},
}

@article{linzen_syntactic_2021,
	title = {Syntactic structure from deep learning},
	volume = {7},
	url = {https://doi.org/10.1146/annurev-linguistics-032020-051035},
	doi = {10.1146/annurev-linguistics-032020-051035},
	abstract = {Modern deep neural networks achieve impressive performance in engineering applications that require extensive linguistic skills, such as machine translation. This success has sparked interest in probing whether these models are inducing human-like grammatical knowledge from the raw data they are exposed to and, consequently, whether they can shed new light on long-standing debates concerning the innate structure necessary for language acquisition. In this article, we survey representative studies of the syntactic abilities of deep networks and discuss the broader implications that this work has for theoretical linguistics.},
	number = {1},
	urldate = {2022-02-01},
	journal = {Annual Review of Linguistics},
	author = {Linzen, Tal and Baroni, Marco},
	year = {2021},
	note = {\_eprint: https://doi.org/10.1146/annurev-linguistics-032020-051035},
	keywords = {deep learning, syntax, nature versus nurture, probing linguistic knowledge},
	pages = {195--212},
}

@inproceedings{marvin_targeted_2018,
	address = {Brussels, Belgium},
	title = {Targeted syntactic evaluation of language models},
	url = {https://aclanthology.org/D18-1151},
	doi = {10.18653/v1/D18-1151},
	abstract = {We present a data set for evaluating the grammaticality of the predictions of a language model. We automatically construct a large number of minimally different pairs of English sentences, each consisting of a grammatical and an ungrammatical sentence. The sentence pairs represent different variations of structure-sensitive phenomena: subject-verb agreement, reflexive anaphora and negative polarity items. We expect a language model to assign a higher probability to the grammatical sentence than the ungrammatical one. In an experiment using this data set, an LSTM language model performed poorly on many of the constructions. Multi-task training with a syntactic objective (CCG supertagging) improved the LSTM's accuracy, but a large gap remained between its performance and the accuracy of human participants recruited online. This suggests that there is considerable room for improvement over LSTMs in capturing syntax in a language model.},
	urldate = {2022-02-01},
	booktitle = {Proceedings of the 2018 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Marvin, Rebecca and Linzen, Tal},
	month = oct,
	year = {2018},
	pages = {1192--1202},
}

@inproceedings{merity_regularizing_2018,
	title = {Regularizing and optimizing {LSTM} language models},
	url = {https://openreview.net/forum?id=SyyGPP0TZ},
	abstract = {Effective regularization and optimization strategies for LSTM-based language models achieves SOTA on PTB and WT2.},
	language = {en},
	urldate = {2022-02-01},
	author = {Merity, Stephen and Keskar, Nitish Shirish and Socher, Richard},
	month = feb,
	year = {2018},
}

@article{merity_pointer_2016,
	title = {Pointer sentinel mixture models},
	url = {https://openreview.net/forum?id=Byj72udxe},
	abstract = {Pointer sentinel mixture models provide a method to combine a traditional vocabulary softmax with a pointer network, providing state of the art results in language modeling on PTB and the newly...},
	language = {en},
	urldate = {2022-02-01},
	author = {Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
	month = nov,
	year = {2016},
}

@inproceedings{oconnor_what_2021,
	address = {Online},
	title = {What context features can transformer language models use?},
	url = {https://aclanthology.org/2021.acl-long.70},
	doi = {10.18653/v1/2021.acl-long.70},
	abstract = {Transformer-based language models benefit from conditioning on contexts of hundreds to thousands of previous tokens. What aspects of these contexts contribute to accurate model prediction? We describe a series of experiments that measure usable information by selectively ablating lexical and structural information in transformer language models trained on English Wikipedia. In both mid- and long-range contexts, we find that several extremely destructive context manipulations—including shuffling word order within sentences and deleting all words other than nouns—remove less than 15\% of the usable information. Our results suggest that long contexts, but not their detailed syntactic and propositional content, are important for the low perplexity of current transformer language models.},
	urldate = {2022-02-01},
	booktitle = {Proceedings of the 59th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics} and the 11th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({Volume} 1: {Long} {Papers})},
	publisher = {Association for Computational Linguistics},
	author = {O'Connor, Joe and Andreas, Jacob},
	month = aug,
	year = {2021},
	pages = {851--864},
}

@inproceedings{ritter_cognitive_2017,
	title = {Cognitive psychology for deep neural networks: {A} shape bias case study},
	shorttitle = {Cognitive {Psychology} for {Deep} {Neural} {Networks}},
	url = {https://proceedings.mlr.press/v70/ritter17a.html},
	abstract = {Deep neural networks (DNNs) have advanced performance on a wide range of complex tasks, rapidly outpacing our understanding of the nature of their solutions. While past work sought to advance our understanding of these models, none has made use of the rich history of problem descriptions, theories, and experimental methods developed by cognitive psychologists to study the human mind. To explore the potential value of these tools, we chose a well-established analysis from developmental psychology that explains how children learn word labels for objects, and applied that analysis to DNNs. Using datasets of stimuli inspired by the original cognitive psychology experiments, we find that state-of-the-art one shot learning models trained on ImageNet exhibit a similar bias to that observed in humans: they prefer to categorize objects according to shape rather than color. The magnitude of this shape bias varies greatly among architecturally identical, but differently seeded models, and even fluctuates within seeds throughout training, despite nearly equivalent classification performance. These results demonstrate the capability of tools from cognitive psychology for exposing hidden computational properties of DNNs, while concurrently providing us with a computational model for human word learning.},
	language = {en},
	urldate = {2022-02-01},
	booktitle = {Proceedings of the 34th {International} {Conference} on {Machine} {Learning}},
	publisher = {PMLR},
	author = {Ritter, Samuel and Barrett, David G. T. and Santoro, Adam and Botvinick, Matt M.},
	month = jul,
	year = {2017},
	note = {ISSN: 2640-3498},
	pages = {2940--2949},
}

@inproceedings{van_schijndel_quantity_2019,
	address = {Hong Kong, China},
	title = {Quantity doesn't buy quality syntax with neural language models},
	url = {https://aclanthology.org/D19-1592},
	doi = {10.18653/v1/D19-1592},
	abstract = {Recurrent neural networks can learn to predict upcoming words remarkably well on average; in syntactically complex contexts, however, they often assign unexpectedly high probabilities to ungrammatical words. We investigate to what extent these shortcomings can be mitigated by increasing the size of the network and the corpus on which it is trained. We find that gains from increasing network size are minimal beyond a certain point. Likewise, expanding the training corpus yields diminishing returns; we estimate that the training corpus would need to be unrealistically large for the models to match human performance. A comparison to GPT and BERT, Transformer-based models trained on billions of words, reveals that these models perform even more poorly than our LSTMs in some constructions. Our results make the case for more data efficient architectures.},
	urldate = {2022-02-01},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {van Schijndel, Marten and Mueller, Aaron and Linzen, Tal},
	month = nov,
	year = {2019},
	pages = {5831--5837},
}

@inproceedings{vaswani_attention_2017,
	address = {Red Hook, NY, USA},
	series = {{NIPS}'17},
	title = {Attention is all you need},
	isbn = {978-1-5108-6096-4},
	abstract = {The dominant sequence transduction models are based on complex recurrent or convolutional neural networks that include an encoder and a decoder. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature.},
	urldate = {2022-02-01},
	booktitle = {Proceedings of the 31st {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Łukasz and Polosukhin, Illia},
	month = dec,
	year = {2017},
	pages = {6000--6010},
}

@inproceedings{weston_memory_2015,
	address = {San Diego},
	title = {Memory networks},
	abstract = {We describe a new class of learning models called memory networks. Memory networks reason with inference components combined with a long-term memory component; they learn how to use these jointly. The long-term memory can be read and written to, with the goal of using it for prediction. We investigate these models in the context of question answering (QA) where the long-term memory effectively acts as a (dynamic) knowledge base, and the output is a textual response. We evaluate them on a large-scale QA task, and a smaller, but more complex, toy task generated from a simulated world. In the latter, we show the reasoning power of such models by chaining multiple supporting sentences to answer questions that require understanding the intension of verbs.},
	language = {English (US)},
	booktitle = {3rd {International} {Conference} on {Learning} {Representations}, {ICLR} 2015},
	publisher = {3rd International Conference on Learning Representations},
	author = {Weston, Jason and Chopra, Sumit and Bordes, Antoine},
	year = {2015},
}

@inproceedings{brown_language_2020,
	title = {Language models are few-shot learners},
	volume = {33},
	url = {https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf},
	booktitle = {Advances in {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates, Inc.},
	author = {Brown, Tom and Mann, Benjamin and Ryder, Nick and Subbiah, Melanie and Kaplan, Jared D and Dhariwal, Prafulla and Neelakantan, Arvind and Shyam, Pranav and Sastry, Girish and Askell, Amanda and Agarwal, Sandhini and Herbert-Voss, Ariel and Krueger, Gretchen and Henighan, Tom and Child, Rewon and Ramesh, Aditya and Ziegler, Daniel and Wu, Jeffrey and Winter, Clemens and Hesse, Chris and Chen, Mark and Sigler, Eric and Litwin, Mateusz and Gray, Scott and Chess, Benjamin and Clark, Jack and Berner, Christopher and McCandlish, Sam and Radford, Alec and Sutskever, Ilya and Amodei, Dario},
	editor = {Larochelle, H. and Ranzato, M. and Hadsell, R. and Balcan, M. F. and Lin, H.},
	year = {2020},
	pages = {1877--1901},
}

@inproceedings{ba_using_2016,
	address = {Red Hook, NY, USA},
	series = {{NIPS}'16},
	title = {Using fast weights to attend to the recent past},
	isbn = {978-1-5108-3881-9},
	abstract = {Until recently, research on artificial neural networks was largely restricted to systems with only two types of variable: Neural activities that represent the current or recent input and weights that learn to capture regularities among inputs, outputs and payoffs. There is no good reason for this restriction. Synapses have dynamics at many different time-scales and this suggests that artificial neural networks might benefit from variables that change slower than activities but much faster than the standard weights. These "fast weights" can be used to store temporary memories of the recent past and they provide a neurally plausible way of implementing the type of attention to the past that has recently proved very helpful in sequence-to-sequence models. By using fast weights we can avoid the need to store copies of neural activity patterns.},
	urldate = {2022-02-01},
	booktitle = {Proceedings of the 30th {International} {Conference} on {Neural} {Information} {Processing} {Systems}},
	publisher = {Curran Associates Inc.},
	author = {Ba, Jimmy and Hinton, Geoffrey and Mnih, Volodymyr and Leibo, Joel Z. and Ionescu, Catalin},
	month = dec,
	year = {2016},
	pages = {4338--4346},
}

@article{linzen_assessing_2016,
	title = {Assessing the ability of {LSTMs} to learn {Syntax}-sensitive dependencies},
	volume = {4},
	issn = {2307-387X},
	url = {https://doi.org/10.1162/tacl_a_00115},
	doi = {10.1162/tacl_a_00115},
	abstract = {The success of long short-term memory (LSTM) neural networks in language
processing is typically attributed to their ability to capture long-distance
statistical regularities. Linguistic regularities are often sensitive to
syntactic structure; can such dependencies be captured by LSTMs, which do not
have explicit structural representations? We begin addressing this question
using number agreement in English subject-verb dependencies. We probe the
architecture’s grammatical competence both using training objectives with an
explicit grammatical target (number prediction, grammaticality judgments) and
using language models. In the strongly supervised settings, the LSTM achieved
very high overall accuracy (less than 1\% errors), but errors increased when
sequential and structural information conflicted. The frequency of such errors
rose sharply in the language-modeling setting. We conclude that LSTMs can
capture a non-trivial amount of grammatical structure given targeted
supervision, but stronger architectures may be required to further reduce
errors; furthermore, the language modeling signal is insufficient for capturing
syntax-sensitive dependencies, and should be supplemented with more direct
supervision if such dependencies need to be captured.},
	urldate = {2022-02-01},
	journal = {Transactions of the Association for Computational Linguistics},
	author = {Linzen, Tal and Dupoux, Emmanuel and Goldberg, Yoav},
	month = dec,
	year = {2016},
	pages = {521--535},
}

@article{ravishankar_word_2022,
	title = {Word {Order} {Does} {Matter} ({And} {Shuffled} {Language} {Models} {Know} {It})},
	url = {http://arxiv.org/abs/2203.10995},
	abstract = {Recent studies have shown that language models pretrained and/or fine-tuned on randomly permuted sentences exhibit competitive performance on GLUE, putting into question the importance of word order information. Somewhat counter-intuitively, some of these studies also report that position embeddings appear to be crucial for models' good performance with shuffled text. We probe these language models for word order information and investigate what position embeddings learned from shuffled text encode, showing that these models retain information pertaining to the original, naturalistic word order. We show this is in part due to a subtlety in how shuffling is implemented in previous work -- before rather than after subword segmentation. Surprisingly, we find even Language models trained on text shuffled after subword segmentation retain some semblance of information about word order because of the statistical dependencies between sentence length and unigram probabilities. Finally, we show that beyond GLUE, a variety of language understanding tasks do require word order information, often to an extent that cannot be learned through fine-tuning.},
	urldate = {2022-03-23},
	journal = {arXiv:2203.10995 [cs]},
	author = {Ravishankar, Vinit and Abdou, Mostafa and Kulmizev, Artur and Søgaard, Anders},
	month = mar,
	year = {2022},
	note = {arXiv: 2203.10995},
}

@inproceedings{mikolov_distributed_2013,
	address = {Red Hook, NY, USA},
	series = {{NIPS}'13},
	title = {Distributed representations of words and phrases and their compositionality},
	url = {https://proceedings.neurips.cc/paper/2013/file/9aa42b31882ec039965f3c4923ce901b-Paper.pdf},
	abstract = {The recently introduced continuous Skip-gram model is an efficient method for learning high-quality distributed vector representations that capture a large number of precise syntactic and semantic word relationships. In this paper we present several extensions that improve both the quality of the vectors and the training speed. By subsampling of the frequent words we obtain significant speedup and also learn more regular word representations. We also describe a simple alternative to the hierarchical softmax called negative sampling.An inherent limitation of word representations is their indifference to word order and their inability to represent idiomatic phrases. For example, the meanings of "Canada" and "Air" cannot be easily combined to obtain "Air Canada". Motivated by this example, we present a simple method for finding phrases in text, and show that learning good vector representations for millions of phrases is possible.},
	booktitle = {Proceedings of the 26th {International} {Conference} on {Neural} {Information} {Processing} {Systems} - {Volume} 2},
	publisher = {Curran Associates Inc.},
	author = {Mikolov, Tomas and Sutskever, Ilya and Chen, Kai and Corrado, Greg and Dean, Jeffrey},
	year = {2013},
	note = {event-place: Lake Tahoe, Nevada},
	pages = {3111--3119},
}

@inproceedings{caucheteux_model-based_2021,
	address = {Punta Cana, Dominican Republic},
	title = {Model-based analysis of brain activity reveals the hierarchy of language in 305 subjects},
	url = {https://aclanthology.org/2021.findings-emnlp.308},
	doi = {10.18653/v1/2021.findings-emnlp.308},
	abstract = {A popular approach to decompose the neural bases of language consists in correlating, across individuals, the brain responses to different stimuli (e.g. regular speech versus scrambled words, sentences, or paragraphs). Although successful, this `model-free' approach necessitates the acquisition of a large and costly set of neuroimaging data. Here, we show that a model-based approach can reach equivalent results within subjects exposed to natural stimuli. We capitalize on the recently-discovered similarities between deep language models and the human brain to compute the mapping between i) the brain responses to regular speech and ii) the activations of deep language models elicited by modified stimuli (e.g. scrambled words, sentences, or paragraphs). Our model-based approach successfully replicates the seminal study of Lerner et al. (2011), which revealed the hierarchy of language areas by comparing the functional-magnetic resonance imaging (fMRI) of seven subjects listening to 7min of both regular and scrambled narratives. We further extend and precise these results to the brain signals of 305 individuals listening to 4.1 hours of narrated stories. Overall, this study paves the way for efficient and flexible analyses of the brain bases of language.},
	urldate = {2022-05-20},
	booktitle = {Findings of the {Association} for {Computational} {Linguistics}: {EMNLP} 2021},
	publisher = {Association for Computational Linguistics},
	author = {Caucheteux, Charlotte and Gramfort, Alexandre and King, Jean-Remi},
	month = nov,
	year = {2021},
	pages = {3635--3644},
}

@misc{binz_using_2022,
	title = {Using cognitive psychology to understand {GPT}-3},
	url = {https://psyarxiv.com/6dfgk/},
	doi = {10.31234/osf.io/6dfgk},
	abstract = {We study GPT-3, a recent large language model, using tools from cognitive psychology. More specifically, we assess GPT-3's decision-making, information search, deliberation, and causal reasoning abilities on a battery of canonical experiments from the literature. We find that much of GPT-3's behavior is impressive: it solves vignette-based tasks similarly or better than human subjects, is able to make decent decisions from descriptions, outperforms humans in a multi-armed bandit task, and shows signatures of model-based reinforcement learning. Yet we also find that small perturbations to vignette-based tasks can lead GPT-3 vastly astray, that it shows no signatures of directed exploration, and that it fails miserably in a causal reasoning task. These results enrich our understanding of current large language models and pave the way for future investigations using tools from cognitive psychology to study increasingly capable and opaque artificial agents.},
	language = {en-us},
	urldate = {2022-06-23},
	publisher = {PsyArXiv},
	author = {Binz, Marcel and Schulz, Eric},
	month = jun,
	year = {2022},
	keywords = {decision-making, cognitive psychology, Language, and Heuristics, Biases, causal reasoning, Cognitive Psychology, deliberation, Framing, information search, Judgment and Decision Making, language models, Learning, Reasoning, Social and Behavioral Sciences},
}

@article{olsson_-context_2022,
	title = {In-context learning and induction heads},
	journal = {Transformer Circuits Thread},
	author = {Olsson, Catherine and Elhage, Nelson and Nanda, Neel and Joseph, Nicholas and DasSarma, Nova and Henighan, Tom and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Johnston, Scott and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
	year = {2022},
}

@article{raffel_exploring_2020,
	title = {Exploring the limits of transfer learning with a unified text-to-text transformer},
	volume = {21},
	issn = {1532-4435},
	abstract = {Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts all text-based language problems into a text-to-text format. Our systematic study compares pretraining objectives, architectures, unlabeled data sets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new "Colossal Clean Crawled Corpus", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our data set, pre-trained models, and code.},
	number = {1},
	journal = {J. Mach. Learn. Res.},
	author = {Raffel, Colin and Shazeer, Noam and Roberts, Adam and Lee, Katherine and Narang, Sharan and Matena, Michael and Zhou, Yanqi and Li, Wei and Liu, Peter J.},
	month = jan,
	year = {2020},
	note = {Publisher: JMLR.org},
	keywords = {deep learning, natural language processing, multi-task learning, transfer learning, attention based models},
}

@inproceedings{armeni_characterizing_2022,
	address = {Abu Dhabi, United Arab Emirates (Hybrid)},
	title = {Characterizing verbatim short-term memory in neural language models},
	url = {https://aclanthology.org/2022.conll-1.28},
	abstract = {When a language model is trained to predict natural language sequences, its prediction at each moment depends on a representation of prior context. What kind of information about the prior context can language models retrieve? We tested whether language models could retrieve the exact words that occurred previously in a text. In our paradigm, language models (transformers and an LSTM) processed English text in which a list of nouns occurred twice. We operationalized retrieval as the reduction in surprisal from the first to the second list. We found that the transformers retrieved both the identity and ordering of nouns from the first list. Further, the transformers' retrieval was markedly enhanced when they were trained on a larger corpus and with greater model depth. Lastly, their ability to index prior tokens was dependent on learned attention patterns. In contrast, the LSTM exhibited less precise retrieval, which was limited to list-initial tokens and to short intervening texts. The LSTM's retrieval was not sensitive to the order of nouns and it improved when the list was semantically coherent. We conclude that transformers implemented something akin to a working memory system that could flexibly retrieve individual token representations across arbitrary delays; conversely, the LSTM maintained a coarser and more rapidly-decaying semantic gist of prior tokens, weighted toward the earliest items.},
	booktitle = {Proceedings of the 26th {Conference} on {Computational} {Natural} {Language} {Learning} ({CoNLL})},
	publisher = {Association for Computational Linguistics},
	author = {Armeni, Kristijan and Honey, Christopher and Linzen, Tal},
	month = dec,
	year = {2022},
	pages = {405--424},
}

@inproceedings{wiegreffe_attention_2019,
	address = {Hong Kong, China},
	title = {Attention is not not {Explanation}},
	url = {https://aclanthology.org/D19-1002},
	doi = {10.18653/v1/D19-1002},
	abstract = {Attention mechanisms play a central role in NLP systems, especially within recurrent neural network (RNN) models. Recently, there has been increasing interest in whether or not the intermediate representations offered by these modules may be used to explain the reasoning for a model's prediction, and consequently reach insights regarding the model's decision-making process. A recent paper claims that `Attention is not Explanation' (Jain and Wallace, 2019). We challenge many of the assumptions underlying this work, arguing that such a claim depends on one's definition of explanation, and that testing it needs to take into account all elements of the model. We propose four alternative tests to determine when/whether attention can be used as explanation: a simple uniform-weights baseline; a variance calibration based on multiple random seed runs; a diagnostic framework using frozen weights from pretrained models; and an end-to-end adversarial attention training protocol. Each allows for meaningful interpretation of attention mechanisms in RNN models. We show that even when reliable adversarial distributions can be found, they don't perform well on the simple diagnostic, indicating that prior work does not disprove the usefulness of attention mechanisms for explainability.},
	urldate = {2023-05-02},
	booktitle = {Proceedings of the 2019 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing} and the 9th {International} {Joint} {Conference} on {Natural} {Language} {Processing} ({EMNLP}-{IJCNLP})},
	publisher = {Association for Computational Linguistics},
	author = {Wiegreffe, Sarah and Pinter, Yuval},
	month = nov,
	year = {2019},
	keywords = {attention, explanation, interpretability, LMs},
	pages = {11--20},
}

@article{elhage_mathematical_2021,
	title = {A mathematical framework for transformer circuits},
	journal = {Transformer Circuits Thread},
	author = {Elhage, Nelson and Nanda, Neel and Olsson, Catherine and Henighan, Tom and Joseph, Nicholas and Mann, Ben and Askell, Amanda and Bai, Yuntao and Chen, Anna and Conerly, Tom and DasSarma, Nova and Drain, Dawn and Ganguli, Deep and Hatfield-Dodds, Zac and Hernandez, Danny and Jones, Andy and Kernion, Jackson and Lovitt, Liane and Ndousse, Kamal and Amodei, Dario and Brown, Tom and Clark, Jack and Kaplan, Jared and McCandlish, Sam and Olah, Chris},
	year = {2021},
}

@inproceedings{vig_analyzing_2019,
	address = {Florence, Italy},
	title = {Analyzing the structure of attention in a transformer language model},
	url = {https://aclanthology.org/W19-4808},
	doi = {10.18653/v1/W19-4808},
	abstract = {The Transformer is a fully attention-based alternative to recurrent networks that has achieved state-of-the-art results across a range of NLP tasks. In this paper, we analyze the structure of attention in a Transformer language model, the GPT-2 small pretrained model. We visualize attention for individual instances and analyze the interaction between attention and syntax over a large corpus. We find that attention targets different parts of speech at different layer depths within the model, and that attention aligns with dependency relations most strongly in the middle layers. We also find that the deepest layers of the model capture the most distant relationships. Finally, we extract exemplar sentences that reveal highly specific patterns targeted by particular attention heads.},
	urldate = {2023-07-13},
	booktitle = {Proceedings of the 2019 {ACL} {Workshop} {BlackboxNLP}: {Analyzing} and {Interpreting} {Neural} {Networks} for {NLP}},
	publisher = {Association for Computational Linguistics},
	author = {Vig, Jesse and Belinkov, Yonatan},
	month = aug,
	year = {2019},
	pages = {63--76},
	file = {Full Text PDF:C\:\\Users\\karmeni1\\Zotero\\storage\\VXDY765P\\Vig and Belinkov - 2019 - Analyzing the Structure of Attention in a Transfor.pdf:application/pdf},
}

@misc{liu_pre-train_2021,
	title = {Pre-train, prompt, and predict: {A} systematic survey of prompting methods in natural language processing},
	shorttitle = {Pre-train, {Prompt}, and {Predict}},
	url = {http://arxiv.org/abs/2107.13586},
	doi = {10.48550/arXiv.2107.13586},
	abstract = {This paper surveys and organizes research works in a new paradigm in natural language processing, which we dub "prompt-based learning". Unlike traditional supervised learning, which trains a model to take in an input x and predict an output y as P(y{\textbar}x), prompt-based learning is based on language models that model the probability of text directly. To use these models to perform prediction tasks, the original input x is modified using a template into a textual string prompt x' that has some unfilled slots, and then the language model is used to probabilistically fill the unfilled information to obtain a final string x, from which the final output y can be derived. This framework is powerful and attractive for a number of reasons: it allows the language model to be pre-trained on massive amounts of raw text, and by defining a new prompting function the model is able to perform few-shot or even zero-shot learning, adapting to new scenarios with few or no labeled data. In this paper we introduce the basics of this promising paradigm, describe a unified set of mathematical notations that can cover a wide variety of existing work, and organize existing work along several dimensions, e.g.the choice of pre-trained models, prompts, and tuning strategies. To make the field more accessible to interested beginners, we not only make a systematic review of existing works and a highly structured typology of prompt-based concepts, but also release other resources, e.g., a website http://pretrain.nlpedia.ai/ including constantly-updated survey, and paperlist.},
	urldate = {2023-07-18},
	publisher = {arXiv},
	author = {Liu, Pengfei and Yuan, Weizhe and Fu, Jinlan and Jiang, Zhengbao and Hayashi, Hiroaki and Neubig, Graham},
	month = jul,
	year = {2021},
	note = {arXiv:2107.13586 [cs]},
}

@misc{von_oswald_transformers_2023,
	title = {Transformers learn in-context by gradient descent},
	url = {http://arxiv.org/abs/2212.07677},
	doi = {10.48550/arXiv.2212.07677},
	abstract = {At present, the mechanisms of in-context learning in Transformers are not well understood and remain mostly an intuition. In this paper, we suggest that training Transformers on auto-regressive objectives is closely related to gradient-based meta-learning formulations. We start by providing a simple weight construction that shows the equivalence of data transformations induced by 1) a single linear self-attention layer and by 2) gradient-descent (GD) on a regression loss. Motivated by that construction, we show empirically that when training self-attention-only Transformers on simple regression tasks either the models learned by GD and Transformers show great similarity or, remarkably, the weights found by optimization match the construction. Thus we show how trained Transformers become mesa-optimizers i.e. learn models by gradient descent in their forward pass. This allows us, at least in the domain of regression problems, to mechanistically understand the inner workings of in-context learning in optimized Transformers. Building on this insight, we furthermore identify how Transformers surpass the performance of plain gradient descent by learning an iterative curvature correction and learn linear models on deep data representations to solve non-linear regression tasks. Finally, we discuss intriguing parallels to a mechanism identified to be crucial for in-context learning termed induction-head (Olsson et al., 2022) and show how it could be understood as a specific case of in-context learning by gradient descent learning within Transformers. Code to reproduce the experiments can be found at https://github.com/google-research/self-organising-systems/tree/master/transformers\_learn\_icl\_by\_gd .},
	urldate = {2023-07-18},
	publisher = {arXiv},
	author = {von Oswald, Johannes and Niklasson, Eyvind and Randazzo, Ettore and Sacramento, João and Mordvintsev, Alexander and Zhmoginov, Andrey and Vladymyrov, Max},
	month = may,
	year = {2023},
	note = {arXiv:2212.07677 [cs]},
	keywords = {LM, gradient descient, in-context learning, induction head},
}

@misc{raccah_memory_2022,
	title = {Memory in humans and deep language models: {Linking} hypotheses for model augmentation},
	shorttitle = {Memory in humans and deep language models},
	url = {http://arxiv.org/abs/2210.01869},
	doi = {10.48550/arXiv.2210.01869},
	abstract = {The computational complexity of the self-attention mechanism in Transformer models significantly limits their ability to generalize over long temporal durations. Memory-augmentation, or the explicit storing of past information in external memory for subsequent predictions, has become a constructive avenue for mitigating this limitation. We argue that memory-augmented Transformers can benefit substantially from considering insights from the memory literature in humans. We detail an approach for integrating evidence from the human memory system through the specification of cross-domain linking hypotheses. We then provide an empirical demonstration to evaluate the use of surprisal as a linking hypothesis, and further identify the limitations of this approach to inform future research.},
	urldate = {2023-07-19},
	publisher = {arXiv},
	author = {Raccah, Omri and Chen, Phoebe and Willke, Ted L. and Poeppel, David and Vo, Vy A.},
	month = nov,
	year = {2022},
	note = {arXiv:2210.01869 [cs]},
	keywords = {LM, transformer, memory, surprisal, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:C\:\\Users\\karmeni1\\Zotero\\storage\\6NIW88I8\\Raccah et al. - 2022 - Memory in humans and deep language models Linking.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\karmeni1\\Zotero\\storage\\PVD5TMQW\\2210.html:text/html},
}

@misc{noauthor_zero--binder_nodate,
	title = {Zero-to-{Binder} — {The} {Turing} {Way}},
	url = {https://the-turing-way.netlify.app/communication/binder/zero-to-binder.html#changing-the-interface},
	urldate = {2023-07-22},
	file = {Zero-to-Binder — The Turing Way:C\:\\Users\\karmeni1\\Zotero\\storage\\U4VNXZ5C\\zero-to-binder.html:text/html},
}

@misc{liu_lost_2023,
	title = {Lost in the middle: {How} language models use long contexts},
	shorttitle = {Lost in the {Middle}},
	url = {http://arxiv.org/abs/2307.03172},
	doi = {10.48550/arXiv.2307.03172},
	abstract = {While recent language models have the ability to take long contexts as input, relatively little is known about how well they use longer context. We analyze language model performance on two tasks that require identifying relevant information within their input contexts: multi-document question answering and key-value retrieval. We find that performance is often highest when relevant information occurs at the beginning or end of the input context, and significantly degrades when models must access relevant information in the middle of long contexts. Furthermore, performance substantially decreases as the input context grows longer, even for explicitly long-context models. Our analysis provides a better understanding of how language models use their input context and provides new evaluation protocols for future long-context models.},
	urldate = {2023-08-24},
	publisher = {arXiv},
	author = {Liu, Nelson F. and Lin, Kevin and Hewitt, John and Paranjape, Ashwin and Bevilacqua, Michele and Petroni, Fabio and Liang, Percy},
	month = jul,
	year = {2023},
	note = {arXiv:2307.03172 [cs]},
	keywords = {LM, transformer, memory, context, ChatGPT, GPT-3.5, document retrieval},
}

@misc{wang_interpretability_2022,
	title = {Interpretability in the wild: a circuit for indirect object identification in {GPT}-2 small},
	shorttitle = {Interpretability in the {Wild}},
	url = {http://arxiv.org/abs/2211.00593},
	doi = {10.48550/arXiv.2211.00593},
	abstract = {Research in mechanistic interpretability seeks to explain behaviors of machine learning models in terms of their internal components. However, most previous work either focuses on simple behaviors in small models, or describes complicated behaviors in larger models with broad strokes. In this work, we bridge this gap by presenting an explanation for how GPT-2 small performs a natural language task called indirect object identification (IOI). Our explanation encompasses 26 attention heads grouped into 7 main classes, which we discovered using a combination of interpretability approaches relying on causal interventions. To our knowledge, this investigation is the largest end-to-end attempt at reverse-engineering a natural behavior "in the wild" in a language model. We evaluate the reliability of our explanation using three quantitative criteria--faithfulness, completeness and minimality. Though these criteria support our explanation, they also point to remaining gaps in our understanding. Our work provides evidence that a mechanistic understanding of large ML models is feasible, opening opportunities to scale our understanding to both larger models and more complex tasks.},
	urldate = {2023-09-19},
	publisher = {arXiv},
	author = {Wang, Kevin and Variengien, Alexandre and Conmy, Arthur and Shlegeris, Buck and Steinhardt, Jacob},
	month = nov,
	year = {2022},
	note = {arXiv:2211.00593 [cs]},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computation and Language, Computer Science - Machine Learning},
	file = {arXiv.org Snapshot:C\:\\Users\\karmeni1\\Zotero\\storage\\6D65ERGB\\2211.html:text/html;Wang et al. - 2022 - Interpretability in the wild a circuit for indire.pdf:C\:\\Users\\karmeni1\\Zotero\\storage\\G4T2C79N\\Wang et al. - 2022 - Interpretability in the wild a circuit for indire.pdf:application/pdf},
}

@misc{zhang_towards_2023,
	title = {Towards best practices of activation patching in language models: metrics and methods},
	shorttitle = {Towards {Best} {Practices} of {Activation} {Patching} in {Language} {Models}},
	url = {http://arxiv.org/abs/2309.16042},
	doi = {10.48550/arXiv.2309.16042},
	abstract = {Mechanistic interpretability seeks to understand the internal mechanisms of machine learning models, where localization -- identifying the important model components -- is a key step. Activation patching, also known as causal tracing or interchange intervention, is a standard technique for this task (Vig et al., 2020), but the literature contains many variants with little consensus on the choice of hyperparameters or methodology. In this work, we systematically examine the impact of methodological details in activation patching, including evaluation metrics and corruption methods. In several settings of localization and circuit discovery in language models, we find that varying these hyperparameters could lead to disparate interpretability results. Backed by empirical observations, we give conceptual arguments for why certain metrics or methods may be preferred. Finally, we provide recommendations for the best practices of activation patching going forwards.},
	urldate = {2023-10-03},
	publisher = {arXiv},
	author = {Zhang, Fred and Nanda, Neel},
	month = sep,
	year = {2023},
	note = {arXiv:2309.16042 [cs]},
	keywords = {ablation, GPT-2, interpretability, LLM, path patching},
	file = {arXiv Fulltext PDF:C\:\\Users\\karmeni1\\Zotero\\storage\\3J3LYBET\\Zhang and Nanda - 2023 - Towards Best Practices of Activation Patching in L.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\karmeni1\\Zotero\\storage\\TSUX4QAQ\\2309.html:text/html},
}

@misc{goldowsky-dill_localizing_2023,
	title = {Localizing model behavior with path patching},
	url = {http://arxiv.org/abs/2304.05969},
	doi = {10.48550/arXiv.2304.05969},
	abstract = {Localizing behaviors of neural networks to a subset of the network's components or a subset of interactions between components is a natural first step towards analyzing network mechanisms and possible failure modes. Existing work is often qualitative and ad-hoc, and there is no consensus on the appropriate way to evaluate localization claims. We introduce path patching, a technique for expressing and quantitatively testing a natural class of hypotheses expressing that behaviors are localized to a set of paths. We refine an explanation of induction heads, characterize a behavior of GPT-2, and open source a framework for efficiently running similar experiments.},
	urldate = {2023-10-03},
	publisher = {arXiv},
	author = {Goldowsky-Dill, Nicholas and MacLeod, Chris and Sato, Lucas and Arora, Aryaman},
	month = may,
	year = {2023},
	note = {arXiv:2304.05969 [cs]},
	keywords = {GPT-2, interpretability, LM, path patching},
	file = {arXiv Fulltext PDF:C\:\\Users\\karmeni1\\Zotero\\storage\\LS739PCI\\Goldowsky-Dill et al. - 2023 - Localizing Model Behavior with Path Patching.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\karmeni1\\Zotero\\storage\\CWF6SLBY\\2304.html:text/html},
}

@misc{sumers_cognitive_2023,
	title = {Cognitive architectures for language agents},
	url = {http://arxiv.org/abs/2309.02427},
	doi = {10.48550/arXiv.2309.02427},
	abstract = {Recent efforts have augmented large language models (LLMs) with external resources (e.g., the Internet) or internal control flows (e.g., prompt chaining) for tasks requiring grounding or reasoning, leading to a new class of language agents. While these agents have achieved substantial empirical success, we lack a systematic framework to organize existing agents and plan future developments. In this paper, we draw on the rich history of cognitive science and symbolic artificial intelligence to propose Cognitive Architectures for Language Agents (CoALA). CoALA describes a language agent with modular memory components, a structured action space to interact with internal memory and external environments, and a generalized decision-making process to choose actions. We use CoALA to retrospectively survey and organize a large body of recent work, and prospectively identify actionable directions towards more capable agents. Taken together, CoALA contextualizes today's language agents within the broader history of AI and outlines a path towards language-based general intelligence.},
	urldate = {2023-10-04},
	publisher = {arXiv},
	author = {Sumers, Theodore R. and Yao, Shunyu and Narasimhan, Karthik and Griffiths, Thomas L.},
	month = sep,
	year = {2023},
	note = {arXiv:2309.02427 [cs]
version: 2},
	keywords = {LLM, LLM agents, production systems, prompting, Soar},
	file = {arXiv Fulltext PDF:C\:\\Users\\karmeni1\\Zotero\\storage\\Y5SNAQFX\\Sumers et al. - 2023 - Cognitive Architectures for Language Agents.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\karmeni1\\Zotero\\storage\\IJ5DAPN4\\2309.html:text/html},
}

@article{laird_soar_1987,
	title = {{SOAR}: {An} architecture for general intelligence},
	volume = {33},
	issn = {00043702},
	shorttitle = {{SOAR}},
	url = {https://linkinghub.elsevier.com/retrieve/pii/0004370287900506},
	doi = {10.1016/0004-3702(87)90050-6},
	language = {en},
	number = {1},
	urldate = {2023-10-04},
	journal = {Artificial Intelligence},
	author = {Laird, John E. and Newell, Allen and Rosenbloom, Paul S.},
	month = sep,
	year = {1987},
	keywords = {artificial general intelligence, production systems, Soar, working memory},
	pages = {1--64},
}
